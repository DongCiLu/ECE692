Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-09-16 15:39:58.373886: step 0, loss = 4.68 (139.0 examples/sec; 0.921 sec/batch)
2017-09-16 15:39:58.993959: step 10, loss = 4.60 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:39:59.319545: step 20, loss = 4.46 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:39:59.647254: step 30, loss = 4.36 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:39:59.973877: step 40, loss = 4.33 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:00.298522: step 50, loss = 4.25 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:00.626679: step 60, loss = 4.15 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:00.952245: step 70, loss = 4.24 (7773.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:01.278585: step 80, loss = 4.22 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:01.603622: step 90, loss = 4.45 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:01.930409: step 100, loss = 4.39 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:02.407737: step 110, loss = 4.07 (7939.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:02.732038: step 120, loss = 4.16 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:03.057809: step 130, loss = 3.96 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:03.385134: step 140, loss = 4.00 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:03.709957: step 150, loss = 3.95 (7924.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:04.035742: step 160, loss = 4.02 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:04.369150: step 170, loss = 3.96 (7882.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:04.697825: step 180, loss = 3.86 (7538.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:05.023326: step 190, loss = 3.90 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:05.347535: step 200, loss = 3.72 (7822.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:05.838158: step 210, loss = 3.96 (7847.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:06.163067: step 220, loss = 3.78 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:06.486968: step 230, loss = 3.60 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:06.811928: step 240, loss = 3.72 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:07.137235: step 250, loss = 3.41 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:07.460350: step 260, loss = 3.61 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:07.785869: step 270, loss = 3.52 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:08.116056: step 280, loss = 3.52 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:08.439349: step 290, loss = 3.57 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:08.766206: step 300, loss = 3.43 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:09.250075: step 310, loss = 3.49 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:09.576743: step 320, loss = 3.81 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:09.900064: step 330, loss = 3.69 (7812.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:10.225461: step 340, loss = 3.33 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:10.551487: step 350, loss = 3.45 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:10.877119: step 360, loss = 3.52 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:11.203422: step 370, loss = 3.37 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:11.529211: step 380, loss = 3.29 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:11.855472: step 390, loss = 3.43 (7711.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:12.185865: step 400, loss = 3.34 (7935.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:12.666037: step 410, loss = 3.27 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:12.989907: step 420, loss = 3.21 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:13.313972: step 430, loss = 3.17 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:13.638093: step 440, loss = 3.31 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:13.967858: step 450, loss = 3.19 (7841.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:14.296966: step 460, loss = 3.32 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:14.621637: step 470, loss = 3.10 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:14.947116: step 480, loss = 3.12 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:15.270667: step 490, loss = 3.26 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:15.594874: step 500, loss = 3.11 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:16.072635: step 510, loss = 3.31 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:16.399721: step 520, loss = 3.15 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:16.724580: step 530, loss = 3.08 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:17.048709: step 540, loss = 2.96 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:17.425016: step 550, loss = 3.17 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:17.750593: step 560, loss = 3.01 (7777.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:18.074917: step 570, loss = 2.94 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:18.399907: step 580, loss = 2.99 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:18.723963: step 590, loss = 3.02 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:19.047304: step 600, loss = 3.05 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:19.525186: step 610, loss = 2.88 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:19.847895: step 620, loss = 2.76 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:20.170979: step 630, loss = 2.89 (7969.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:20.496924: step 640, loss = 2.78 (7689.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:20.821569: step 650, loss = 2.86 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:21.144487: step 660, loss = 2.93 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:21.471047: step 670, loss = 2.78 (7759.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:21.795436: step 680, loss = 2.89 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:22.119050: step 690, loss = 2.79 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:22.443036: step 700, loss = 2.80 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:22.920617: step 710, loss = 2.72 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:23.246961: step 720, loss = 2.74 (7934.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:23.571645: step 730, loss = 2.82 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:23.895183: step 740, loss = 2.69 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:24.219152: step 750, loss = 2.79 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:24.543425: step 760, loss = 2.71 (7815.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:24.870625: step 770, loss = 2.69 (7544.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:25.196912: step 780, loss = 2.64 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:25.522962: step 790, loss = 2.70 (7849.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:25.847799: step 800, loss = 2.70 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:26.333802: step 810, loss = 2.52 (7747.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:26.657725: step 820, loss = 2.91 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:26.981340: step 830, loss = 2.65 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:27.306110: step 840, loss = 2.38 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:27.633048: step 850, loss = 2.58 (7672.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:27.956301: step 860, loss = 2.43 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:28.277640: step 870, loss = 2.50 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:28.599306: step 880, loss = 2.78 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:28.924157: step 890, loss = 2.58 (7810.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:29.250388: step 900, loss = 2.37 (7859.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:29.723808: step 910, loss = 2.31 (7782.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:30.050870: step 920, loss = 2.42 (7841.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:30.378032: step 930, loss = 2.45 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:30.702570: step 940, loss = 2.34 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:31.027095: step 950, loss = 2.44 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:31.350852: step 960, loss = 2.48 (7862.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:31.674253: step 970, loss = 2.34 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:32.002868: step 980, loss = 2.32 (7446.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:32.326902: step 990, loss = 2.39 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:32.651083: step 1000, loss = 2.38 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:33.270652: step 1010, loss = 2.54 (7766.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:33.595328: step 1020, loss = 2.30 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:33.917581: step 1030, loss = 2.37 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:34.242975: step 1040, loss = 2.28 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:34.570330: step 1050, loss = 2.36 (7542.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:34.895866: step 1060, loss = 2.46 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:35.222198: step 1070, loss = 2.42 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:35.545468: step 1080, loss = 2.11 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:35.869482: step 1090, loss = 2.38 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:36.193953: step 1100, loss = 2.20 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:36.668238: step 1110, loss = 2.12 (7948.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:36.994821: step 1120, loss = 2.14 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:37.317435: step 1130, loss = 2.05 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:37.642202: step 1140, loss = 2.37 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:37.965123: step 1150, loss = 2.14 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:38.288813: step 1160, loss = 2.16 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:38.612698: step 1170, loss = 2.24 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:38.940647: step 1180, loss = 2.15 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:39.265129: step 1190, loss = 2.34 (7922.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:39.588269: step 1200, loss = 2.13 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:40.062177: step 1210, loss = 2.24 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:40.385625: step 1220, loss = 2.09 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:40.710166: step 1230, loss = 2.09 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:41.033082: step 1240, loss = 2.24 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:41.358675: step 1250, loss = 2.04 (7765.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:41.685410: step 1260, loss = 1.99 (7817.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:42.009230: step 1270, loss = 1.99 (7800.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:42.334736: step 1280, loss = 2.13 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:42.659936: step 1290, loss = 2.28 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:42.983861: step 1300, loss = 2.18 (7783.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:43.455546: step 1310, loss = 1.99 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:43.781369: step 1320, loss = 2.01 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:44.102745: step 1330, loss = 2.17 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:44.431897: step 1340, loss = 2.13 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:44.755721: step 1350, loss = 1.99 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:45.077935: step 1360, loss = 2.04 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:45.401702: step 1370, loss = 2.16 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:45.726092: step 1380, loss = 1.96 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:46.047662: step 1390, loss = 2.20 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:46.370937: step 1400, loss = 1.93 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:46.842156: step 1410, loss = 2.02 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:47.167755: step 1420, loss = 1.92 (7776.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:47.488464: step 1430, loss = 1.95 (7953.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:47.816280: step 1440, loss = 2.03 (7708.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:40:48.138670: step 1450, loss = 1.77 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:48.463755: step 1460, loss = 1.97 (7786.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:48.789417: step 1470, loss = 1.86 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:49.112937: step 1480, loss = 2.00 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:49.434035: step 1490, loss = 1.81 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:49.757072: step 1500, loss = 1.85 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:50.216506: step 1510, loss = 1.80 (7834.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:50.541361: step 1520, loss = 1.94 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:50.864960: step 1530, loss = 1.77 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:51.187119: step 1540, loss = 1.93 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:51.511115: step 1550, loss = 1.82 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:51.834596: step 1560, loss = 1.63 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:52.159722: step 1570, loss = 1.87 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:52.482745: step 1580, loss = 1.68 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:52.807759: step 1590, loss = 1.93 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:53.131308: step 1600, loss = 1.90 (7814.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:53.593489: step 1610, loss = 1.84 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:53.916739: step 1620, loss = 1.83 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:54.239470: step 1630, loss = 1.69 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:54.563857: step 1640, loss = 1.88 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:54.885531: step 1650, loss = 1.61 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:55.209999: step 1660, loss = 1.60 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:55.531928: step 1670, loss = 1.62 (7791.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:55.855101: step 1680, loss = 1.86 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:56.178744: step 1690, loss = 1.73 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:56.500695: step 1700, loss = 1.73 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:56.974221: step 1710, loss = 2.01 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:57.297959: step 1720, loss = 2.04 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:57.621321: step 1730, loss = 1.76 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:57.943660: step 1740, loss = 1.60 (7918.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:58.266392: step 1750, loss = 1.84 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:58.590979: step 1760, loss = 1.72 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:58.914910: step 1770, loss = 1.91 (7916.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:59.237138: step 1780, loss = 1.60 (7843.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:59.563358: step 1790, loss = 1.75 (8059.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:40:59.888649: step 1800, loss = 1.66 (7846.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:00.357887: step 1810, loss = 1.68 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:00.681568: step 1820, loss = 1.53 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:01.003032: step 1830, loss = 1.67 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:01.326652: step 1840, loss = 1.57 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:01.650415: step 1850, loss = 1.78 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:01.974105: step 1860, loss = 1.77 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:02.296460: step 1870, loss = 1.68 (7798.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:02.617129: step 1880, loss = 1.66 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:02.941429: step 1890, loss = 1.66 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:03.270018: step 1900, loss = 1.46 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:03.732783: step 1910, loss = 1.62 (7404.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:41:04.057867: step 1920, loss = 1.64 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:04.380153: step 1930, loss = 1.68 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:04.702457: step 1940, loss = 1.59 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:05.024684: step 1950, loss = 1.76 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:05.347636: step 1960, loss = 1.66 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:05.670154: step 1970, loss = 1.62 (7823.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:05.992055: step 1980, loss = 1.57 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:06.312665: step 1990, loss = 1.57 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:06.635946: step 2000, loss = 1.47 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:07.193315: step 2010, loss = 1.61 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:07.515718: step 2020, loss = 1.66 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:07.836570: step 2030, loss = 1.54 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:08.160173: step 2040, loss = 1.52 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:08.483242: step 2050, loss = 1.52 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:08.806590: step 2060, loss = 1.66 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:09.129995: step 2070, loss = 1.47 (7870.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:09.454808: step 2080, loss = 1.54 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:09.776455: step 2090, loss = 1.68 (7882.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:10.099087: step 2100, loss = 1.46 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:10.568574: step 2110, loss = 1.62 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:10.892307: step 2120, loss = 1.53 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:11.218127: step 2130, loss = 1.61 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:11.541319: step 2140, loss = 1.44 (7773.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:11.864227: step 2150, loss = 1.41 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:12.186330: step 2160, loss = 1.45 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:12.509437: step 2170, loss = 1.39 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:12.833572: step 2180, loss = 1.50 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:13.154863: step 2190, loss = 1.78 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:13.475558: step 2200, loss = 1.42 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:13.944127: step 2210, loss = 1.48 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:14.269169: step 2220, loss = 1.31 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:14.592144: step 2230, loss = 1.42 (7806.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:14.914001: step 2240, loss = 1.49 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:15.237648: step 2250, loss = 1.51 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:15.559280: step 2260, loss = 1.50 (7861.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:15.884006: step 2270, loss = 1.45 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:16.210556: step 2280, loss = 1.23 (7814.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:16.533017: step 2290, loss = 1.36 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:16.860534: step 2300, loss = 1.54 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:17.327601: step 2310, loss = 1.52 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:17.651290: step 2320, loss = 1.51 (7850.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:17.974186: step 2330, loss = 1.43 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:18.299854: step 2340, loss = 1.67 (7780.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:18.622884: step 2350, loss = 1.33 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:18.946690: step 2360, loss = 1.30 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:19.267510: step 2370, loss = 1.52 (8150.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:19.590140: step 2380, loss = 1.33 (7849.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:19.911726: step 2390, loss = 1.28 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:20.233900: step 2400, loss = 1.26 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:20.699751: step 2410, loss = 1.35 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:21.022525: step 2420, loss = 1.37 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:21.344194: step 2430, loss = 1.30 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:21.666680: step 2440, loss = 1.42 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:21.991462: step 2450, loss = 1.37 (7786.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:22.313456: step 2460, loss = 1.38 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:22.634363: step 2470, loss = 1.50 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:22.958672: step 2480, loss = 1.32 (7825.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:23.282459: step 2490, loss = 1.32 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:23.606717: step 2500, loss = 1.33 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:24.069230: step 2510, loss = 1.40 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:24.391222: step 2520, loss = 1.21 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:24.713686: step 2530, loss = 1.42 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:25.036666: step 2540, loss = 1.46 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:25.360468: step 2550, loss = 1.31 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:25.680128: step 2560, loss = 1.34 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:26.003713: step 2570, loss = 1.29 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:26.325304: step 2580, loss = 1.22 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:26.646514: step 2590, loss = 1.35 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:26.966747: step 2600, loss = 1.43 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:27.444052: step 2610, loss = 1.39 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:27.766863: step 2620, loss = 1.16 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:28.089618: step 2630, loss = 1.23 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:28.412869: step 2640, loss = 1.26 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:28.736059: step 2650, loss = 1.37 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:29.059613: step 2660, loss = 1.63 (7740.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:41:29.382190: step 2670, loss = 1.39 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:29.702727: step 2680, loss = 1.26 (7971.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:30.025257: step 2690, loss = 1.16 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:30.347175: step 2700, loss = 1.33 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:30.810245: step 2710, loss = 1.29 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:31.133631: step 2720, loss = 1.29 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:31.453035: step 2730, loss = 1.33 (8062.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:31.774121: step 2740, loss = 1.29 (7849.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:32.097307: step 2750, loss = 1.37 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:32.421206: step 2760, loss = 1.29 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:32.743108: step 2770, loss = 1.17 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:33.064742: step 2780, loss = 1.15 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:33.385933: step 2790, loss = 1.19 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:33.707745: step 2800, loss = 1.41 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:34.174839: step 2810, loss = 1.16 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:34.495291: step 2820, loss = 1.14 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:34.816747: step 2830, loss = 1.35 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:35.139390: step 2840, loss = 1.20 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:35.459930: step 2850, loss = 1.40 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:35.783797: step 2860, loss = 1.25 (7830.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:36.105120: step 2870, loss = 1.09 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:36.429559: step 2880, loss = 1.11 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:36.749102: step 2890, loss = 1.19 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:37.075944: step 2900, loss = 1.39 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:37.543874: step 2910, loss = 1.23 (7681.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:41:37.866783: step 2920, loss = 1.19 (7765.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:38.190071: step 2930, loss = 1.01 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:38.511485: step 2940, loss = 1.22 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:38.833635: step 2950, loss = 1.24 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:39.154266: step 2960, loss = 0.99 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:39.476925: step 2970, loss = 1.09 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:39.800373: step 2980, loss = 1.31 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:40.124384: step 2990, loss = 1.27 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:40.445964: step 3000, loss = 1.18 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:40.988938: step 3010, loss = 1.25 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:41.311965: step 3020, loss = 1.12 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:41.632970: step 3030, loss = 1.02 (7800.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:41.952515: step 3040, loss = 1.36 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:42.272737: step 3050, loss = 1.19 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:42.594383: step 3060, loss = 1.35 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:42.914559: step 3070, loss = 1.26 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:43.236345: step 3080, loss = 1.27 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:43.559250: step 3090, loss = 1.08 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:43.884806: step 3100, loss = 1.16 (7802.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:44.351117: step 3110, loss = 1.22 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:44.673189: step 3120, loss = 1.21 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:44.995748: step 3130, loss = 1.14 (7681.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:41:45.321534: step 3140, loss = 1.23 (7814.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:45.643214: step 3150, loss = 1.35 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:45.965316: step 3160, loss = 1.17 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:46.287565: step 3170, loss = 1.03 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:46.609727: step 3180, loss = 1.20 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:46.931208: step 3190, loss = 1.42 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:47.252678: step 3200, loss = 1.00 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:47.720345: step 3210, loss = 1.16 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:48.042137: step 3220, loss = 1.26 (7932.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:48.361573: step 3230, loss = 1.17 (7815.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:48.683661: step 3240, loss = 1.08 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:49.004677: step 3250, loss = 1.00 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:49.326145: step 3260, loss = 1.22 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:49.647860: step 3270, loss = 1.12 (7810.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:49.968607: step 3280, loss = 1.04 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:50.291905: step 3290, loss = 1.27 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:50.615237: step 3300, loss = 1.12 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:51.071484: step 3310, loss = 1.13 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:51.392898: step 3320, loss = 1.21 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:51.711953: step 3330, loss = 1.21 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:52.033620: step 3340, loss = 1.02 (7774.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:52.357614: step 3350, loss = 1.11 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:52.679010: step 3360, loss = 1.05 (7915.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:53.001514: step 3370, loss = 1.29 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:53.325477: step 3380, loss = 1.14 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:53.650319: step 3390, loss = 1.00 (7915.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:53.974199: step 3400, loss = 1.01 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:54.436914: step 3410, loss = 1.10 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:54.759631: step 3420, loss = 1.21 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:55.080238: step 3430, loss = 1.09 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:55.402932: step 3440, loss = 0.89 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:55.723624: step 3450, loss = 1.10 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:56.046974: step 3460, loss = 1.17 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:56.368581: step 3470, loss = 1.17 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:56.689343: step 3480, loss = 1.14 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:57.012697: step 3490, loss = 1.17 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:57.336550: step 3500, loss = 1.19 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:57.798638: step 3510, loss = 1.14 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:58.127190: step 3520, loss = 1.05 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:58.449035: step 3530, loss = 1.14 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:58.772829: step 3540, loss = 1.02 (7861.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:59.098196: step 3550, loss = 1.13 (7791.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:59.421479: step 3560, loss = 0.91 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:41:59.743903: step 3570, loss = 1.25 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:00.066340: step 3580, loss = 1.12 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:00.390418: step 3590, loss = 1.08 (7506.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:00.717683: step 3600, loss = 0.95 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:01.176150: step 3610, loss = 0.90 (7543.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:01.498887: step 3620, loss = 1.16 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:01.822632: step 3630, loss = 1.08 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:02.146061: step 3640, loss = 1.02 (7778.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:02.466421: step 3650, loss = 1.02 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:02.788341: step 3660, loss = 0.86 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:03.114816: step 3670, loss = 0.97 (7954.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:03.439380: step 3680, loss = 1.09 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:03.763178: step 3690, loss = 1.10 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:04.086008: step 3700, loss = 0.92 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:04.541621: step 3710, loss = 0.88 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:04.865604: step 3720, loss = 1.15 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:05.186714: step 3730, loss = 1.07 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:05.510149: step 3740, loss = 1.03 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:05.831921: step 3750, loss = 0.94 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:06.155068: step 3760, loss = 1.11 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:06.477422: step 3770, loss = 1.02 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:06.801357: step 3780, loss = 0.90 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:07.127416: step 3790, loss = 1.04 (7759.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:07.454177: step 3800, loss = 1.11 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:07.925933: step 3810, loss = 0.98 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:08.248542: step 3820, loss = 0.95 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:08.571257: step 3830, loss = 0.96 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:08.894347: step 3840, loss = 1.06 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:09.220411: step 3850, loss = 0.99 (7718.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:09.547918: step 3860, loss = 1.10 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:09.869935: step 3870, loss = 0.90 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:10.192275: step 3880, loss = 0.99 (7813.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:10.512621: step 3890, loss = 1.12 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:10.835535: step 3900, loss = 1.09 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:11.289320: step 3910, loss = 1.29 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:11.613252: step 3920, loss = 0.98 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:11.934139: step 3930, loss = 1.01 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:12.256455: step 3940, loss = 1.05 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:12.580911: step 3950, loss = 1.02 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:12.904887: step 3960, loss = 1.08 (7815.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:13.226786: step 3970, loss = 1.16 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:13.550668: step 3980, loss = 0.99 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:13.873644: step 3990, loss = 1.06 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:14.196370: step 4000, loss = 0.99 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:14.747252: step 4010, loss = 0.89 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:15.068609: step 4020, loss = 1.02 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:15.391253: step 4030, loss = 1.22 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:15.715208: step 4040, loss = 1.10 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:16.036681: step 4050, loss = 0.96 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:16.358484: step 4060, loss = 1.05 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:16.679488: step 4070, loss = 0.94 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:17.001847: step 4080, loss = 0.88 (7945.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:17.322811: step 4090, loss = 1.05 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:17.644342: step 4100, loss = 1.19 (7787.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:18.100692: step 4110, loss = 0.90 (7807.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:18.423091: step 4120, loss = 0.80 (7771.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:18.744137: step 4130, loss = 0.93 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:19.068412: step 4140, loss = 0.93 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:19.390256: step 4150, loss = 0.93 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:19.713488: step 4160, loss = 0.98 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:20.034705: step 4170, loss = 1.05 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:20.357766: step 4180, loss = 0.90 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:20.679728: step 4190, loss = 0.97 (7607.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:21.003523: step 4200, loss = 0.98 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:21.472185: step 4210, loss = 1.21 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:21.794359: step 4220, loss = 0.99 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:22.116431: step 4230, loss = 1.11 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:22.439996: step 4240, loss = 0.79 (7792.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:22.761781: step 4250, loss = 0.94 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:23.083647: step 4260, loss = 0.94 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:23.411712: step 4270, loss = 1.06 (7488.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:23.734235: step 4280, loss = 0.91 (7705.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:24.057637: step 4290, loss = 0.95 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:24.382148: step 4300, loss = 0.85 (7822.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:24.853350: step 4310, loss = 1.06 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:25.174526: step 4320, loss = 0.89 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:25.498495: step 4330, loss = 1.02 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:25.821580: step 4340, loss = 1.08 (7765.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:26.145220: step 4350, loss = 1.01 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:26.470333: step 4360, loss = 0.83 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:26.797696: step 4370, loss = 0.97 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:27.124249: step 4380, loss = 1.06 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:27.450153: step 4390, loss = 0.80 (7795.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:27.771860: step 4400, loss = 1.06 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:28.227280: step 4410, loss = 1.04 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:28.555676: step 4420, loss = 1.07 (7861.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:28.878146: step 4430, loss = 0.88 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:29.200905: step 4440, loss = 0.92 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:29.525456: step 4450, loss = 0.82 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:29.846845: step 4460, loss = 0.95 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:30.169900: step 4470, loss = 1.08 (7692.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:30.492912: step 4480, loss = 0.89 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:30.817962: step 4490, loss = 0.94 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:31.140397: step 4500, loss = 0.95 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:31.595192: step 4510, loss = 1.01 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:31.915997: step 4520, loss = 1.03 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:32.239401: step 4530, loss = 0.96 (8130.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:32.562651: step 4540, loss = 0.98 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:32.887150: step 4550, loss = 0.94 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:33.215236: step 4560, loss = 1.13 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:33.538682: step 4570, loss = 1.03 (7560.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:33.862806: step 4580, loss = 0.88 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:34.185613: step 4590, loss = 1.04 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:34.510351: step 4600, loss = 0.79 (7832.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:34.969237: step 4610, loss = 0.81 (7796.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:35.290809: step 4620, loss = 1.04 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:35.613464: step 4630, loss = 1.00 (7921.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:35.935823: step 4640, loss = 0.83 (7884.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:36.260531: step 4650, loss = 0.89 (7483.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:36.582926: step 4660, loss = 0.96 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:36.905847: step 4670, loss = 1.01 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:37.227829: step 4680, loss = 0.98 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:37.549996: step 4690, loss = 1.10 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:37.873338: step 4700, loss = 0.96 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:38.339897: step 4710, loss = 0.85 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:38.661329: step 4720, loss = 0.83 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:38.982966: step 4730, loss = 0.89 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:39.305417: step 4740, loss = 0.98 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:39.628760: step 4750, loss = 0.94 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:39.949633: step 4760, loss = 1.14 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:40.272051: step 4770, loss = 0.89 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:40.592294: step 4780, loss = 0.93 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:40.916689: step 4790, loss = 0.93 (7925.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:41.242968: step 4800, loss = 0.76 (7953.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:41.705426: step 4810, loss = 1.27 (7703.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:42.026193: step 4820, loss = 0.99 (7749.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:42.348499: step 4830, loss = 0.77 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:42.670476: step 4840, loss = 0.93 (7804.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:42.992734: step 4850, loss = 1.00 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:43.315578: step 4860, loss = 0.88 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:43.637024: step 4870, loss = 0.85 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:43.963883: step 4880, loss = 1.02 (7687.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:44.287684: step 4890, loss = 0.88 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:44.608845: step 4900, loss = 0.94 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:45.076892: step 4910, loss = 0.87 (7828.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:45.399427: step 4920, loss = 0.84 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:45.720869: step 4930, loss = 0.99 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:46.043139: step 4940, loss = 0.89 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:46.365783: step 4950, loss = 1.00 (7734.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:42:46.687896: step 4960, loss = 0.81 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:47.007908: step 4970, loss = 0.90 (7886.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:47.330228: step 4980, loss = 0.92 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:47.654216: step 4990, loss = 0.89 (7846.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:47.976049: step 5000, loss = 1.26 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:48.535927: step 5010, loss = 0.89 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:48.857127: step 5020, loss = 0.91 (7835.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:49.177783: step 5030, loss = 0.87 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:49.500117: step 5040, loss = 0.85 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:49.820529: step 5050, loss = 0.86 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:50.142023: step 5060, loss = 0.90 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:50.463474: step 5070, loss = 0.76 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:50.784567: step 5080, loss = 0.97 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:51.105781: step 5090, loss = 0.89 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:51.426226: step 5100, loss = 1.05 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:51.902704: step 5110, loss = 0.84 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:52.224612: step 5120, loss = 0.79 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:52.548394: step 5130, loss = 0.79 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:52.871147: step 5140, loss = 0.73 (7954.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:53.193113: step 5150, loss = 0.87 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:53.514599: step 5160, loss = 0.96 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:53.835130: step 5170, loss = 0.75 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:54.157154: step 5180, loss = 0.93 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:54.479668: step 5190, loss = 0.90 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:54.805791: step 5200, loss = 0.83 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:55.263643: step 5210, loss = 0.90 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:55.587081: step 5220, loss = 0.87 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:55.908364: step 5230, loss = 1.04 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:56.229056: step 5240, loss = 0.93 (7801.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:56.549252: step 5250, loss = 0.95 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:56.874068: step 5260, loss = 0.95 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:57.199149: step 5270, loss = 0.96 (7828.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:57.525411: step 5280, loss = 0.95 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:57.848251: step 5290, loss = 0.91 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:58.171314: step 5300, loss = 0.91 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:58.638031: step 5310, loss = 1.20 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:58.960821: step 5320, loss = 1.09 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:59.283614: step 5330, loss = 0.91 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:59.604212: step 5340, loss = 0.80 (8134.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:42:59.928002: step 5350, loss = 1.02 (7823.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:00.250315: step 5360, loss = 0.88 (7768.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:00.570790: step 5370, loss = 0.94 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:00.894074: step 5380, loss = 0.98 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:01.216619: step 5390, loss = 1.07 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:01.539527: step 5400, loss = 0.89 (8133.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:01.997156: step 5410, loss = 1.01 (7968.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:02.320366: step 5420, loss = 0.69 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:02.642617: step 5430, loss = 0.95 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:02.964814: step 5440, loss = 1.00 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:03.285115: step 5450, loss = 0.82 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:03.607845: step 5460, loss = 0.85 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:03.930515: step 5470, loss = 0.95 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:04.252654: step 5480, loss = 0.92 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:04.575313: step 5490, loss = 0.73 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:04.897021: step 5500, loss = 0.89 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:05.365427: step 5510, loss = 0.89 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:05.690691: step 5520, loss = 0.86 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:06.015611: step 5530, loss = 0.94 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:06.336874: step 5540, loss = 0.83 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:06.658823: step 5550, loss = 0.84 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:06.982203: step 5560, loss = 0.83 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:07.305243: step 5570, loss = 0.97 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:07.628298: step 5580, loss = 0.82 (7833.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:07.950683: step 5590, loss = 0.88 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:08.274595: step 5600, loss = 0.90 (7741.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:08.742495: step 5610, loss = 0.77 (7789.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:09.066758: step 5620, loss = 0.76 (7637.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:09.390026: step 5630, loss = 0.96 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:09.715944: step 5640, loss = 0.84 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:10.041927: step 5650, loss = 0.98 (7813.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:10.364631: step 5660, loss = 0.91 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:10.688211: step 5670, loss = 0.89 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:11.009017: step 5680, loss = 0.81 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:11.330158: step 5690, loss = 0.97 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:11.655182: step 5700, loss = 0.93 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:12.114391: step 5710, loss = 0.92 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:12.438157: step 5720, loss = 0.90 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:12.759225: step 5730, loss = 0.94 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:13.080810: step 5740, loss = 0.98 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:13.401932: step 5750, loss = 0.96 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:13.726947: step 5760, loss = 0.72 (7756.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:14.046520: step 5770, loss = 0.98 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:14.366897: step 5780, loss = 0.70 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:14.687998: step 5790, loss = 0.83 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:15.008602: step 5800, loss = 0.76 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:15.470128: step 5810, loss = 1.12 (7920.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:15.791588: step 5820, loss = 0.76 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:16.112663: step 5830, loss = 0.96 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:16.433989: step 5840, loss = 0.80 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:16.757652: step 5850, loss = 0.86 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:17.078372: step 5860, loss = 0.72 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:17.400523: step 5870, loss = 0.91 (7894.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:17.722772: step 5880, loss = 0.90 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:18.047629: step 5890, loss = 0.93 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:18.370518: step 5900, loss = 0.88 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:18.837304: step 5910, loss = 0.90 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:19.157861: step 5920, loss = 0.94 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:19.479588: step 5930, loss = 0.92 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:19.801226: step 5940, loss = 0.90 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:20.123463: step 5950, loss = 0.78 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:20.447625: step 5960, loss = 0.78 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:20.769452: step 5970, loss = 0.94 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:21.091540: step 5980, loss = 1.14 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:21.417975: step 5990, loss = 0.75 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:21.742861: step 6000, loss = 0.81 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:22.308656: step 6010, loss = 0.90 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:22.630796: step 6020, loss = 0.99 (7772.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:22.955065: step 6030, loss = 0.71 (7829.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:23.277629: step 6040, loss = 0.78 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:23.604283: step 6050, loss = 0.92 (7454.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:23.929179: step 6060, loss = 0.96 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:24.250670: step 6070, loss = 0.97 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:24.574608: step 6080, loss = 0.74 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:24.898028: step 6090, loss = 0.92 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:25.221979: step 6100, loss = 0.76 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:25.676654: step 6110, loss = 0.89 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:25.996947: step 6120, loss = 0.81 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:26.320829: step 6130, loss = 1.05 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:26.644588: step 6140, loss = 0.94 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:26.965988: step 6150, loss = 0.74 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:27.287466: step 6160, loss = 0.88 (7927.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:27.609895: step 6170, loss = 0.84 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:27.933515: step 6180, loss = 0.86 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:28.256024: step 6190, loss = 0.85 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:28.578628: step 6200, loss = 0.97 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:29.034320: step 6210, loss = 1.09 (7953.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:29.355459: step 6220, loss = 0.95 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:29.676374: step 6230, loss = 0.80 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:30.000230: step 6240, loss = 0.78 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:30.321147: step 6250, loss = 0.90 (7891.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:30.646094: step 6260, loss = 0.74 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:30.970896: step 6270, loss = 0.86 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:31.293643: step 6280, loss = 1.03 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:31.615732: step 6290, loss = 0.71 (7795.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:31.937561: step 6300, loss = 0.87 (8124.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:32.398342: step 6310, loss = 0.74 (7950.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:32.723997: step 6320, loss = 0.99 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:33.048977: step 6330, loss = 0.99 (7891.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:33.373903: step 6340, loss = 0.73 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:33.697087: step 6350, loss = 1.05 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:34.020565: step 6360, loss = 0.96 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:34.342854: step 6370, loss = 0.88 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:34.663046: step 6380, loss = 0.85 (7970.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:34.985483: step 6390, loss = 0.72 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:35.308081: step 6400, loss = 0.97 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:35.765345: step 6410, loss = 0.91 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:36.087485: step 6420, loss = 0.91 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:36.413143: step 6430, loss = 0.89 (7810.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:36.735976: step 6440, loss = 0.95 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:37.059278: step 6450, loss = 0.78 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:37.381715: step 6460, loss = 0.76 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:37.703219: step 6470, loss = 0.91 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:38.024270: step 6480, loss = 0.85 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:38.345876: step 6490, loss = 0.74 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:38.667417: step 6500, loss = 0.90 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:39.119881: step 6510, loss = 0.77 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:39.442814: step 6520, loss = 0.84 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:39.765810: step 6530, loss = 0.72 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:40.089847: step 6540, loss = 0.90 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:40.420201: step 6550, loss = 0.87 (7413.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:40.752672: step 6560, loss = 0.73 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:41.078169: step 6570, loss = 0.79 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:41.403737: step 6580, loss = 0.81 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:41.723833: step 6590, loss = 0.70 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:42.046720: step 6600, loss = 1.02 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:42.504640: step 6610, loss = 0.86 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:42.823420: step 6620, loss = 0.84 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:43.145951: step 6630, loss = 0.84 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:43.468371: step 6640, loss = 1.04 (7849.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:43.789453: step 6650, loss = 0.85 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:44.112325: step 6660, loss = 0.83 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:44.436998: step 6670, loss = 0.64 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:44.760284: step 6680, loss = 0.80 (7803.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:45.083829: step 6690, loss = 0.87 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:45.403606: step 6700, loss = 0.81 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:45.868352: step 6710, loss = 0.79 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:46.188973: step 6720, loss = 0.71 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:46.510073: step 6730, loss = 1.03 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:46.831593: step 6740, loss = 0.71 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:47.152845: step 6750, loss = 0.78 (7857.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:47.473532: step 6760, loss = 0.78 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:47.794335: step 6770, loss = 0.72 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:48.117460: step 6780, loss = 0.85 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:48.438700: step 6790, loss = 1.03 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:48.761014: step 6800, loss = 0.70 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:49.231867: step 6810, loss = 0.76 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:49.555544: step 6820, loss = 0.80 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:49.878958: step 6830, loss = 0.87 (7757.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:43:50.210259: step 6840, loss = 0.77 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:50.533791: step 6850, loss = 0.85 (7941.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:50.855366: step 6860, loss = 0.86 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:51.178324: step 6870, loss = 0.72 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:51.500559: step 6880, loss = 0.75 (7801.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:51.822460: step 6890, loss = 0.75 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:52.146005: step 6900, loss = 0.80 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:52.612781: step 6910, loss = 0.89 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:52.935799: step 6920, loss = 0.68 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:53.256910: step 6930, loss = 0.86 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:53.580679: step 6940, loss = 0.78 (7923.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:53.904269: step 6950, loss = 0.77 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:54.227758: step 6960, loss = 0.66 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:54.547845: step 6970, loss = 0.82 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:54.870062: step 6980, loss = 0.75 (7861.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:55.191894: step 6990, loss = 0.74 (7892.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:55.512643: step 7000, loss = 0.94 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:56.075734: step 7010, loss = 0.83 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:56.397360: step 7020, loss = 0.83 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:56.716886: step 7030, loss = 0.96 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:57.038011: step 7040, loss = 0.96 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:57.359785: step 7050, loss = 0.79 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:57.688968: step 7060, loss = 0.74 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:58.012425: step 7070, loss = 0.64 (7819.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:58.336064: step 7080, loss = 0.80 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:58.659076: step 7090, loss = 0.69 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:58.981943: step 7100, loss = 0.84 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:59.436517: step 7110, loss = 0.83 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:43:59.759217: step 7120, loss = 0.83 (7671.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:00.080747: step 7130, loss = 0.74 (7789.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:00.401268: step 7140, loss = 0.74 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:00.722485: step 7150, loss = 0.79 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:01.046540: step 7160, loss = 1.03 (7822.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:01.368534: step 7170, loss = 1.00 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:01.692730: step 7180, loss = 0.99 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:02.016320: step 7190, loss = 0.76 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:02.339994: step 7200, loss = 0.88 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:02.806786: step 7210, loss = 0.97 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:03.130679: step 7220, loss = 0.93 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:03.455182: step 7230, loss = 0.85 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:03.777064: step 7240, loss = 0.96 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:04.099996: step 7250, loss = 0.85 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:04.421028: step 7260, loss = 0.84 (7917.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:04.743642: step 7270, loss = 0.76 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:05.066165: step 7280, loss = 0.83 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:05.386648: step 7290, loss = 0.73 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:05.706704: step 7300, loss = 0.95 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:06.174088: step 7310, loss = 0.82 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:06.500946: step 7320, loss = 0.69 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:06.823191: step 7330, loss = 1.02 (7983.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:07.148669: step 7340, loss = 0.82 (7892.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:07.470611: step 7350, loss = 0.80 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:07.791116: step 7360, loss = 0.64 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:08.116483: step 7370, loss = 0.93 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:08.437971: step 7380, loss = 0.81 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:08.760052: step 7390, loss = 0.74 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:09.081211: step 7400, loss = 0.69 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:09.531968: step 7410, loss = 0.90 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:09.854986: step 7420, loss = 0.97 (7792.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:10.176201: step 7430, loss = 0.82 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:10.499222: step 7440, loss = 0.86 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:10.819783: step 7450, loss = 0.84 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:11.142134: step 7460, loss = 0.72 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:11.464440: step 7470, loss = 0.84 (7402.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:11.787881: step 7480, loss = 0.76 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:12.110981: step 7490, loss = 0.94 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:12.433434: step 7500, loss = 0.74 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:12.891302: step 7510, loss = 0.88 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:13.214310: step 7520, loss = 0.66 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:13.536534: step 7530, loss = 0.71 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:13.859363: step 7540, loss = 0.77 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:14.180149: step 7550, loss = 0.75 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:14.505528: step 7560, loss = 0.63 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:14.830962: step 7570, loss = 0.75 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:15.153843: step 7580, loss = 0.72 (7872.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:15.478477: step 7590, loss = 0.90 (7938.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:15.801055: step 7600, loss = 0.85 (7834.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:16.265204: step 7610, loss = 0.78 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:16.587002: step 7620, loss = 0.88 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:16.909911: step 7630, loss = 0.74 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:17.229910: step 7640, loss = 0.74 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:17.549435: step 7650, loss = 0.59 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:17.872184: step 7660, loss = 0.67 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:18.192621: step 7670, loss = 0.95 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:18.513381: step 7680, loss = 0.78 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:18.835278: step 7690, loss = 0.92 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:19.156595: step 7700, loss = 0.82 (7877.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:19.620571: step 7710, loss = 0.69 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:19.942355: step 7720, loss = 0.79 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:20.264309: step 7730, loss = 0.74 (7801.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:20.587491: step 7740, loss = 0.73 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:20.912142: step 7750, loss = 0.82 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:21.231543: step 7760, loss = 0.71 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:21.552365: step 7770, loss = 0.81 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:21.872321: step 7780, loss = 0.88 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:22.195646: step 7790, loss = 0.85 (7795.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:22.518127: step 7800, loss = 0.76 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:22.971145: step 7810, loss = 0.81 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:23.294201: step 7820, loss = 0.81 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:23.614750: step 7830, loss = 0.77 (7533.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:23.935843: step 7840, loss = 0.82 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:24.257653: step 7850, loss = 0.83 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:24.579485: step 7860, loss = 0.76 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:24.901013: step 7870, loss = 0.99 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:25.223132: step 7880, loss = 0.77 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:25.546055: step 7890, loss = 0.83 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:25.869040: step 7900, loss = 0.90 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:26.324220: step 7910, loss = 0.76 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:26.646149: step 7920, loss = 0.76 (7931.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:26.971557: step 7930, loss = 0.75 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:27.297160: step 7940, loss = 0.93 (7825.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:27.620192: step 7950, loss = 0.66 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:27.943157: step 7960, loss = 0.79 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:28.265695: step 7970, loss = 0.88 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:28.587748: step 7980, loss = 0.70 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:28.910168: step 7990, loss = 0.84 (7664.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:29.232954: step 8000, loss = 0.84 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:29.843831: step 8010, loss = 1.12 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:30.167153: step 8020, loss = 0.79 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:30.491019: step 8030, loss = 0.90 (7755.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:30.813864: step 8040, loss = 0.74 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:31.136898: step 8050, loss = 0.82 (7859.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:31.459424: step 8060, loss = 0.70 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:31.779722: step 8070, loss = 0.93 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:32.103509: step 8080, loss = 0.79 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:32.429050: step 8090, loss = 0.92 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:32.752731: step 8100, loss = 0.79 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:33.218716: step 8110, loss = 0.74 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:33.541121: step 8120, loss = 0.90 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:33.866215: step 8130, loss = 0.72 (7780.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:34.187066: step 8140, loss = 0.76 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:34.506534: step 8150, loss = 0.95 (7886.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:34.830910: step 8160, loss = 0.75 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:35.155253: step 8170, loss = 0.75 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:35.477621: step 8180, loss = 0.95 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:35.801636: step 8190, loss = 0.76 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:36.122935: step 8200, loss = 0.93 (7910.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:36.586848: step 8210, loss = 0.75 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:36.909089: step 8220, loss = 0.88 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:37.231613: step 8230, loss = 0.82 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:37.551903: step 8240, loss = 0.92 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:37.874429: step 8250, loss = 0.75 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:38.203907: step 8260, loss = 0.77 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:38.532067: step 8270, loss = 0.78 (8159.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:38.855770: step 8280, loss = 0.84 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:39.179905: step 8290, loss = 0.75 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:39.500455: step 8300, loss = 0.93 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:39.966960: step 8310, loss = 0.85 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:40.290410: step 8320, loss = 0.86 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:40.615936: step 8330, loss = 0.89 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:40.936339: step 8340, loss = 0.90 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:41.261857: step 8350, loss = 0.79 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:41.582927: step 8360, loss = 0.69 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:41.905654: step 8370, loss = 0.74 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:42.231979: step 8380, loss = 0.91 (7787.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:42.553777: step 8390, loss = 0.82 (7985.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:42.884637: step 8400, loss = 0.75 (7443.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:43.349888: step 8410, loss = 0.80 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:43.671084: step 8420, loss = 0.87 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:43.991894: step 8430, loss = 0.81 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:44.315189: step 8440, loss = 0.64 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:44.638342: step 8450, loss = 0.78 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:44.959391: step 8460, loss = 0.84 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:45.280963: step 8470, loss = 0.86 (7815.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:45.607298: step 8480, loss = 0.75 (7378.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:44:45.934479: step 8490, loss = 0.75 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:46.255519: step 8500, loss = 0.94 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:46.725343: step 8510, loss = 0.79 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:47.051258: step 8520, loss = 0.69 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:47.373200: step 8530, loss = 0.73 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:47.698274: step 8540, loss = 0.85 (7815.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:48.021880: step 8550, loss = 0.75 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:48.345374: step 8560, loss = 0.88 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:48.672454: step 8570, loss = 0.90 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:48.993524: step 8580, loss = 0.73 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:49.316733: step 8590, loss = 0.71 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:49.638370: step 8600, loss = 0.84 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:50.099573: step 8610, loss = 0.90 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:50.421521: step 8620, loss = 0.68 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:50.744189: step 8630, loss = 0.59 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:51.066755: step 8640, loss = 0.72 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:51.389805: step 8650, loss = 0.73 (7849.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:51.710926: step 8660, loss = 0.73 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:52.034503: step 8670, loss = 0.70 (7831.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:52.355364: step 8680, loss = 0.67 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:52.677262: step 8690, loss = 0.77 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:53.000795: step 8700, loss = 0.84 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:53.461589: step 8710, loss = 0.75 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:53.783478: step 8720, loss = 0.71 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:54.106480: step 8730, loss = 0.74 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:54.428300: step 8740, loss = 0.75 (7885.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:54.751010: step 8750, loss = 0.66 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:55.074270: step 8760, loss = 0.99 (7861.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:55.394827: step 8770, loss = 0.87 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:55.715980: step 8780, loss = 0.95 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:56.035999: step 8790, loss = 0.90 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:56.357153: step 8800, loss = 0.75 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:56.813422: step 8810, loss = 0.68 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:57.135285: step 8820, loss = 0.86 (8128.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:57.460251: step 8830, loss = 0.66 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:57.781952: step 8840, loss = 0.68 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:58.103831: step 8850, loss = 0.76 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:58.430629: step 8860, loss = 0.72 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:58.753739: step 8870, loss = 0.78 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:59.076035: step 8880, loss = 0.78 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:59.398283: step 8890, loss = 0.63 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:44:59.722672: step 8900, loss = 0.85 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:00.188246: step 8910, loss = 0.71 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:00.512049: step 8920, loss = 0.73 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:00.838257: step 8930, loss = 0.92 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:01.158765: step 8940, loss = 1.00 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:01.479776: step 8950, loss = 0.56 (8133.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:01.802700: step 8960, loss = 0.87 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:02.124731: step 8970, loss = 0.66 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:02.447127: step 8980, loss = 0.84 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:02.769686: step 8990, loss = 0.76 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:03.095152: step 9000, loss = 0.96 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:03.662312: step 9010, loss = 0.88 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:03.986081: step 9020, loss = 0.89 (8115.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:04.309236: step 9030, loss = 0.59 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:04.631294: step 9040, loss = 1.02 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:04.953726: step 9050, loss = 0.73 (7910.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:05.274898: step 9060, loss = 0.80 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:05.595883: step 9070, loss = 0.85 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:05.918282: step 9080, loss = 0.90 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:06.243245: step 9090, loss = 0.78 (7875.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:06.567011: step 9100, loss = 0.70 (7908.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:07.029730: step 9110, loss = 0.78 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:07.349523: step 9120, loss = 0.73 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:07.670119: step 9130, loss = 0.75 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:07.991623: step 9140, loss = 0.76 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:08.314630: step 9150, loss = 0.66 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:08.635829: step 9160, loss = 0.74 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:08.959078: step 9170, loss = 0.65 (7808.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:09.283408: step 9180, loss = 0.66 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:09.603039: step 9190, loss = 0.70 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:09.924689: step 9200, loss = 0.78 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:10.378356: step 9210, loss = 0.77 (7701.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:45:10.699034: step 9220, loss = 0.69 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:11.025171: step 9230, loss = 0.82 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:11.348820: step 9240, loss = 0.72 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:11.671982: step 9250, loss = 0.70 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:11.994310: step 9260, loss = 0.74 (7883.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:12.320131: step 9270, loss = 0.69 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:12.641410: step 9280, loss = 0.67 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:12.965562: step 9290, loss = 0.73 (7783.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:13.288095: step 9300, loss = 0.85 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:13.755687: step 9310, loss = 0.69 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:14.077825: step 9320, loss = 0.78 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:14.398290: step 9330, loss = 0.85 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:14.721735: step 9340, loss = 0.92 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:15.044772: step 9350, loss = 0.70 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:15.367423: step 9360, loss = 0.78 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:15.686596: step 9370, loss = 0.79 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:16.008586: step 9380, loss = 0.83 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:16.336501: step 9390, loss = 0.76 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:16.656651: step 9400, loss = 0.75 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:17.129363: step 9410, loss = 0.78 (8113.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:17.453081: step 9420, loss = 0.79 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:17.773752: step 9430, loss = 0.78 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:18.096172: step 9440, loss = 0.68 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:18.415283: step 9450, loss = 0.74 (7823.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:18.736990: step 9460, loss = 0.86 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:19.058782: step 9470, loss = 0.99 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:19.378813: step 9480, loss = 0.59 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:19.701568: step 9490, loss = 0.61 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:20.024017: step 9500, loss = 0.81 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:20.492150: step 9510, loss = 0.82 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:20.815890: step 9520, loss = 0.60 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:21.136275: step 9530, loss = 0.70 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:21.462864: step 9540, loss = 0.72 (7542.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:45:21.785430: step 9550, loss = 0.91 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:22.107774: step 9560, loss = 0.56 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:22.430877: step 9570, loss = 0.78 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:22.755604: step 9580, loss = 0.59 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:23.081049: step 9590, loss = 0.87 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:23.402291: step 9600, loss = 0.83 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:23.869242: step 9610, loss = 0.73 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:24.195303: step 9620, loss = 0.86 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:24.521508: step 9630, loss = 0.71 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:24.843083: step 9640, loss = 0.92 (8130.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:25.166774: step 9650, loss = 0.67 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:25.485783: step 9660, loss = 0.73 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:25.806742: step 9670, loss = 0.63 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:26.128155: step 9680, loss = 0.65 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:26.448132: step 9690, loss = 0.75 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:26.769001: step 9700, loss = 0.65 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:27.232324: step 9710, loss = 0.62 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:27.553276: step 9720, loss = 0.81 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:27.871950: step 9730, loss = 0.71 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:28.193384: step 9740, loss = 0.78 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:28.514984: step 9750, loss = 0.86 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:28.835738: step 9760, loss = 0.69 (7970.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:29.156805: step 9770, loss = 0.85 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:29.477312: step 9780, loss = 0.94 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:29.796175: step 9790, loss = 0.64 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:30.117492: step 9800, loss = 0.80 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:30.586059: step 9810, loss = 0.80 (7851.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:30.906941: step 9820, loss = 0.73 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:31.228472: step 9830, loss = 0.71 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:31.551449: step 9840, loss = 0.71 (7848.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:31.872420: step 9850, loss = 0.92 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:32.197657: step 9860, loss = 0.87 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:32.520427: step 9870, loss = 0.72 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:32.840009: step 9880, loss = 0.83 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:33.160398: step 9890, loss = 0.77 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:33.480898: step 9900, loss = 0.71 (7921.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:33.952688: step 9910, loss = 0.83 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:34.274069: step 9920, loss = 0.76 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:34.601795: step 9930, loss = 0.62 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:34.921286: step 9940, loss = 0.66 (8089.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:35.241998: step 9950, loss = 0.79 (7963.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:35.563642: step 9960, loss = 0.61 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:35.885847: step 9970, loss = 0.78 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:36.208549: step 9980, loss = 0.78 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:36.531388: step 9990, loss = 0.68 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:36.855088: step 10000, loss = 0.72 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:37.419393: step 10010, loss = 0.83 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:37.739912: step 10020, loss = 0.76 (7882.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:38.061398: step 10030, loss = 0.78 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:38.382906: step 10040, loss = 0.74 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:38.709304: step 10050, loss = 0.70 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:39.029816: step 10060, loss = 0.85 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:39.351721: step 10070, loss = 0.82 (7804.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:39.675636: step 10080, loss = 0.77 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:39.997828: step 10090, loss = 0.80 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:40.319059: step 10100, loss = 0.78 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:40.785864: step 10110, loss = 0.78 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:41.112605: step 10120, loss = 0.78 (7920.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:41.437946: step 10130, loss = 0.74 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:41.758496: step 10140, loss = 0.71 (7853.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:42.083284: step 10150, loss = 0.83 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:42.405551: step 10160, loss = 0.66 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:42.728103: step 10170, loss = 1.11 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:43.050675: step 10180, loss = 0.65 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:43.369017: step 10190, loss = 0.75 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:43.690600: step 10200, loss = 0.76 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:44.140966: step 10210, loss = 0.67 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:44.461756: step 10220, loss = 0.70 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:44.782932: step 10230, loss = 0.93 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:45.104094: step 10240, loss = 0.80 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:45.423874: step 10250, loss = 0.76 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:45.743652: step 10260, loss = 0.76 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:46.068096: step 10270, loss = 1.15 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:46.395475: step 10280, loss = 0.83 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:46.714283: step 10290, loss = 0.83 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:47.035129: step 10300, loss = 0.72 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:47.490964: step 10310, loss = 0.71 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:47.812877: step 10320, loss = 0.78 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:48.136468: step 10330, loss = 0.71 (7876.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:48.459828: step 10340, loss = 0.82 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:48.781543: step 10350, loss = 0.68 (8130.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:49.103565: step 10360, loss = 0.79 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:49.425494: step 10370, loss = 0.82 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:49.747867: step 10380, loss = 0.76 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:50.069184: step 10390, loss = 0.74 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:50.390972: step 10400, loss = 0.70 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:50.851870: step 10410, loss = 0.79 (7866.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:51.173608: step 10420, loss = 0.78 (7791.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:51.496343: step 10430, loss = 0.76 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:51.818083: step 10440, loss = 0.80 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:52.140608: step 10450, loss = 0.81 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:52.464408: step 10460, loss = 0.72 (7601.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:45:52.786356: step 10470, loss = 0.63 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:53.108502: step 10480, loss = 0.69 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:53.430037: step 10490, loss = 0.76 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:53.753287: step 10500, loss = 0.60 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:54.214938: step 10510, loss = 0.73 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:54.540967: step 10520, loss = 0.90 (7584.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:45:54.863236: step 10530, loss = 0.79 (7807.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:55.183766: step 10540, loss = 0.70 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:55.505192: step 10550, loss = 1.00 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:55.828582: step 10560, loss = 0.60 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:56.151072: step 10570, loss = 0.89 (7995.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:56.473563: step 10580, loss = 0.67 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:56.795458: step 10590, loss = 0.81 (8123.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:57.122812: step 10600, loss = 0.63 (7574.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:45:57.592763: step 10610, loss = 0.85 (7950.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:57.915503: step 10620, loss = 0.83 (8164.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:58.239635: step 10630, loss = 0.74 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:58.562266: step 10640, loss = 0.69 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:58.883939: step 10650, loss = 0.74 (7854.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:59.206558: step 10660, loss = 0.69 (8036.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:59.528281: step 10670, loss = 0.62 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:45:59.850940: step 10680, loss = 0.69 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:00.173154: step 10690, loss = 0.57 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:00.501532: step 10700, loss = 0.62 (7575.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:46:00.968303: step 10710, loss = 0.82 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:01.290770: step 10720, loss = 0.78 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:01.613684: step 10730, loss = 0.65 (7796.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:01.936743: step 10740, loss = 0.90 (7965.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:02.260065: step 10750, loss = 0.77 (8146.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:02.582170: step 10760, loss = 0.67 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:02.908546: step 10770, loss = 0.87 (7767.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:03.230672: step 10780, loss = 0.82 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:03.552103: step 10790, loss = 0.71 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:03.875854: step 10800, loss = 0.66 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:04.337572: step 10810, loss = 0.63 (7929.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:04.660129: step 10820, loss = 0.89 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:04.983096: step 10830, loss = 0.79 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:05.304868: step 10840, loss = 0.68 (7837.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:05.627731: step 10850, loss = 0.78 (7827.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:05.949508: step 10860, loss = 0.85 (7898.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:06.271726: step 10870, loss = 0.70 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:06.597440: step 10880, loss = 0.83 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:06.920806: step 10890, loss = 0.82 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:07.242743: step 10900, loss = 0.95 (7823.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:07.700014: step 10910, loss = 0.80 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:08.024979: step 10920, loss = 0.85 (7765.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:08.347040: step 10930, loss = 0.74 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:08.669597: step 10940, loss = 0.87 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:08.991787: step 10950, loss = 0.82 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:09.315632: step 10960, loss = 0.89 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:09.637205: step 10970, loss = 0.73 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:09.959542: step 10980, loss = 0.78 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:10.282684: step 10990, loss = 0.77 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:10.603980: step 11000, loss = 0.73 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:11.172499: step 11010, loss = 0.85 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:11.496638: step 11020, loss = 0.63 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:11.816708: step 11030, loss = 0.79 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:12.140616: step 11040, loss = 0.75 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:12.463530: step 11050, loss = 0.82 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:12.789176: step 11060, loss = 0.76 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:13.111004: step 11070, loss = 0.80 (8120.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:13.431032: step 11080, loss = 0.67 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:13.752770: step 11090, loss = 0.56 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:14.074359: step 11100, loss = 0.82 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:14.541690: step 11110, loss = 0.89 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:14.870849: step 11120, loss = 0.67 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:15.192120: step 11130, loss = 0.80 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:15.515670: step 11140, loss = 0.74 (7814.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:15.836238: step 11150, loss = 0.80 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:16.156933: step 11160, loss = 0.75 (7826.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:16.477892: step 11170, loss = 0.79 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:16.800140: step 11180, loss = 0.77 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:17.122899: step 11190, loss = 0.78 (7816.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:17.442932: step 11200, loss = 0.76 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:17.920312: step 11210, loss = 0.70 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:18.241939: step 11220, loss = 0.72 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:18.564226: step 11230, loss = 0.69 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:18.885624: step 11240, loss = 0.84 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:19.207463: step 11250, loss = 0.56 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:19.529717: step 11260, loss = 0.85 (7812.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:19.850474: step 11270, loss = 0.73 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:20.172035: step 11280, loss = 0.77 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:20.498192: step 11290, loss = 0.82 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:20.820338: step 11300, loss = 0.64 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:21.286780: step 11310, loss = 0.86 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:21.606360: step 11320, loss = 0.77 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:21.932568: step 11330, loss = 0.76 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:22.255921: step 11340, loss = 0.94 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:22.576275: step 11350, loss = 0.73 (8113.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:22.896078: step 11360, loss = 0.86 (8122.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:23.217028: step 11370, loss = 0.73 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:23.539026: step 11380, loss = 0.67 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:23.862955: step 11390, loss = 0.77 (7868.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:24.187437: step 11400, loss = 0.80 (7883.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:24.645186: step 11410, loss = 0.73 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:24.966966: step 11420, loss = 0.70 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:25.288215: step 11430, loss = 0.80 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:25.609232: step 11440, loss = 0.92 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:25.931521: step 11450, loss = 0.83 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:26.255872: step 11460, loss = 0.68 (7912.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:26.580132: step 11470, loss = 0.88 (8005.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:26.901603: step 11480, loss = 0.68 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:27.223792: step 11490, loss = 0.66 (7859.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:27.545505: step 11500, loss = 0.61 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:28.014621: step 11510, loss = 0.81 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:28.336605: step 11520, loss = 0.62 (7922.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:28.662493: step 11530, loss = 0.78 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:28.985523: step 11540, loss = 0.73 (7822.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:29.308031: step 11550, loss = 0.80 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:29.630934: step 11560, loss = 0.85 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:29.951829: step 11570, loss = 0.60 (7916.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:30.274006: step 11580, loss = 0.73 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:30.598136: step 11590, loss = 0.69 (7859.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:30.920280: step 11600, loss = 0.91 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:31.371709: step 11610, loss = 0.70 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:31.695024: step 11620, loss = 0.58 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:32.024383: step 11630, loss = 0.80 (6228.4 examples/sec; 0.021 sec/batch)
2017-09-16 15:46:32.345517: step 11640, loss = 0.69 (8145.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:32.668393: step 11650, loss = 0.75 (7689.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:46:32.992918: step 11660, loss = 0.69 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:33.312466: step 11670, loss = 0.72 (7909.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:33.633959: step 11680, loss = 0.69 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:33.959452: step 11690, loss = 0.72 (7652.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:46:34.281233: step 11700, loss = 0.82 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:34.744088: step 11710, loss = 0.72 (8182.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:35.069107: step 11720, loss = 0.73 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:35.390774: step 11730, loss = 0.76 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:35.710891: step 11740, loss = 1.11 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:36.032220: step 11750, loss = 0.94 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:36.352456: step 11760, loss = 0.65 (8080.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:36.676770: step 11770, loss = 0.77 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:36.999827: step 11780, loss = 0.74 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:37.326008: step 11790, loss = 0.91 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:37.650019: step 11800, loss = 0.69 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:38.106223: step 11810, loss = 0.82 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:38.426584: step 11820, loss = 0.79 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:38.749540: step 11830, loss = 0.63 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:39.076686: step 11840, loss = 0.85 (7954.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:39.401536: step 11850, loss = 0.67 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:39.726030: step 11860, loss = 0.69 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:40.048370: step 11870, loss = 0.88 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:40.373173: step 11880, loss = 0.70 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:40.696329: step 11890, loss = 0.78 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:41.018853: step 11900, loss = 0.59 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:41.478799: step 11910, loss = 0.71 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:41.800424: step 11920, loss = 0.81 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:42.120908: step 11930, loss = 0.80 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:42.446164: step 11940, loss = 0.79 (7798.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:42.767729: step 11950, loss = 0.77 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:43.089085: step 11960, loss = 0.76 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:43.413674: step 11970, loss = 0.73 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:43.734429: step 11980, loss = 0.88 (7946.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:44.056975: step 11990, loss = 0.71 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:44.384857: step 12000, loss = 0.88 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:44.961575: step 12010, loss = 0.87 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:45.284959: step 12020, loss = 0.95 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:45.609025: step 12030, loss = 0.80 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:45.932883: step 12040, loss = 0.68 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:46.253975: step 12050, loss = 0.64 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:46.575728: step 12060, loss = 0.71 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:46.897504: step 12070, loss = 0.78 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:47.219220: step 12080, loss = 0.59 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:47.542316: step 12090, loss = 0.76 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:47.865007: step 12100, loss = 0.73 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:48.338023: step 12110, loss = 0.76 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:48.660295: step 12120, loss = 0.86 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:48.981487: step 12130, loss = 0.79 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:49.308913: step 12140, loss = 0.75 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:49.630430: step 12150, loss = 0.66 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:49.952153: step 12160, loss = 0.73 (7863.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:50.275832: step 12170, loss = 0.76 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:50.597980: step 12180, loss = 0.72 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:50.921369: step 12190, loss = 0.89 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:51.240295: step 12200, loss = 0.63 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:51.695607: step 12210, loss = 0.69 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:52.019977: step 12220, loss = 0.73 (7801.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:52.341489: step 12230, loss = 0.77 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:52.662896: step 12240, loss = 0.79 (7855.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:52.986155: step 12250, loss = 0.73 (7791.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:53.307702: step 12260, loss = 0.76 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:53.628611: step 12270, loss = 0.70 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:53.951503: step 12280, loss = 0.63 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:54.274314: step 12290, loss = 0.61 (7494.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:46:54.596736: step 12300, loss = 0.78 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:55.063681: step 12310, loss = 0.89 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:55.384519: step 12320, loss = 0.79 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:55.707332: step 12330, loss = 0.75 (7922.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:56.030111: step 12340, loss = 0.76 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:56.350912: step 12350, loss = 0.79 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:56.674150: step 12360, loss = 0.71 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:56.996904: step 12370, loss = 0.74 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:57.317289: step 12380, loss = 0.75 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:57.640691: step 12390, loss = 0.60 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:57.964723: step 12400, loss = 0.81 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:58.432228: step 12410, loss = 0.69 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:58.756245: step 12420, loss = 0.76 (7493.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:46:59.078611: step 12430, loss = 0.76 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:59.401851: step 12440, loss = 0.79 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:46:59.725837: step 12450, loss = 0.84 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:00.048416: step 12460, loss = 0.79 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:00.369255: step 12470, loss = 0.71 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:00.691300: step 12480, loss = 0.89 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:01.012895: step 12490, loss = 0.90 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:01.333518: step 12500, loss = 0.76 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:01.786480: step 12510, loss = 0.82 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:02.110436: step 12520, loss = 0.81 (7911.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:02.433191: step 12530, loss = 0.78 (7810.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:02.753328: step 12540, loss = 0.70 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:03.076231: step 12550, loss = 0.75 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:03.399946: step 12560, loss = 0.65 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:03.723886: step 12570, loss = 0.69 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:04.046761: step 12580, loss = 0.76 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:04.369974: step 12590, loss = 0.71 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:04.694116: step 12600, loss = 0.81 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:05.159754: step 12610, loss = 0.69 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:05.482865: step 12620, loss = 0.73 (7851.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:05.805487: step 12630, loss = 0.75 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:06.127319: step 12640, loss = 0.82 (7931.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:06.449925: step 12650, loss = 0.77 (7825.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:06.773758: step 12660, loss = 0.69 (7779.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:07.099147: step 12670, loss = 0.60 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:07.422200: step 12680, loss = 0.71 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:07.745770: step 12690, loss = 0.91 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:08.069344: step 12700, loss = 0.74 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:08.528458: step 12710, loss = 0.68 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:08.852394: step 12720, loss = 0.61 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:09.175433: step 12730, loss = 0.86 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:09.500409: step 12740, loss = 0.78 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:09.823645: step 12750, loss = 0.69 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:10.145107: step 12760, loss = 0.89 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:10.468729: step 12770, loss = 0.73 (7884.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:10.788712: step 12780, loss = 0.69 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:11.109181: step 12790, loss = 0.68 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:11.430272: step 12800, loss = 0.75 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:11.884635: step 12810, loss = 0.80 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:12.207402: step 12820, loss = 0.63 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:12.532850: step 12830, loss = 0.88 (7802.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:12.858904: step 12840, loss = 0.62 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:13.180796: step 12850, loss = 0.87 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:13.503349: step 12860, loss = 0.74 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:13.825363: step 12870, loss = 1.03 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:14.148263: step 12880, loss = 0.79 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:14.468075: step 12890, loss = 0.70 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:14.789944: step 12900, loss = 0.88 (7829.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:15.254877: step 12910, loss = 0.78 (7454.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:15.578087: step 12920, loss = 0.74 (7816.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:15.901658: step 12930, loss = 0.80 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:16.224211: step 12940, loss = 0.80 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:16.544274: step 12950, loss = 0.78 (7969.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:16.867039: step 12960, loss = 0.69 (7824.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:17.186978: step 12970, loss = 0.64 (7868.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:17.510039: step 12980, loss = 0.71 (7806.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:17.831397: step 12990, loss = 0.73 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:18.153949: step 13000, loss = 0.75 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:18.701184: step 13010, loss = 0.62 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:19.027887: step 13020, loss = 0.86 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:19.351065: step 13030, loss = 0.81 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:19.676223: step 13040, loss = 0.74 (7836.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:19.997804: step 13050, loss = 0.63 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:20.318663: step 13060, loss = 0.73 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:20.643178: step 13070, loss = 0.79 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:20.964487: step 13080, loss = 0.63 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:21.287264: step 13090, loss = 0.66 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:21.611345: step 13100, loss = 0.74 (7922.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:22.072411: step 13110, loss = 0.98 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:22.392974: step 13120, loss = 0.73 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:22.713998: step 13130, loss = 0.77 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:23.034386: step 13140, loss = 0.80 (7785.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:23.357723: step 13150, loss = 0.91 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:23.680928: step 13160, loss = 0.67 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:24.004331: step 13170, loss = 0.72 (8010.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:24.324206: step 13180, loss = 0.81 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:24.648249: step 13190, loss = 0.69 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:24.972151: step 13200, loss = 0.72 (8037.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:25.437464: step 13210, loss = 0.78 (7688.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:25.760270: step 13220, loss = 0.66 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:26.084665: step 13230, loss = 0.88 (7841.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:26.406526: step 13240, loss = 0.77 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:26.731300: step 13250, loss = 1.09 (7494.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:27.052489: step 13260, loss = 0.66 (7763.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:27.373336: step 13270, loss = 0.75 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:27.696387: step 13280, loss = 0.75 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:28.017029: step 13290, loss = 0.70 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:28.338667: step 13300, loss = 0.82 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:28.805992: step 13310, loss = 0.68 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:29.127518: step 13320, loss = 0.74 (7978.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:29.452417: step 13330, loss = 0.66 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:29.774822: step 13340, loss = 0.78 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:30.096326: step 13350, loss = 0.64 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:30.418392: step 13360, loss = 0.80 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:30.740089: step 13370, loss = 0.69 (7824.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:31.062266: step 13380, loss = 0.66 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:31.381800: step 13390, loss = 0.84 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:31.704614: step 13400, loss = 0.68 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:32.166712: step 13410, loss = 0.77 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:32.489011: step 13420, loss = 0.62 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:32.812068: step 13430, loss = 0.69 (7773.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:33.134746: step 13440, loss = 0.70 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:33.456651: step 13450, loss = 0.81 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:33.780496: step 13460, loss = 0.85 (7992.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:34.103393: step 13470, loss = 0.73 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:34.425214: step 13480, loss = 0.65 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:34.746846: step 13490, loss = 0.78 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:35.069720: step 13500, loss = 0.68 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:35.534663: step 13510, loss = 0.87 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:35.861623: step 13520, loss = 0.77 (8024.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:36.187471: step 13530, loss = 0.73 (7617.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:36.508849: step 13540, loss = 0.81 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:36.832436: step 13550, loss = 0.65 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:37.154948: step 13560, loss = 0.74 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:37.480661: step 13570, loss = 0.86 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:37.801314: step 13580, loss = 0.64 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:38.124170: step 13590, loss = 0.67 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:38.448625: step 13600, loss = 0.66 (7828.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:38.906276: step 13610, loss = 0.62 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:39.229129: step 13620, loss = 0.78 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:39.552095: step 13630, loss = 0.79 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:39.873342: step 13640, loss = 0.65 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:40.194605: step 13650, loss = 0.63 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:40.516753: step 13660, loss = 0.56 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:40.839016: step 13670, loss = 0.72 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:41.162587: step 13680, loss = 0.88 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:41.484308: step 13690, loss = 0.69 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:41.806718: step 13700, loss = 0.61 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:42.266998: step 13710, loss = 0.82 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:42.586974: step 13720, loss = 0.70 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:42.909133: step 13730, loss = 0.73 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:43.232151: step 13740, loss = 0.62 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:43.553125: step 13750, loss = 0.76 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:43.873641: step 13760, loss = 0.62 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:44.196204: step 13770, loss = 0.78 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:44.518702: step 13780, loss = 0.75 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:44.841285: step 13790, loss = 0.82 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:45.165426: step 13800, loss = 0.86 (7900.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:45.618953: step 13810, loss = 0.84 (7913.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:45.939704: step 13820, loss = 0.66 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:46.259008: step 13830, loss = 0.60 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:46.581720: step 13840, loss = 0.74 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:46.904526: step 13850, loss = 0.79 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:47.225534: step 13860, loss = 0.71 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:47.546302: step 13870, loss = 0.71 (7997.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:47.869998: step 13880, loss = 0.79 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:48.192462: step 13890, loss = 0.72 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:48.513942: step 13900, loss = 0.68 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:48.985681: step 13910, loss = 0.78 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:49.307973: step 13920, loss = 0.58 (7771.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:49.630540: step 13930, loss = 0.75 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:49.951991: step 13940, loss = 0.57 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:50.273949: step 13950, loss = 0.69 (7892.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:50.594880: step 13960, loss = 0.69 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:50.915886: step 13970, loss = 0.62 (7820.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:51.238278: step 13980, loss = 0.52 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:51.558956: step 13990, loss = 0.78 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:51.883493: step 14000, loss = 0.68 (7712.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:52.450396: step 14010, loss = 0.59 (7474.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:47:52.772093: step 14020, loss = 0.84 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:53.093580: step 14030, loss = 0.95 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:53.416456: step 14040, loss = 0.72 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:53.740033: step 14050, loss = 0.78 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:54.059402: step 14060, loss = 0.58 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:54.381177: step 14070, loss = 0.73 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:54.704590: step 14080, loss = 0.70 (7996.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:55.026554: step 14090, loss = 0.63 (7781.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:55.348197: step 14100, loss = 0.64 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:55.805535: step 14110, loss = 0.63 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:56.125949: step 14120, loss = 0.76 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:56.449141: step 14130, loss = 0.86 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:56.770555: step 14140, loss = 0.71 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:57.092333: step 14150, loss = 0.73 (8155.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:57.410650: step 14160, loss = 0.60 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:57.731704: step 14170, loss = 0.63 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:58.052620: step 14180, loss = 0.64 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:58.375948: step 14190, loss = 0.68 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:58.697132: step 14200, loss = 0.89 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:59.163805: step 14210, loss = 0.80 (7815.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:59.485350: step 14220, loss = 0.70 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:47:59.807349: step 14230, loss = 0.72 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:00.130453: step 14240, loss = 0.71 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:00.448728: step 14250, loss = 0.74 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:00.771027: step 14260, loss = 0.77 (7940.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:01.091454: step 14270, loss = 0.77 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:01.413219: step 14280, loss = 0.67 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:01.734812: step 14290, loss = 0.79 (7746.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:02.055850: step 14300, loss = 0.82 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:02.521095: step 14310, loss = 0.73 (7934.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:02.842400: step 14320, loss = 0.60 (7679.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:03.167624: step 14330, loss = 0.77 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:03.488811: step 14340, loss = 0.81 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:03.814611: step 14350, loss = 0.64 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:04.136319: step 14360, loss = 0.74 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:04.458820: step 14370, loss = 0.66 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:04.781169: step 14380, loss = 0.71 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:05.102803: step 14390, loss = 0.69 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:05.428820: step 14400, loss = 0.65 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:05.896360: step 14410, loss = 0.65 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:06.218843: step 14420, loss = 0.79 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:06.538588: step 14430, loss = 0.73 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:06.870561: step 14440, loss = 0.72 (7883.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:07.198011: step 14450, loss = 0.64 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:07.519825: step 14460, loss = 0.70 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:07.844962: step 14470, loss = 0.65 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:08.168593: step 14480, loss = 0.66 (7844.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:08.490973: step 14490, loss = 0.82 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:08.812543: step 14500, loss = 0.60 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:09.276457: step 14510, loss = 0.70 (7862.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:09.596991: step 14520, loss = 0.85 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:09.919827: step 14530, loss = 0.72 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:10.242135: step 14540, loss = 0.87 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:10.569047: step 14550, loss = 0.70 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:10.890289: step 14560, loss = 0.65 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:11.210972: step 14570, loss = 0.70 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:11.533150: step 14580, loss = 0.62 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:11.853574: step 14590, loss = 0.71 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:12.176242: step 14600, loss = 0.67 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:12.637721: step 14610, loss = 0.83 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:12.961059: step 14620, loss = 0.72 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:13.283139: step 14630, loss = 0.80 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:13.604548: step 14640, loss = 0.68 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:13.925467: step 14650, loss = 0.75 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:14.246622: step 14660, loss = 0.77 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:14.568600: step 14670, loss = 0.72 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:14.890707: step 14680, loss = 0.93 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:15.210089: step 14690, loss = 0.70 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:15.530802: step 14700, loss = 0.72 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:15.999424: step 14710, loss = 0.61 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:16.318819: step 14720, loss = 0.85 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:16.640232: step 14730, loss = 0.67 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:16.961132: step 14740, loss = 0.60 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:17.280303: step 14750, loss = 0.73 (8130.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:17.604488: step 14760, loss = 0.58 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:17.926662: step 14770, loss = 0.64 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:18.249219: step 14780, loss = 0.70 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:18.570773: step 14790, loss = 0.64 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:18.894222: step 14800, loss = 0.72 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:19.361856: step 14810, loss = 0.78 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:19.684828: step 14820, loss = 0.84 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:20.006898: step 14830, loss = 0.76 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:20.330477: step 14840, loss = 0.81 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:20.653979: step 14850, loss = 0.73 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:20.981032: step 14860, loss = 0.71 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:21.305164: step 14870, loss = 0.72 (7936.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:21.626166: step 14880, loss = 0.75 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:21.951847: step 14890, loss = 0.78 (8164.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:22.272840: step 14900, loss = 0.88 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:22.738584: step 14910, loss = 0.80 (7872.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:23.067604: step 14920, loss = 0.71 (7912.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:23.391040: step 14930, loss = 0.82 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:23.715616: step 14940, loss = 0.61 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:24.039654: step 14950, loss = 0.58 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:24.361035: step 14960, loss = 0.61 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:24.685178: step 14970, loss = 0.76 (7908.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:25.009412: step 14980, loss = 0.81 (7884.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:25.333544: step 14990, loss = 0.68 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:25.656699: step 15000, loss = 0.80 (7570.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:26.282467: step 15010, loss = 0.81 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:26.606458: step 15020, loss = 0.72 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:26.927191: step 15030, loss = 0.62 (8074.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:27.247882: step 15040, loss = 0.78 (7887.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:27.569590: step 15050, loss = 0.57 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:27.889990: step 15060, loss = 0.88 (7720.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:28.210738: step 15070, loss = 0.64 (7927.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:28.535486: step 15080, loss = 0.69 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:28.858362: step 15090, loss = 0.55 (7873.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:29.179971: step 15100, loss = 0.75 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:29.647969: step 15110, loss = 0.66 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:29.971779: step 15120, loss = 0.76 (7622.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:30.292998: step 15130, loss = 0.71 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:30.616580: step 15140, loss = 0.68 (7475.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:30.938125: step 15150, loss = 0.57 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:31.262489: step 15160, loss = 0.81 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:31.587026: step 15170, loss = 0.60 (7825.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:31.912449: step 15180, loss = 0.62 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:32.232692: step 15190, loss = 0.74 (7897.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:32.553830: step 15200, loss = 0.80 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:33.025284: step 15210, loss = 0.65 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:33.348589: step 15220, loss = 0.67 (7810.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:33.672840: step 15230, loss = 0.84 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:33.999492: step 15240, loss = 0.66 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:34.321815: step 15250, loss = 0.75 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:34.640860: step 15260, loss = 0.68 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:34.963495: step 15270, loss = 0.80 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:35.285537: step 15280, loss = 0.75 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:35.608901: step 15290, loss = 0.72 (7802.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:35.929719: step 15300, loss = 0.75 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:36.390504: step 15310, loss = 0.62 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:36.714327: step 15320, loss = 0.80 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:37.035320: step 15330, loss = 0.71 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:37.357627: step 15340, loss = 0.62 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:37.680072: step 15350, loss = 0.75 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:38.004772: step 15360, loss = 0.83 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:38.327709: step 15370, loss = 0.76 (7821.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:38.650378: step 15380, loss = 0.81 (7779.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:38.973580: step 15390, loss = 0.65 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:39.294508: step 15400, loss = 0.69 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:39.756067: step 15410, loss = 0.85 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:40.078121: step 15420, loss = 0.67 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:40.401099: step 15430, loss = 0.66 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:40.722682: step 15440, loss = 0.66 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:41.046661: step 15450, loss = 0.71 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:41.369402: step 15460, loss = 0.70 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:41.691609: step 15470, loss = 0.72 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:42.015056: step 15480, loss = 0.74 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:42.336203: step 15490, loss = 0.69 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:42.657484: step 15500, loss = 0.76 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:43.120153: step 15510, loss = 0.64 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:43.445326: step 15520, loss = 0.74 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:43.766387: step 15530, loss = 0.69 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:44.091615: step 15540, loss = 0.69 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:44.413926: step 15550, loss = 0.69 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:44.735258: step 15560, loss = 0.65 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:45.056516: step 15570, loss = 0.74 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:45.376589: step 15580, loss = 0.66 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:45.699341: step 15590, loss = 0.65 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:46.020603: step 15600, loss = 0.60 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:46.484755: step 15610, loss = 0.66 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:46.804614: step 15620, loss = 0.84 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:47.128259: step 15630, loss = 0.85 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:47.450926: step 15640, loss = 0.60 (7905.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:47.769778: step 15650, loss = 0.59 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:48.091552: step 15660, loss = 0.73 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:48.419271: step 15670, loss = 0.83 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:48.739655: step 15680, loss = 0.84 (7808.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:49.061405: step 15690, loss = 0.67 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:49.383903: step 15700, loss = 0.55 (7941.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:49.846571: step 15710, loss = 0.58 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:50.170530: step 15720, loss = 0.63 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:50.491869: step 15730, loss = 0.85 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:50.812995: step 15740, loss = 0.76 (8125.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:51.135923: step 15750, loss = 0.70 (8143.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:51.458491: step 15760, loss = 0.76 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:51.781796: step 15770, loss = 0.62 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:52.100782: step 15780, loss = 0.66 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:52.424283: step 15790, loss = 0.79 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:52.747985: step 15800, loss = 0.75 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:53.213632: step 15810, loss = 0.75 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:53.536420: step 15820, loss = 0.67 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:53.858045: step 15830, loss = 0.89 (7747.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:48:54.180566: step 15840, loss = 0.73 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:54.502559: step 15850, loss = 0.81 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:54.825039: step 15860, loss = 0.66 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:55.147707: step 15870, loss = 0.69 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:55.471553: step 15880, loss = 0.74 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:55.794048: step 15890, loss = 0.70 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:56.114855: step 15900, loss = 0.76 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:56.568908: step 15910, loss = 0.74 (7834.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:56.891753: step 15920, loss = 0.68 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:57.212692: step 15930, loss = 0.70 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:57.533670: step 15940, loss = 0.65 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:57.856335: step 15950, loss = 0.70 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:58.178835: step 15960, loss = 0.63 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:58.500072: step 15970, loss = 0.80 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:58.825047: step 15980, loss = 0.67 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:59.147858: step 15990, loss = 0.74 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:48:59.469784: step 16000, loss = 0.56 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:00.028376: step 16010, loss = 0.65 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:00.352814: step 16020, loss = 0.65 (7825.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:00.674489: step 16030, loss = 0.67 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:00.995808: step 16040, loss = 0.68 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:01.318895: step 16050, loss = 0.70 (7940.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:01.641399: step 16060, loss = 0.82 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:01.966387: step 16070, loss = 0.70 (8010.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:02.288856: step 16080, loss = 0.62 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:02.614307: step 16090, loss = 0.70 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:02.934920: step 16100, loss = 0.65 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:03.402929: step 16110, loss = 0.80 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:03.723223: step 16120, loss = 0.79 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:04.045149: step 16130, loss = 0.71 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:04.367304: step 16140, loss = 0.85 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:04.692803: step 16150, loss = 0.67 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:05.014215: step 16160, loss = 0.76 (7995.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:05.336932: step 16170, loss = 0.63 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:05.658575: step 16180, loss = 0.78 (7858.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:05.979222: step 16190, loss = 0.70 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:06.302797: step 16200, loss = 0.78 (7854.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:06.763231: step 16210, loss = 0.83 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:07.084048: step 16220, loss = 0.75 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:07.404951: step 16230, loss = 0.64 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:07.727998: step 16240, loss = 0.73 (7873.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:08.048631: step 16250, loss = 0.69 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:08.373056: step 16260, loss = 0.60 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:08.700856: step 16270, loss = 0.58 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:09.024600: step 16280, loss = 0.53 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:09.349308: step 16290, loss = 0.83 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:09.667572: step 16300, loss = 0.71 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:10.126649: step 16310, loss = 0.75 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:10.447881: step 16320, loss = 0.75 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:10.767538: step 16330, loss = 0.60 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:11.093507: step 16340, loss = 0.66 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:11.417444: step 16350, loss = 0.79 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:11.738927: step 16360, loss = 0.56 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:12.062603: step 16370, loss = 0.74 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:12.385273: step 16380, loss = 0.92 (8144.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:12.706843: step 16390, loss = 0.68 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:13.028540: step 16400, loss = 0.56 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:13.488225: step 16410, loss = 0.65 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:13.809359: step 16420, loss = 0.71 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:14.132539: step 16430, loss = 0.79 (7444.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:49:14.454891: step 16440, loss = 0.82 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:14.775945: step 16450, loss = 0.82 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:15.095928: step 16460, loss = 0.74 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:15.415908: step 16470, loss = 0.66 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:15.736830: step 16480, loss = 0.71 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:16.060457: step 16490, loss = 0.73 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:16.379929: step 16500, loss = 0.66 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:16.845115: step 16510, loss = 0.77 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:17.167985: step 16520, loss = 0.72 (7934.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:17.490033: step 16530, loss = 0.66 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:17.812313: step 16540, loss = 0.54 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:18.132502: step 16550, loss = 0.71 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:18.456042: step 16560, loss = 0.70 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:18.779656: step 16570, loss = 0.87 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:19.099352: step 16580, loss = 0.78 (8143.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:19.424470: step 16590, loss = 0.62 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:19.745818: step 16600, loss = 0.70 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:20.198625: step 16610, loss = 0.83 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:20.520400: step 16620, loss = 0.69 (7896.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:20.842444: step 16630, loss = 0.60 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:21.161372: step 16640, loss = 0.63 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:21.483687: step 16650, loss = 0.68 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:21.804621: step 16660, loss = 0.82 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:22.125217: step 16670, loss = 0.53 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:22.449099: step 16680, loss = 0.66 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:22.770038: step 16690, loss = 0.96 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:23.090209: step 16700, loss = 0.68 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:23.546956: step 16710, loss = 0.78 (8115.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:23.869874: step 16720, loss = 0.75 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:24.190547: step 16730, loss = 0.78 (8044.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:24.513329: step 16740, loss = 0.64 (7596.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:49:24.837652: step 16750, loss = 0.64 (7811.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:25.159133: step 16760, loss = 0.62 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:25.479940: step 16770, loss = 0.70 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:25.800369: step 16780, loss = 0.80 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:26.123671: step 16790, loss = 0.62 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:26.444094: step 16800, loss = 0.85 (7857.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:26.914350: step 16810, loss = 0.76 (7538.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:49:27.236098: step 16820, loss = 0.64 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:27.555915: step 16830, loss = 0.76 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:27.875828: step 16840, loss = 0.78 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:28.196871: step 16850, loss = 0.73 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:28.520530: step 16860, loss = 0.73 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:28.840672: step 16870, loss = 0.72 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:29.162580: step 16880, loss = 0.69 (7841.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:29.483213: step 16890, loss = 0.78 (7896.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:29.803600: step 16900, loss = 0.76 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:30.261254: step 16910, loss = 0.73 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:30.587092: step 16920, loss = 0.55 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:30.913444: step 16930, loss = 0.84 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:31.235565: step 16940, loss = 0.72 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:31.555298: step 16950, loss = 0.65 (7980.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:31.876913: step 16960, loss = 0.61 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:32.197890: step 16970, loss = 0.86 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:32.519835: step 16980, loss = 0.81 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:32.842232: step 16990, loss = 0.75 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:33.163581: step 17000, loss = 0.89 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:33.727339: step 17010, loss = 0.68 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:34.048165: step 17020, loss = 0.57 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:34.369544: step 17030, loss = 0.71 (7783.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:34.691517: step 17040, loss = 0.75 (7793.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:35.012719: step 17050, loss = 0.74 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:35.331471: step 17060, loss = 0.62 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:35.652657: step 17070, loss = 0.62 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:35.971766: step 17080, loss = 0.66 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:36.292594: step 17090, loss = 0.66 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:36.614123: step 17100, loss = 0.81 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:37.070155: step 17110, loss = 0.68 (8138.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:37.393017: step 17120, loss = 0.64 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:37.713482: step 17130, loss = 0.66 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:38.034842: step 17140, loss = 0.78 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:38.357300: step 17150, loss = 0.68 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:38.678014: step 17160, loss = 0.83 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:38.998179: step 17170, loss = 0.84 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:39.319532: step 17180, loss = 0.80 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:39.641196: step 17190, loss = 0.79 (7935.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:39.962959: step 17200, loss = 0.89 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:40.431303: step 17210, loss = 0.70 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:40.753456: step 17220, loss = 0.70 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:41.074132: step 17230, loss = 0.69 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:41.395571: step 17240, loss = 0.62 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:41.716174: step 17250, loss = 0.72 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:42.036065: step 17260, loss = 0.70 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:42.359185: step 17270, loss = 0.86 (7541.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:49:42.682928: step 17280, loss = 0.83 (7856.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:43.002146: step 17290, loss = 0.72 (7977.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:43.324706: step 17300, loss = 0.68 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:43.796560: step 17310, loss = 0.78 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:44.118676: step 17320, loss = 0.69 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:44.437814: step 17330, loss = 0.65 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:44.759391: step 17340, loss = 0.69 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:45.080611: step 17350, loss = 0.75 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:45.401328: step 17360, loss = 0.63 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:45.721871: step 17370, loss = 0.65 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:46.047975: step 17380, loss = 0.81 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:46.370299: step 17390, loss = 0.69 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:46.691400: step 17400, loss = 0.66 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:47.154643: step 17410, loss = 0.66 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:47.478862: step 17420, loss = 0.80 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:47.800511: step 17430, loss = 0.67 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:48.122303: step 17440, loss = 0.72 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:48.445032: step 17450, loss = 0.70 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:48.767099: step 17460, loss = 0.63 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:49.091347: step 17470, loss = 0.69 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:49.416993: step 17480, loss = 0.70 (7802.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:49.737299: step 17490, loss = 0.76 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:50.060340: step 17500, loss = 0.67 (7764.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:50.523994: step 17510, loss = 0.65 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:50.850291: step 17520, loss = 0.75 (7871.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:51.177991: step 17530, loss = 0.87 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:51.502446: step 17540, loss = 0.68 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:51.824762: step 17550, loss = 0.57 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:52.148134: step 17560, loss = 0.80 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:52.471439: step 17570, loss = 0.86 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:52.794164: step 17580, loss = 0.68 (7809.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:53.115357: step 17590, loss = 0.69 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:53.436827: step 17600, loss = 0.69 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:53.896828: step 17610, loss = 0.68 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:54.217929: step 17620, loss = 0.73 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:54.541637: step 17630, loss = 0.71 (7922.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:54.864535: step 17640, loss = 0.68 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:55.186682: step 17650, loss = 0.65 (8139.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:55.510221: step 17660, loss = 0.63 (7947.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:55.833790: step 17670, loss = 0.64 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:56.156830: step 17680, loss = 0.78 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:56.476945: step 17690, loss = 0.74 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:56.800620: step 17700, loss = 0.75 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:57.267588: step 17710, loss = 0.66 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:57.587190: step 17720, loss = 0.66 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:57.907573: step 17730, loss = 0.67 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:58.227703: step 17740, loss = 0.69 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:58.553665: step 17750, loss = 0.62 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:58.875472: step 17760, loss = 0.59 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:59.197072: step 17770, loss = 0.57 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:59.519429: step 17780, loss = 0.69 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:49:59.843462: step 17790, loss = 0.76 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:00.166256: step 17800, loss = 0.73 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:00.635907: step 17810, loss = 0.81 (7906.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:00.957182: step 17820, loss = 0.71 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:01.281143: step 17830, loss = 0.69 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:01.603682: step 17840, loss = 0.61 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:01.922875: step 17850, loss = 0.72 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:02.244696: step 17860, loss = 0.79 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:02.565283: step 17870, loss = 0.75 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:02.889914: step 17880, loss = 0.71 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:03.210360: step 17890, loss = 0.82 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:03.530361: step 17900, loss = 0.62 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:03.984903: step 17910, loss = 0.77 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:04.306442: step 17920, loss = 0.61 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:04.629470: step 17930, loss = 0.75 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:04.952076: step 17940, loss = 0.64 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:05.275893: step 17950, loss = 0.73 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:05.598288: step 17960, loss = 0.63 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:05.920703: step 17970, loss = 0.65 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:06.245344: step 17980, loss = 1.01 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:06.566371: step 17990, loss = 0.60 (7876.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:06.886828: step 18000, loss = 0.60 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:07.441227: step 18010, loss = 0.70 (7887.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:07.763579: step 18020, loss = 0.71 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:08.085613: step 18030, loss = 0.60 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:08.408557: step 18040, loss = 0.68 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:08.733153: step 18050, loss = 0.64 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:09.053727: step 18060, loss = 0.77 (7876.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:09.374097: step 18070, loss = 0.82 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:09.693052: step 18080, loss = 0.62 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:10.013647: step 18090, loss = 0.81 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:10.336371: step 18100, loss = 0.83 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:10.798460: step 18110, loss = 0.70 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:11.120731: step 18120, loss = 0.79 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:11.443621: step 18130, loss = 0.77 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:11.766767: step 18140, loss = 0.72 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:12.088616: step 18150, loss = 0.71 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:12.409650: step 18160, loss = 0.69 (8168.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:12.731408: step 18170, loss = 0.65 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:13.053889: step 18180, loss = 0.76 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:13.377436: step 18190, loss = 0.87 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:13.700335: step 18200, loss = 0.67 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:14.165494: step 18210, loss = 0.67 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:14.485668: step 18220, loss = 0.66 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:14.806890: step 18230, loss = 0.63 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:15.127774: step 18240, loss = 0.59 (7804.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:15.452698: step 18250, loss = 0.76 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:15.772771: step 18260, loss = 0.62 (8140.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:16.096099: step 18270, loss = 0.67 (7735.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:50:16.417321: step 18280, loss = 0.71 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:16.739611: step 18290, loss = 0.68 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:17.061129: step 18300, loss = 0.69 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:17.526313: step 18310, loss = 0.67 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:17.849746: step 18320, loss = 0.81 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:18.170882: step 18330, loss = 0.63 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:18.493931: step 18340, loss = 0.71 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:18.815508: step 18350, loss = 0.79 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:19.139449: step 18360, loss = 0.70 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:19.460977: step 18370, loss = 0.71 (7977.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:19.783698: step 18380, loss = 0.84 (7862.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:20.107029: step 18390, loss = 0.69 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:20.427592: step 18400, loss = 0.63 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:20.888400: step 18410, loss = 0.68 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:21.208823: step 18420, loss = 0.67 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:21.529885: step 18430, loss = 0.80 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:21.851336: step 18440, loss = 0.67 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:22.174144: step 18450, loss = 0.74 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:22.492887: step 18460, loss = 0.66 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:22.814757: step 18470, loss = 0.68 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:23.135510: step 18480, loss = 0.70 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:23.462318: step 18490, loss = 0.82 (7532.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:50:23.785643: step 18500, loss = 0.76 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:24.248968: step 18510, loss = 0.71 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:24.573731: step 18520, loss = 0.64 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:24.899639: step 18530, loss = 0.65 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:25.224069: step 18540, loss = 0.58 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:25.547975: step 18550, loss = 0.80 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:25.873977: step 18560, loss = 0.75 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:26.198180: step 18570, loss = 0.59 (7613.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:50:26.518888: step 18580, loss = 0.80 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:26.841465: step 18590, loss = 0.83 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:27.163077: step 18600, loss = 0.63 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:27.628021: step 18610, loss = 0.56 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:27.949526: step 18620, loss = 0.72 (7808.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:28.275418: step 18630, loss = 0.73 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:28.599126: step 18640, loss = 0.64 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:28.924688: step 18650, loss = 0.68 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:29.247728: step 18660, loss = 0.71 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:29.570712: step 18670, loss = 0.69 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:29.891253: step 18680, loss = 0.73 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:30.213972: step 18690, loss = 0.81 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:30.536250: step 18700, loss = 0.60 (7774.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:30.996927: step 18710, loss = 0.75 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:31.320951: step 18720, loss = 0.79 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:31.644574: step 18730, loss = 0.69 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:31.965676: step 18740, loss = 0.68 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:32.289757: step 18750, loss = 0.74 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:32.613134: step 18760, loss = 0.54 (7785.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:32.935393: step 18770, loss = 0.92 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:33.265623: step 18780, loss = 0.76 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:33.585714: step 18790, loss = 0.79 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:33.909991: step 18800, loss = 0.67 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:34.370654: step 18810, loss = 0.74 (7779.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:34.691326: step 18820, loss = 0.81 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:35.012606: step 18830, loss = 0.75 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:35.333170: step 18840, loss = 0.69 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:35.659316: step 18850, loss = 0.67 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:35.984330: step 18860, loss = 0.68 (7505.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:50:36.307516: step 18870, loss = 0.71 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:36.628333: step 18880, loss = 0.67 (7856.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:36.950601: step 18890, loss = 0.74 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:37.272448: step 18900, loss = 0.79 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:37.741409: step 18910, loss = 0.96 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:38.064837: step 18920, loss = 0.65 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:38.390830: step 18930, loss = 0.67 (7305.1 examples/sec; 0.018 sec/batch)
2017-09-16 15:50:38.712431: step 18940, loss = 0.72 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:39.033831: step 18950, loss = 0.71 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:39.355550: step 18960, loss = 0.62 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:39.679613: step 18970, loss = 0.57 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:40.001200: step 18980, loss = 0.77 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:40.323912: step 18990, loss = 0.59 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:40.647511: step 19000, loss = 0.72 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:41.205577: step 19010, loss = 0.70 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:41.527852: step 19020, loss = 0.82 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:41.849969: step 19030, loss = 0.50 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:42.173218: step 19040, loss = 0.91 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:42.492101: step 19050, loss = 0.64 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:42.810730: step 19060, loss = 0.77 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:43.133694: step 19070, loss = 0.95 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:43.455238: step 19080, loss = 0.67 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:43.776050: step 19090, loss = 0.67 (8120.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:44.098308: step 19100, loss = 0.79 (7903.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:44.552208: step 19110, loss = 0.71 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:44.876418: step 19120, loss = 0.59 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:45.196015: step 19130, loss = 0.74 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:45.517735: step 19140, loss = 0.71 (7789.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:45.841100: step 19150, loss = 0.73 (8048.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:46.163100: step 19160, loss = 0.73 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:46.483578: step 19170, loss = 0.77 (8133.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:46.805084: step 19180, loss = 0.72 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:47.125896: step 19190, loss = 0.75 (8077.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:47.447511: step 19200, loss = 0.68 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:47.902548: step 19210, loss = 0.57 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:48.225487: step 19220, loss = 0.72 (7851.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:48.548288: step 19230, loss = 0.75 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:48.873662: step 19240, loss = 0.80 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:49.197934: step 19250, loss = 0.64 (7973.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:49.518264: step 19260, loss = 0.74 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:49.839239: step 19270, loss = 0.91 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:50.159915: step 19280, loss = 0.55 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:50.482072: step 19290, loss = 0.76 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:50.802850: step 19300, loss = 0.66 (7800.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:51.268383: step 19310, loss = 0.69 (7946.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:51.592164: step 19320, loss = 0.55 (7594.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:50:51.915252: step 19330, loss = 0.77 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:52.238554: step 19340, loss = 0.80 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:52.560494: step 19350, loss = 0.74 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:52.883593: step 19360, loss = 0.83 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:53.202973: step 19370, loss = 0.75 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:53.526480: step 19380, loss = 0.68 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:53.849141: step 19390, loss = 0.66 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:54.171062: step 19400, loss = 0.72 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:54.634773: step 19410, loss = 0.74 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:54.957134: step 19420, loss = 0.68 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:55.279928: step 19430, loss = 0.77 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:55.607706: step 19440, loss = 0.75 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:55.929863: step 19450, loss = 0.56 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:56.251370: step 19460, loss = 0.66 (7798.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:56.576207: step 19470, loss = 0.75 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:56.896766: step 19480, loss = 0.66 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:57.216659: step 19490, loss = 0.95 (7869.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:57.537483: step 19500, loss = 0.66 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:58.005336: step 19510, loss = 0.53 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:58.327248: step 19520, loss = 0.60 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:58.648822: step 19530, loss = 0.71 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:58.970806: step 19540, loss = 0.75 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:59.293543: step 19550, loss = 0.62 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:59.614570: step 19560, loss = 0.70 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:50:59.939435: step 19570, loss = 0.60 (7602.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:00.259499: step 19580, loss = 0.70 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:00.579176: step 19590, loss = 0.77 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:00.901128: step 19600, loss = 0.63 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:01.357706: step 19610, loss = 0.77 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:01.681128: step 19620, loss = 0.67 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:02.001489: step 19630, loss = 0.65 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:02.323223: step 19640, loss = 0.76 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:02.645868: step 19650, loss = 0.61 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:02.965851: step 19660, loss = 0.68 (7858.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:03.287076: step 19670, loss = 0.73 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:03.608239: step 19680, loss = 0.78 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:03.929277: step 19690, loss = 0.68 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:04.248031: step 19700, loss = 0.66 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:04.704881: step 19710, loss = 0.83 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:05.027778: step 19720, loss = 0.74 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:05.347699: step 19730, loss = 0.73 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:05.669978: step 19740, loss = 0.67 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:05.990559: step 19750, loss = 0.73 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:06.311501: step 19760, loss = 0.66 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:06.636463: step 19770, loss = 0.73 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:06.961251: step 19780, loss = 0.69 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:07.283295: step 19790, loss = 0.79 (7878.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:07.606794: step 19800, loss = 0.54 (7781.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:08.064132: step 19810, loss = 0.84 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:08.383898: step 19820, loss = 0.59 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:08.709707: step 19830, loss = 0.59 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:09.033236: step 19840, loss = 0.67 (7627.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:09.356232: step 19850, loss = 0.76 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:09.676761: step 19860, loss = 0.85 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:09.997789: step 19870, loss = 0.71 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:10.319672: step 19880, loss = 0.75 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:10.644942: step 19890, loss = 0.68 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:10.967947: step 19900, loss = 0.69 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:11.433970: step 19910, loss = 0.66 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:11.759470: step 19920, loss = 0.69 (7929.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:12.081014: step 19930, loss = 0.62 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:12.402359: step 19940, loss = 0.60 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:12.723955: step 19950, loss = 0.71 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:13.046542: step 19960, loss = 0.77 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:13.367977: step 19970, loss = 0.69 (7846.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:13.689858: step 19980, loss = 0.64 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:14.014059: step 19990, loss = 0.70 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:14.337095: step 20000, loss = 0.72 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:14.898481: step 20010, loss = 0.67 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:15.218872: step 20020, loss = 0.78 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:15.540649: step 20030, loss = 0.71 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:15.862092: step 20040, loss = 0.68 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:16.184308: step 20050, loss = 0.73 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:16.507483: step 20060, loss = 0.72 (7824.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:16.829040: step 20070, loss = 0.58 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:17.150982: step 20080, loss = 0.74 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:17.468864: step 20090, loss = 0.60 (8143.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:17.790205: step 20100, loss = 0.60 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:18.255511: step 20110, loss = 0.73 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:18.576574: step 20120, loss = 0.78 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:18.898589: step 20130, loss = 0.64 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:19.220562: step 20140, loss = 0.69 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:19.544862: step 20150, loss = 0.68 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:19.866245: step 20160, loss = 0.57 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:20.192023: step 20170, loss = 0.66 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:20.513183: step 20180, loss = 0.66 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:20.836202: step 20190, loss = 0.68 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:21.157489: step 20200, loss = 0.67 (7843.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:21.614675: step 20210, loss = 0.75 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:21.938803: step 20220, loss = 0.61 (7539.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:22.261300: step 20230, loss = 0.75 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:22.583821: step 20240, loss = 0.65 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:22.903665: step 20250, loss = 0.52 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:23.224711: step 20260, loss = 0.80 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:23.548955: step 20270, loss = 0.75 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:23.870305: step 20280, loss = 0.68 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:24.191323: step 20290, loss = 0.70 (7873.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:24.514443: step 20300, loss = 0.68 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:24.980382: step 20310, loss = 0.71 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:25.303145: step 20320, loss = 0.61 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:25.625683: step 20330, loss = 0.65 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:25.947661: step 20340, loss = 0.84 (7571.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:26.270678: step 20350, loss = 0.76 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:26.594623: step 20360, loss = 0.56 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:26.917766: step 20370, loss = 0.68 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:27.236750: step 20380, loss = 0.83 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:27.558593: step 20390, loss = 0.60 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:27.879826: step 20400, loss = 0.66 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:28.339339: step 20410, loss = 0.68 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:28.659640: step 20420, loss = 0.60 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:28.987046: step 20430, loss = 0.61 (7436.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:29.308326: step 20440, loss = 0.62 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:29.633449: step 20450, loss = 0.71 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:29.957684: step 20460, loss = 0.75 (8140.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:30.279779: step 20470, loss = 0.61 (8088.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:30.603401: step 20480, loss = 0.65 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:30.924159: step 20490, loss = 0.70 (7741.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:31.251741: step 20500, loss = 0.71 (7656.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:31.717712: step 20510, loss = 0.73 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:32.041652: step 20520, loss = 0.72 (7737.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:32.364112: step 20530, loss = 0.66 (7855.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:32.687013: step 20540, loss = 0.63 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:33.009444: step 20550, loss = 0.64 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:33.333622: step 20560, loss = 0.93 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:33.657300: step 20570, loss = 0.76 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:33.980613: step 20580, loss = 0.76 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:34.304841: step 20590, loss = 0.65 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:34.629605: step 20600, loss = 0.59 (7954.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:35.090058: step 20610, loss = 0.59 (7819.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:35.410518: step 20620, loss = 0.60 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:35.733883: step 20630, loss = 0.67 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:36.053533: step 20640, loss = 0.67 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:36.376338: step 20650, loss = 0.77 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:36.697589: step 20660, loss = 0.64 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:37.016602: step 20670, loss = 0.76 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:37.338422: step 20680, loss = 0.58 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:37.660163: step 20690, loss = 0.63 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:37.981347: step 20700, loss = 0.62 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:38.435550: step 20710, loss = 0.77 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:38.758157: step 20720, loss = 0.66 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:39.078862: step 20730, loss = 0.79 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:39.398235: step 20740, loss = 0.87 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:39.720722: step 20750, loss = 0.79 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:40.043901: step 20760, loss = 0.68 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:40.364410: step 20770, loss = 0.77 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:40.687756: step 20780, loss = 0.62 (7849.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:41.007318: step 20790, loss = 0.67 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:41.325710: step 20800, loss = 0.53 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:41.796028: step 20810, loss = 0.59 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:42.117877: step 20820, loss = 0.64 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:42.445900: step 20830, loss = 0.77 (7481.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:42.765404: step 20840, loss = 0.79 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:43.085418: step 20850, loss = 0.63 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:43.409841: step 20860, loss = 0.61 (7591.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:43.734070: step 20870, loss = 0.76 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:44.054385: step 20880, loss = 0.73 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:44.376718: step 20890, loss = 0.82 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:44.696855: step 20900, loss = 0.63 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:45.158978: step 20910, loss = 0.88 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:45.481653: step 20920, loss = 0.70 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:45.802850: step 20930, loss = 0.69 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:46.130391: step 20940, loss = 0.67 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:46.450964: step 20950, loss = 0.69 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:46.774083: step 20960, loss = 0.75 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:47.095902: step 20970, loss = 0.62 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:47.415478: step 20980, loss = 0.60 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:47.736742: step 20990, loss = 0.76 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:48.056263: step 21000, loss = 0.78 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:48.611375: step 21010, loss = 0.64 (7692.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:48.933704: step 21020, loss = 0.66 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:49.253964: step 21030, loss = 0.72 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:49.575794: step 21040, loss = 0.65 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:49.898076: step 21050, loss = 0.68 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:50.220198: step 21060, loss = 0.53 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:50.545307: step 21070, loss = 0.80 (7557.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:50.869972: step 21080, loss = 0.64 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:51.194256: step 21090, loss = 0.70 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:51.515607: step 21100, loss = 0.71 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:51.979675: step 21110, loss = 0.60 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:52.302924: step 21120, loss = 0.50 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:52.626773: step 21130, loss = 0.57 (7515.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:52.947913: step 21140, loss = 0.62 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:53.272954: step 21150, loss = 0.51 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:53.592115: step 21160, loss = 0.71 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:53.915216: step 21170, loss = 0.81 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:54.238060: step 21180, loss = 0.76 (7818.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:54.559935: step 21190, loss = 0.56 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:54.878985: step 21200, loss = 0.74 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:55.329447: step 21210, loss = 0.87 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:55.652128: step 21220, loss = 0.68 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:55.974785: step 21230, loss = 0.79 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:56.296301: step 21240, loss = 0.73 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:56.619622: step 21250, loss = 0.62 (7611.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:51:56.941839: step 21260, loss = 0.84 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:57.260619: step 21270, loss = 0.68 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:57.583243: step 21280, loss = 0.86 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:57.903163: step 21290, loss = 0.79 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:58.224732: step 21300, loss = 0.77 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:58.683981: step 21310, loss = 0.72 (7828.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:59.005462: step 21320, loss = 0.87 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:59.326414: step 21330, loss = 0.78 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:59.647974: step 21340, loss = 0.65 (7948.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:51:59.969605: step 21350, loss = 0.65 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:00.294177: step 21360, loss = 0.65 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:00.618160: step 21370, loss = 0.62 (7765.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:00.943461: step 21380, loss = 0.58 (7916.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:01.266393: step 21390, loss = 0.73 (8130.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:01.588754: step 21400, loss = 0.73 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:02.053765: step 21410, loss = 0.68 (7915.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:02.378263: step 21420, loss = 0.77 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:02.699312: step 21430, loss = 0.59 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:03.021929: step 21440, loss = 0.78 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:03.347924: step 21450, loss = 0.69 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:03.668939: step 21460, loss = 0.82 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:03.990900: step 21470, loss = 0.64 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:04.310976: step 21480, loss = 0.78 (7969.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:04.632152: step 21490, loss = 0.60 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:04.955235: step 21500, loss = 0.61 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:05.407123: step 21510, loss = 0.63 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:05.727848: step 21520, loss = 0.70 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:06.047092: step 21530, loss = 0.76 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:06.376470: step 21540, loss = 0.67 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:06.698439: step 21550, loss = 0.59 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:07.020588: step 21560, loss = 0.76 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:07.341660: step 21570, loss = 0.74 (7891.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:07.663304: step 21580, loss = 0.77 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:07.985063: step 21590, loss = 0.79 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:08.310224: step 21600, loss = 0.79 (7491.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:08.771588: step 21610, loss = 0.71 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:09.094210: step 21620, loss = 0.75 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:09.416644: step 21630, loss = 0.67 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:09.737867: step 21640, loss = 0.55 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:10.058854: step 21650, loss = 0.63 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:10.379903: step 21660, loss = 0.53 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:10.703548: step 21670, loss = 0.66 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:11.026291: step 21680, loss = 0.66 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:11.349106: step 21690, loss = 0.75 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:11.670784: step 21700, loss = 0.69 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:12.132594: step 21710, loss = 0.68 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:12.454243: step 21720, loss = 0.90 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:12.774486: step 21730, loss = 0.49 (8005.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:13.095335: step 21740, loss = 0.72 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:13.417485: step 21750, loss = 0.64 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:13.738443: step 21760, loss = 0.66 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:14.063664: step 21770, loss = 0.68 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:14.385020: step 21780, loss = 0.68 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:14.704973: step 21790, loss = 0.62 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:15.026476: step 21800, loss = 0.75 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:15.488753: step 21810, loss = 0.82 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:15.812852: step 21820, loss = 0.70 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:16.134500: step 21830, loss = 0.63 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:16.454800: step 21840, loss = 0.80 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:16.776279: step 21850, loss = 0.71 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:17.097185: step 21860, loss = 0.74 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:17.420290: step 21870, loss = 0.55 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:17.741356: step 21880, loss = 0.83 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:18.063376: step 21890, loss = 0.90 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:18.384788: step 21900, loss = 0.71 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:18.846289: step 21910, loss = 0.68 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:19.166409: step 21920, loss = 0.58 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:19.487960: step 21930, loss = 0.65 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:19.811146: step 21940, loss = 0.71 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:20.131718: step 21950, loss = 0.69 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:20.451978: step 21960, loss = 0.64 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:20.774851: step 21970, loss = 0.66 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:21.096062: step 21980, loss = 0.71 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:21.417641: step 21990, loss = 0.66 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:21.739585: step 22000, loss = 0.68 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:22.295981: step 22010, loss = 0.74 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:22.616753: step 22020, loss = 0.66 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:22.937538: step 22030, loss = 0.62 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:23.259662: step 22040, loss = 0.67 (7856.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:23.584577: step 22050, loss = 0.77 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:23.906291: step 22060, loss = 0.61 (7923.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:24.226728: step 22070, loss = 0.70 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:24.549143: step 22080, loss = 0.73 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:24.870043: step 22090, loss = 0.83 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:25.192266: step 22100, loss = 0.81 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:25.653244: step 22110, loss = 0.80 (7824.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:25.977173: step 22120, loss = 0.85 (7804.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:26.301404: step 22130, loss = 0.58 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:26.622608: step 22140, loss = 0.66 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:26.946885: step 22150, loss = 0.72 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:27.269840: step 22160, loss = 0.77 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:27.589091: step 22170, loss = 0.67 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:27.911378: step 22180, loss = 0.59 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:28.231969: step 22190, loss = 0.55 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:28.554155: step 22200, loss = 0.62 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:29.016334: step 22210, loss = 0.67 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:29.338267: step 22220, loss = 0.86 (7888.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:29.666379: step 22230, loss = 0.69 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:29.989041: step 22240, loss = 0.55 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:30.311475: step 22250, loss = 0.71 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:30.633911: step 22260, loss = 0.70 (7938.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:30.954909: step 22270, loss = 0.62 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:31.276691: step 22280, loss = 0.66 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:31.599575: step 22290, loss = 0.70 (7891.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:31.919986: step 22300, loss = 0.63 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:32.379746: step 22310, loss = 0.63 (7989.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:32.698596: step 22320, loss = 0.67 (8177.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:33.022216: step 22330, loss = 0.66 (7784.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:33.343727: step 22340, loss = 0.79 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:33.671807: step 22350, loss = 0.68 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:33.990970: step 22360, loss = 0.69 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:34.314166: step 22370, loss = 0.71 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:34.636198: step 22380, loss = 0.62 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:34.956828: step 22390, loss = 0.78 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:35.279840: step 22400, loss = 0.59 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:35.748680: step 22410, loss = 0.76 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:36.073988: step 22420, loss = 0.75 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:36.395156: step 22430, loss = 0.52 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:36.715394: step 22440, loss = 0.73 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:37.037559: step 22450, loss = 0.56 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:37.358066: step 22460, loss = 0.56 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:37.679070: step 22470, loss = 0.62 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:38.001110: step 22480, loss = 0.58 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:38.322510: step 22490, loss = 0.60 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:38.644118: step 22500, loss = 0.64 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:39.111459: step 22510, loss = 0.74 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:39.432932: step 22520, loss = 0.83 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:39.756553: step 22530, loss = 0.64 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:40.076572: step 22540, loss = 0.62 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:40.398287: step 22550, loss = 0.72 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:40.720160: step 22560, loss = 0.74 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:41.040913: step 22570, loss = 0.66 (7811.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:41.364141: step 22580, loss = 0.60 (7953.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:41.683180: step 22590, loss = 0.68 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:42.006315: step 22600, loss = 0.61 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:42.470836: step 22610, loss = 0.59 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:42.792685: step 22620, loss = 0.70 (7895.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:43.110745: step 22630, loss = 0.57 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:43.432542: step 22640, loss = 0.66 (7759.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:43.754451: step 22650, loss = 0.84 (7675.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:44.075560: step 22660, loss = 0.76 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:44.398454: step 22670, loss = 0.63 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:44.719676: step 22680, loss = 0.83 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:45.040955: step 22690, loss = 0.66 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:45.363222: step 22700, loss = 0.86 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:45.822238: step 22710, loss = 0.68 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:46.151251: step 22720, loss = 0.59 (7642.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:46.477631: step 22730, loss = 0.98 (7348.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:46.801808: step 22740, loss = 0.69 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:47.124033: step 22750, loss = 0.76 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:47.444506: step 22760, loss = 0.71 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:47.765346: step 22770, loss = 0.71 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:48.088320: step 22780, loss = 0.83 (7955.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:48.411134: step 22790, loss = 0.79 (7882.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:48.732274: step 22800, loss = 0.55 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:49.187693: step 22810, loss = 0.72 (7898.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:49.507976: step 22820, loss = 0.78 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:49.829835: step 22830, loss = 0.77 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:50.152473: step 22840, loss = 0.84 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:50.471771: step 22850, loss = 0.62 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:50.792585: step 22860, loss = 0.57 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:51.113089: step 22870, loss = 0.57 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:51.433962: step 22880, loss = 0.62 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:51.755444: step 22890, loss = 0.74 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:52.082305: step 22900, loss = 0.66 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:52.538662: step 22910, loss = 0.61 (7616.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:52.858702: step 22920, loss = 0.63 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:53.182371: step 22930, loss = 0.67 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:53.505883: step 22940, loss = 0.59 (7574.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:52:53.828071: step 22950, loss = 0.75 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:54.148143: step 22960, loss = 0.63 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:54.469600: step 22970, loss = 0.52 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:54.791445: step 22980, loss = 0.70 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:55.110825: step 22990, loss = 0.62 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:55.433114: step 23000, loss = 0.71 (8001.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:56.082481: step 23010, loss = 0.68 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:56.405935: step 23020, loss = 0.65 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:56.726903: step 23030, loss = 0.66 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:57.049303: step 23040, loss = 0.69 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:57.370107: step 23050, loss = 0.75 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:57.698539: step 23060, loss = 0.69 (7860.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:58.020893: step 23070, loss = 0.68 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:58.343157: step 23080, loss = 0.63 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:58.665234: step 23090, loss = 0.54 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:58.985965: step 23100, loss = 0.94 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:59.451885: step 23110, loss = 0.81 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:52:59.770862: step 23120, loss = 0.53 (7838.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:00.093250: step 23130, loss = 0.76 (7839.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:00.415305: step 23140, loss = 0.64 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:00.736757: step 23150, loss = 0.62 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:01.057533: step 23160, loss = 0.78 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:01.378212: step 23170, loss = 0.67 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:01.699265: step 23180, loss = 0.68 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:02.021422: step 23190, loss = 0.67 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:02.342617: step 23200, loss = 0.73 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:02.807497: step 23210, loss = 0.73 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:03.128897: step 23220, loss = 0.69 (7831.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:03.451195: step 23230, loss = 0.72 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:03.771023: step 23240, loss = 0.84 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:04.091608: step 23250, loss = 0.64 (7975.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:04.412328: step 23260, loss = 0.60 (7895.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:04.734162: step 23270, loss = 0.54 (8147.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:05.054273: step 23280, loss = 0.66 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:05.375787: step 23290, loss = 0.67 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:05.700779: step 23300, loss = 0.72 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:06.162983: step 23310, loss = 0.67 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:06.483094: step 23320, loss = 0.81 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:06.807413: step 23330, loss = 0.66 (7855.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:07.129766: step 23340, loss = 0.54 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:07.451395: step 23350, loss = 0.70 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:07.774522: step 23360, loss = 0.68 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:08.097211: step 23370, loss = 0.63 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:08.422689: step 23380, loss = 0.69 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:08.745045: step 23390, loss = 0.60 (8150.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:09.068600: step 23400, loss = 0.82 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:09.518847: step 23410, loss = 0.77 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:09.837384: step 23420, loss = 0.65 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:10.157491: step 23430, loss = 0.93 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:10.483746: step 23440, loss = 0.69 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:10.806966: step 23450, loss = 0.63 (7502.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:11.127196: step 23460, loss = 0.73 (8043.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:11.448746: step 23470, loss = 0.73 (7903.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:11.767684: step 23480, loss = 0.69 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:12.090377: step 23490, loss = 0.68 (8066.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:12.413881: step 23500, loss = 0.70 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:12.879928: step 23510, loss = 0.67 (7798.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:13.201247: step 23520, loss = 0.81 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:13.522068: step 23530, loss = 0.66 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:13.843130: step 23540, loss = 0.70 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:14.166736: step 23550, loss = 0.67 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:14.490560: step 23560, loss = 0.72 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:14.812593: step 23570, loss = 0.74 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:15.135894: step 23580, loss = 0.63 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:15.458042: step 23590, loss = 0.62 (7404.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:15.779938: step 23600, loss = 0.71 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:16.247598: step 23610, loss = 0.62 (7536.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:16.570978: step 23620, loss = 0.51 (7803.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:16.891537: step 23630, loss = 0.63 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:17.214645: step 23640, loss = 0.70 (7782.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:17.535502: step 23650, loss = 0.67 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:17.856688: step 23660, loss = 0.74 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:18.182126: step 23670, loss = 0.77 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:18.502850: step 23680, loss = 0.56 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:18.822571: step 23690, loss = 0.60 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:19.145264: step 23700, loss = 0.69 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:19.597560: step 23710, loss = 0.73 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:19.917995: step 23720, loss = 0.65 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:20.238698: step 23730, loss = 0.68 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:20.560541: step 23740, loss = 0.70 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:20.883592: step 23750, loss = 0.68 (7753.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:21.207012: step 23760, loss = 0.69 (7576.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:21.528204: step 23770, loss = 0.62 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:21.852257: step 23780, loss = 0.76 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:22.174151: step 23790, loss = 0.85 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:22.496181: step 23800, loss = 0.69 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:22.948915: step 23810, loss = 0.74 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:23.270137: step 23820, loss = 0.66 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:23.591995: step 23830, loss = 0.61 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:23.915326: step 23840, loss = 0.73 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:24.238114: step 23850, loss = 0.65 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:24.560837: step 23860, loss = 0.74 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:24.881475: step 23870, loss = 0.63 (7848.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:25.202940: step 23880, loss = 0.57 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:25.524602: step 23890, loss = 0.72 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:25.843696: step 23900, loss = 0.67 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:26.304081: step 23910, loss = 0.73 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:26.626861: step 23920, loss = 0.65 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:26.948238: step 23930, loss = 0.68 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:27.269845: step 23940, loss = 0.68 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:27.594195: step 23950, loss = 0.78 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:27.916375: step 23960, loss = 0.68 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:28.236504: step 23970, loss = 0.59 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:28.557268: step 23980, loss = 0.62 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:28.875882: step 23990, loss = 0.65 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:29.195106: step 24000, loss = 0.90 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:29.763602: step 24010, loss = 0.69 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:30.086101: step 24020, loss = 0.67 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:30.408201: step 24030, loss = 0.71 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:30.728835: step 24040, loss = 0.65 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:31.051442: step 24050, loss = 0.66 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:31.373015: step 24060, loss = 0.66 (7448.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:31.695543: step 24070, loss = 0.63 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:32.016618: step 24080, loss = 0.64 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:32.338755: step 24090, loss = 0.66 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:32.661120: step 24100, loss = 0.68 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:33.111515: step 24110, loss = 0.60 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:33.432728: step 24120, loss = 0.83 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:33.758205: step 24130, loss = 0.74 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:34.083939: step 24140, loss = 0.58 (7550.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:34.405692: step 24150, loss = 0.63 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:34.730118: step 24160, loss = 0.75 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:35.050550: step 24170, loss = 0.62 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:35.371699: step 24180, loss = 0.69 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:35.690589: step 24190, loss = 0.74 (8166.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:36.009443: step 24200, loss = 0.64 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:36.461467: step 24210, loss = 0.75 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:36.783197: step 24220, loss = 0.56 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:37.104191: step 24230, loss = 0.61 (7783.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:37.424342: step 24240, loss = 0.63 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:37.745462: step 24250, loss = 0.58 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:38.066947: step 24260, loss = 0.63 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:38.387923: step 24270, loss = 0.62 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:38.711068: step 24280, loss = 0.66 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:39.030969: step 24290, loss = 0.73 (7936.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:39.354934: step 24300, loss = 0.75 (8074.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:39.815371: step 24310, loss = 0.57 (7450.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:40.136980: step 24320, loss = 0.57 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:40.457540: step 24330, loss = 0.83 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:40.779268: step 24340, loss = 0.54 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:41.102857: step 24350, loss = 0.72 (7808.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:41.424872: step 24360, loss = 0.73 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:41.749137: step 24370, loss = 0.66 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:42.072628: step 24380, loss = 0.79 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:42.393747: step 24390, loss = 0.63 (8115.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:42.715528: step 24400, loss = 0.65 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:43.175580: step 24410, loss = 0.73 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:43.498111: step 24420, loss = 0.59 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:43.821310: step 24430, loss = 0.64 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:44.142104: step 24440, loss = 0.65 (7973.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:44.463490: step 24450, loss = 0.69 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:44.783360: step 24460, loss = 0.67 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:45.103941: step 24470, loss = 0.86 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:45.425835: step 24480, loss = 0.62 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:45.748001: step 24490, loss = 0.74 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:46.070676: step 24500, loss = 0.68 (7831.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:46.522843: step 24510, loss = 0.56 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:46.845051: step 24520, loss = 0.61 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:47.170897: step 24530, loss = 0.66 (7873.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:47.496945: step 24540, loss = 0.59 (7886.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:47.819841: step 24550, loss = 0.57 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:48.144100: step 24560, loss = 0.61 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:48.466769: step 24570, loss = 0.65 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:48.791156: step 24580, loss = 0.63 (7805.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:49.112922: step 24590, loss = 0.49 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:49.435183: step 24600, loss = 0.69 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:49.896764: step 24610, loss = 0.50 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:50.219704: step 24620, loss = 0.75 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:50.545247: step 24630, loss = 0.58 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:50.866301: step 24640, loss = 0.68 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:51.186032: step 24650, loss = 0.63 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:51.506721: step 24660, loss = 0.69 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:51.831687: step 24670, loss = 0.73 (7478.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:52.154984: step 24680, loss = 0.63 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:52.476715: step 24690, loss = 0.60 (7798.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:52.797527: step 24700, loss = 0.64 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:53.266395: step 24710, loss = 0.65 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:53.589586: step 24720, loss = 0.71 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:53.911486: step 24730, loss = 0.78 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:54.232938: step 24740, loss = 0.74 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:54.554541: step 24750, loss = 0.59 (7848.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:54.874345: step 24760, loss = 0.62 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:55.194819: step 24770, loss = 0.65 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:55.515749: step 24780, loss = 0.59 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:55.838400: step 24790, loss = 0.65 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:56.160639: step 24800, loss = 0.73 (7654.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:53:56.624900: step 24810, loss = 0.81 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:56.945318: step 24820, loss = 0.56 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:57.267214: step 24830, loss = 0.79 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:57.589291: step 24840, loss = 0.61 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:57.910555: step 24850, loss = 0.56 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:58.232429: step 24860, loss = 0.70 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:58.557280: step 24870, loss = 0.68 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:58.879347: step 24880, loss = 0.66 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:59.200268: step 24890, loss = 0.71 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:59.519836: step 24900, loss = 0.79 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:53:59.986617: step 24910, loss = 0.54 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:00.309513: step 24920, loss = 0.70 (7875.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:00.628561: step 24930, loss = 0.59 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:00.950113: step 24940, loss = 0.72 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:01.270868: step 24950, loss = 0.56 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:01.593019: step 24960, loss = 0.77 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:01.915768: step 24970, loss = 0.60 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:02.236662: step 24980, loss = 0.61 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:02.559619: step 24990, loss = 0.73 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:02.879343: step 25000, loss = 0.73 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:03.430280: step 25010, loss = 0.63 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:03.751181: step 25020, loss = 0.66 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:04.070620: step 25030, loss = 0.75 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:04.391312: step 25040, loss = 0.64 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:04.713248: step 25050, loss = 0.88 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:05.035786: step 25060, loss = 0.71 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:05.358368: step 25070, loss = 0.75 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:05.678364: step 25080, loss = 0.66 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:06.000865: step 25090, loss = 0.75 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:06.322499: step 25100, loss = 0.68 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:06.780191: step 25110, loss = 0.79 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:07.104037: step 25120, loss = 0.50 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:07.427199: step 25130, loss = 0.56 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:07.749364: step 25140, loss = 0.65 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:08.073280: step 25150, loss = 0.73 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:08.395138: step 25160, loss = 0.64 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:08.719094: step 25170, loss = 0.61 (7812.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:09.042229: step 25180, loss = 0.72 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:09.364848: step 25190, loss = 0.64 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:09.688354: step 25200, loss = 0.61 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:10.148783: step 25210, loss = 0.63 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:10.469829: step 25220, loss = 0.66 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:10.790906: step 25230, loss = 0.72 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:11.113339: step 25240, loss = 0.62 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:11.435435: step 25250, loss = 0.70 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:11.756440: step 25260, loss = 0.77 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:12.077706: step 25270, loss = 0.71 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:12.407385: step 25280, loss = 0.70 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:12.730558: step 25290, loss = 0.80 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:13.051786: step 25300, loss = 0.64 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:13.511339: step 25310, loss = 0.62 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:13.835313: step 25320, loss = 0.62 (7968.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:14.157639: step 25330, loss = 0.83 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:14.479743: step 25340, loss = 0.63 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:14.802212: step 25350, loss = 0.71 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:15.121305: step 25360, loss = 0.74 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:15.441855: step 25370, loss = 0.65 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:15.766951: step 25380, loss = 0.68 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:16.089191: step 25390, loss = 0.64 (8148.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:16.413102: step 25400, loss = 0.76 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:16.875571: step 25410, loss = 0.80 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:17.195206: step 25420, loss = 0.68 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:17.518049: step 25430, loss = 0.77 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:17.839939: step 25440, loss = 0.55 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:18.162001: step 25450, loss = 0.68 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:18.484804: step 25460, loss = 0.71 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:18.806543: step 25470, loss = 0.69 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:19.128261: step 25480, loss = 0.54 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:19.449516: step 25490, loss = 0.90 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:19.770923: step 25500, loss = 0.79 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:20.232975: step 25510, loss = 0.66 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:20.557501: step 25520, loss = 0.60 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:20.877892: step 25530, loss = 0.60 (8147.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:21.200887: step 25540, loss = 0.73 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:21.523334: step 25550, loss = 0.64 (7701.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:21.844637: step 25560, loss = 0.64 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:22.165927: step 25570, loss = 0.66 (7964.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:22.491275: step 25580, loss = 0.75 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:22.814453: step 25590, loss = 0.61 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:23.137647: step 25600, loss = 0.80 (7779.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:23.597063: step 25610, loss = 0.68 (7629.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:23.922649: step 25620, loss = 0.70 (7750.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:24.244350: step 25630, loss = 0.61 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:24.567832: step 25640, loss = 0.67 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:24.887732: step 25650, loss = 0.65 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:25.209908: step 25660, loss = 0.73 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:25.531447: step 25670, loss = 0.71 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:25.852631: step 25680, loss = 0.60 (8104.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:26.174408: step 25690, loss = 0.63 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:26.497521: step 25700, loss = 0.68 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:26.961821: step 25710, loss = 0.63 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:27.284312: step 25720, loss = 0.65 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:27.607585: step 25730, loss = 0.70 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:27.928897: step 25740, loss = 0.72 (7774.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:28.252339: step 25750, loss = 0.58 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:28.570818: step 25760, loss = 0.77 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:28.890820: step 25770, loss = 0.55 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:29.213939: step 25780, loss = 0.60 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:29.535467: step 25790, loss = 0.75 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:29.854080: step 25800, loss = 0.60 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:30.319654: step 25810, loss = 0.61 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:30.641842: step 25820, loss = 0.59 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:30.964001: step 25830, loss = 0.83 (7791.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:31.284391: step 25840, loss = 0.60 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:31.609772: step 25850, loss = 0.60 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:31.932259: step 25860, loss = 0.71 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:32.254059: step 25870, loss = 0.74 (7933.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:32.574406: step 25880, loss = 0.72 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:32.898011: step 25890, loss = 0.66 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:33.216866: step 25900, loss = 0.77 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:33.684255: step 25910, loss = 0.57 (7598.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:34.006930: step 25920, loss = 0.71 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:34.328812: step 25930, loss = 0.74 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:34.649947: step 25940, loss = 0.74 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:34.973949: step 25950, loss = 0.62 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:35.294579: step 25960, loss = 0.60 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:35.618494: step 25970, loss = 0.65 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:35.938750: step 25980, loss = 0.67 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:36.259762: step 25990, loss = 0.70 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:36.585753: step 26000, loss = 0.57 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:37.148041: step 26010, loss = 0.80 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:37.472320: step 26020, loss = 0.65 (7359.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:37.798604: step 26030, loss = 0.67 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:38.122334: step 26040, loss = 0.56 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:38.444291: step 26050, loss = 0.75 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:38.766417: step 26060, loss = 0.84 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:39.087954: step 26070, loss = 0.63 (7830.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:39.408580: step 26080, loss = 0.77 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:39.728209: step 26090, loss = 0.69 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:40.050466: step 26100, loss = 0.77 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:40.520079: step 26110, loss = 0.70 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:40.842019: step 26120, loss = 0.64 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:41.162792: step 26130, loss = 0.80 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:41.484314: step 26140, loss = 0.64 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:41.805204: step 26150, loss = 0.63 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:42.127199: step 26160, loss = 0.62 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:42.449136: step 26170, loss = 0.61 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:42.775149: step 26180, loss = 0.60 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:43.099738: step 26190, loss = 0.62 (7770.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:43.423515: step 26200, loss = 0.62 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:43.881521: step 26210, loss = 0.75 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:44.201774: step 26220, loss = 0.65 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:44.524961: step 26230, loss = 0.51 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:44.846141: step 26240, loss = 0.65 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:45.169505: step 26250, loss = 0.63 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:45.489789: step 26260, loss = 0.68 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:45.811558: step 26270, loss = 0.67 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:46.135137: step 26280, loss = 0.72 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:46.457309: step 26290, loss = 0.77 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:46.779528: step 26300, loss = 0.73 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:47.244520: step 26310, loss = 0.61 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:47.567728: step 26320, loss = 0.75 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:47.890817: step 26330, loss = 0.70 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:48.210425: step 26340, loss = 0.65 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:48.534078: step 26350, loss = 0.59 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:48.857283: step 26360, loss = 0.81 (8123.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:49.179450: step 26370, loss = 0.60 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:49.502667: step 26380, loss = 0.72 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:49.825710: step 26390, loss = 0.61 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:50.150095: step 26400, loss = 0.70 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:50.614712: step 26410, loss = 0.64 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:50.937735: step 26420, loss = 0.72 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:51.258656: step 26430, loss = 0.64 (7996.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:51.578912: step 26440, loss = 0.66 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:51.899872: step 26450, loss = 0.69 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:52.221998: step 26460, loss = 0.64 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:52.544674: step 26470, loss = 0.68 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:52.865103: step 26480, loss = 0.60 (7935.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:53.188798: step 26490, loss = 0.55 (7682.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:54:53.508214: step 26500, loss = 0.66 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:53.973399: step 26510, loss = 0.79 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:54.295369: step 26520, loss = 0.60 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:54.615487: step 26530, loss = 0.76 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:54.939551: step 26540, loss = 0.58 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:55.264001: step 26550, loss = 0.75 (7945.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:55.585495: step 26560, loss = 0.62 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:55.906905: step 26570, loss = 0.58 (8012.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:56.230101: step 26580, loss = 0.70 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:56.553461: step 26590, loss = 0.67 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:56.873939: step 26600, loss = 0.69 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:57.339110: step 26610, loss = 0.61 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:57.665616: step 26620, loss = 0.56 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:57.985069: step 26630, loss = 0.68 (8096.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:58.304368: step 26640, loss = 0.66 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:58.624499: step 26650, loss = 0.55 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:58.947408: step 26660, loss = 0.77 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:59.268085: step 26670, loss = 0.63 (7820.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:59.588209: step 26680, loss = 0.57 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:54:59.910237: step 26690, loss = 0.75 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:00.232578: step 26700, loss = 0.61 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:00.682955: step 26710, loss = 0.77 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:01.004990: step 26720, loss = 0.70 (7969.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:01.327515: step 26730, loss = 0.67 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:01.649906: step 26740, loss = 0.82 (7850.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:01.972864: step 26750, loss = 0.84 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:02.294272: step 26760, loss = 0.71 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:02.616706: step 26770, loss = 0.67 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:02.941437: step 26780, loss = 0.63 (7427.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:03.265920: step 26790, loss = 0.62 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:03.589410: step 26800, loss = 0.84 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:04.052456: step 26810, loss = 0.59 (7794.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:04.375248: step 26820, loss = 0.72 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:04.697137: step 26830, loss = 0.46 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:05.019276: step 26840, loss = 0.80 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:05.341901: step 26850, loss = 0.70 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:05.663514: step 26860, loss = 0.64 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:05.984192: step 26870, loss = 0.71 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:06.305962: step 26880, loss = 0.60 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:06.626270: step 26890, loss = 0.74 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:06.948678: step 26900, loss = 0.93 (8132.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:07.405787: step 26910, loss = 0.69 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:07.731007: step 26920, loss = 0.63 (7865.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:08.056449: step 26930, loss = 0.66 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:08.379280: step 26940, loss = 0.69 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:08.705038: step 26950, loss = 0.72 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:09.027188: step 26960, loss = 0.51 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:09.348034: step 26970, loss = 0.66 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:09.669240: step 26980, loss = 0.67 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:09.991968: step 26990, loss = 0.77 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:10.315230: step 27000, loss = 0.61 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:10.872261: step 27010, loss = 0.66 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:11.195116: step 27020, loss = 0.65 (7886.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:11.517894: step 27030, loss = 0.78 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:11.839678: step 27040, loss = 0.66 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:12.162197: step 27050, loss = 0.64 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:12.484448: step 27060, loss = 0.51 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:12.807136: step 27070, loss = 0.53 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:13.128755: step 27080, loss = 0.62 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:13.450350: step 27090, loss = 0.72 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:13.771313: step 27100, loss = 0.71 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:14.245771: step 27110, loss = 0.61 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:14.567765: step 27120, loss = 0.76 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:14.888527: step 27130, loss = 0.75 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:15.209150: step 27140, loss = 0.69 (8075.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:15.538490: step 27150, loss = 0.67 (7514.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:15.864431: step 27160, loss = 0.69 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:16.187270: step 27170, loss = 0.56 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:16.510469: step 27180, loss = 0.54 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:16.831408: step 27190, loss = 0.54 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:17.154432: step 27200, loss = 0.73 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:17.605450: step 27210, loss = 0.69 (7820.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:17.928085: step 27220, loss = 0.76 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:18.253135: step 27230, loss = 0.74 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:18.578738: step 27240, loss = 0.74 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:18.902570: step 27250, loss = 0.50 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:19.224798: step 27260, loss = 0.65 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:19.549218: step 27270, loss = 0.75 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:19.871328: step 27280, loss = 0.73 (7926.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:20.192804: step 27290, loss = 0.70 (7903.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:20.514809: step 27300, loss = 0.56 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:20.979958: step 27310, loss = 0.53 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:21.302745: step 27320, loss = 0.68 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:21.625568: step 27330, loss = 0.55 (8130.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:21.953202: step 27340, loss = 0.72 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:22.275236: step 27350, loss = 0.64 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:22.598049: step 27360, loss = 0.70 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:22.920379: step 27370, loss = 0.66 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:23.242466: step 27380, loss = 0.77 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:23.565687: step 27390, loss = 0.64 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:23.886766: step 27400, loss = 0.70 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:24.348327: step 27410, loss = 0.81 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:24.672147: step 27420, loss = 0.64 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:24.993455: step 27430, loss = 0.76 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:25.317683: step 27440, loss = 0.67 (7441.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:25.645981: step 27450, loss = 0.78 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:25.970308: step 27460, loss = 0.58 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:26.291676: step 27470, loss = 0.72 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:26.610769: step 27480, loss = 0.68 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:26.933493: step 27490, loss = 0.68 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:27.252209: step 27500, loss = 0.64 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:27.712203: step 27510, loss = 0.87 (8118.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:28.031525: step 27520, loss = 0.77 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:28.351717: step 27530, loss = 0.66 (8122.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:28.674344: step 27540, loss = 0.72 (8181.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:28.995027: step 27550, loss = 0.53 (7920.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:29.316114: step 27560, loss = 0.75 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:29.638167: step 27570, loss = 0.69 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:29.959649: step 27580, loss = 0.59 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:30.281715: step 27590, loss = 0.64 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:30.603133: step 27600, loss = 0.70 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:31.064808: step 27610, loss = 0.68 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:31.390426: step 27620, loss = 0.65 (7767.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:31.712896: step 27630, loss = 0.93 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:32.037050: step 27640, loss = 0.62 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:32.355558: step 27650, loss = 0.59 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:32.676214: step 27660, loss = 0.73 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:33.000055: step 27670, loss = 0.69 (7386.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:33.321440: step 27680, loss = 0.70 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:33.644090: step 27690, loss = 0.68 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:33.967375: step 27700, loss = 0.68 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:34.431393: step 27710, loss = 0.65 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:34.752474: step 27720, loss = 0.70 (8136.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:35.074685: step 27730, loss = 0.64 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:35.394669: step 27740, loss = 0.59 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:35.721113: step 27750, loss = 0.67 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:36.040258: step 27760, loss = 0.65 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:36.362526: step 27770, loss = 0.73 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:36.685978: step 27780, loss = 0.79 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:37.009052: step 27790, loss = 0.69 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:37.328967: step 27800, loss = 0.82 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:37.795949: step 27810, loss = 0.57 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:38.117554: step 27820, loss = 0.52 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:38.442688: step 27830, loss = 0.61 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:38.763893: step 27840, loss = 0.75 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:39.088621: step 27850, loss = 0.74 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:39.412775: step 27860, loss = 0.53 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:39.733619: step 27870, loss = 0.77 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:40.057140: step 27880, loss = 0.72 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:40.381915: step 27890, loss = 0.58 (7818.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:40.707693: step 27900, loss = 0.78 (7522.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:41.179750: step 27910, loss = 0.54 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:41.502003: step 27920, loss = 0.70 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:41.825059: step 27930, loss = 0.65 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:42.147388: step 27940, loss = 0.60 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:42.469368: step 27950, loss = 0.53 (7828.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:42.793548: step 27960, loss = 0.78 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:43.118367: step 27970, loss = 0.60 (7379.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:43.444831: step 27980, loss = 0.64 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:43.770068: step 27990, loss = 0.71 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:44.092754: step 28000, loss = 0.67 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:44.657267: step 28010, loss = 0.80 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:44.979815: step 28020, loss = 0.74 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:45.302954: step 28030, loss = 0.55 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:45.627571: step 28040, loss = 0.56 (7496.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:45.947154: step 28050, loss = 0.60 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:46.267817: step 28060, loss = 0.65 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:46.591203: step 28070, loss = 0.67 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:46.912436: step 28080, loss = 0.74 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:47.237701: step 28090, loss = 0.72 (7929.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:47.558723: step 28100, loss = 0.66 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:48.022164: step 28110, loss = 0.71 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:48.342516: step 28120, loss = 0.71 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:48.664899: step 28130, loss = 0.66 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:48.984674: step 28140, loss = 0.67 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:49.306241: step 28150, loss = 0.77 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:49.630308: step 28160, loss = 0.60 (7748.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:49.954722: step 28170, loss = 0.65 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:50.276244: step 28180, loss = 0.63 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:50.598206: step 28190, loss = 0.68 (7740.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:50.921317: step 28200, loss = 0.76 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:51.394558: step 28210, loss = 0.66 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:51.720702: step 28220, loss = 0.62 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:52.041679: step 28230, loss = 0.60 (7877.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:52.361146: step 28240, loss = 0.54 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:52.683417: step 28250, loss = 0.56 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:53.006069: step 28260, loss = 0.76 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:53.327086: step 28270, loss = 0.61 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:53.650420: step 28280, loss = 0.70 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:53.970651: step 28290, loss = 0.61 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:54.295657: step 28300, loss = 0.78 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:54.745309: step 28310, loss = 0.69 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:55.070550: step 28320, loss = 0.76 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:55.391807: step 28330, loss = 0.69 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:55.712614: step 28340, loss = 0.62 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:56.036362: step 28350, loss = 0.75 (8153.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:56.359619: step 28360, loss = 0.79 (7691.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:56.681658: step 28370, loss = 0.58 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:57.003405: step 28380, loss = 0.60 (7977.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:57.325813: step 28390, loss = 0.71 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:57.647192: step 28400, loss = 0.53 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:58.110766: step 28410, loss = 0.64 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:58.434782: step 28420, loss = 0.61 (7629.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:55:58.757800: step 28430, loss = 0.69 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:59.080160: step 28440, loss = 0.71 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:59.401408: step 28450, loss = 0.58 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:55:59.724293: step 28460, loss = 0.66 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:00.048028: step 28470, loss = 0.67 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:00.371530: step 28480, loss = 0.61 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:00.691984: step 28490, loss = 0.63 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:01.012372: step 28500, loss = 0.74 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:01.475425: step 28510, loss = 0.77 (7849.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:01.796986: step 28520, loss = 0.67 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:02.116564: step 28530, loss = 0.62 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:02.438545: step 28540, loss = 0.68 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:02.762162: step 28550, loss = 0.83 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:03.087747: step 28560, loss = 0.72 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:03.410326: step 28570, loss = 0.72 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:03.730756: step 28580, loss = 0.70 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:04.052428: step 28590, loss = 0.72 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:04.373036: step 28600, loss = 0.49 (8148.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:04.836945: step 28610, loss = 0.61 (7929.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:05.158944: step 28620, loss = 0.61 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:05.478653: step 28630, loss = 0.64 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:05.804191: step 28640, loss = 0.75 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:06.122961: step 28650, loss = 0.69 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:06.443197: step 28660, loss = 0.60 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:06.763416: step 28670, loss = 0.65 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:07.083744: step 28680, loss = 0.72 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:07.406182: step 28690, loss = 0.76 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:07.728361: step 28700, loss = 0.56 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:08.191827: step 28710, loss = 0.63 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:08.517940: step 28720, loss = 0.67 (7396.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:08.839943: step 28730, loss = 0.65 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:09.163291: step 28740, loss = 0.64 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:09.483768: step 28750, loss = 0.65 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:09.806065: step 28760, loss = 0.76 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:10.127430: step 28770, loss = 0.57 (8145.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:10.448812: step 28780, loss = 0.60 (7963.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:10.771866: step 28790, loss = 0.66 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:11.097327: step 28800, loss = 0.53 (8006.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:11.564349: step 28810, loss = 0.73 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:11.884864: step 28820, loss = 0.59 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:12.204742: step 28830, loss = 0.77 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:12.526616: step 28840, loss = 0.71 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:12.845269: step 28850, loss = 0.68 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:13.168847: step 28860, loss = 0.70 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:13.490740: step 28870, loss = 0.77 (7474.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:13.813918: step 28880, loss = 0.56 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:14.137621: step 28890, loss = 0.60 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:14.457846: step 28900, loss = 0.78 (7882.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:14.924375: step 28910, loss = 0.60 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:15.245306: step 28920, loss = 0.75 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:15.569480: step 28930, loss = 0.71 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:15.896213: step 28940, loss = 0.68 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:16.218517: step 28950, loss = 0.59 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:16.539148: step 28960, loss = 0.68 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:16.859536: step 28970, loss = 0.73 (8001.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:17.181878: step 28980, loss = 0.64 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:17.502091: step 28990, loss = 0.66 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:17.822883: step 29000, loss = 0.58 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:18.380765: step 29010, loss = 0.67 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:18.702533: step 29020, loss = 0.74 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:19.026148: step 29030, loss = 0.56 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:19.348192: step 29040, loss = 0.80 (8177.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:19.673774: step 29050, loss = 0.57 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:19.992341: step 29060, loss = 0.70 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:20.314022: step 29070, loss = 0.52 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:20.636485: step 29080, loss = 0.77 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:20.958054: step 29090, loss = 0.65 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:21.278425: step 29100, loss = 0.80 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:21.735447: step 29110, loss = 0.70 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:22.065869: step 29120, loss = 0.72 (7606.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:22.385685: step 29130, loss = 0.65 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:22.706321: step 29140, loss = 0.63 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:23.027531: step 29150, loss = 0.65 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:23.350430: step 29160, loss = 0.71 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:23.669417: step 29170, loss = 0.54 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:23.989278: step 29180, loss = 0.69 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:24.311395: step 29190, loss = 0.62 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:24.634049: step 29200, loss = 0.63 (7556.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:25.098840: step 29210, loss = 0.72 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:25.422763: step 29220, loss = 0.66 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:25.744776: step 29230, loss = 0.58 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:26.066070: step 29240, loss = 0.60 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:26.386343: step 29250, loss = 0.74 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:26.707472: step 29260, loss = 0.60 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:27.034244: step 29270, loss = 0.72 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:27.360482: step 29280, loss = 0.55 (7750.1 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:27.684034: step 29290, loss = 0.66 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:28.004884: step 29300, loss = 0.61 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:28.468602: step 29310, loss = 0.61 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:28.788585: step 29320, loss = 0.91 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:29.110883: step 29330, loss = 0.65 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:29.433247: step 29340, loss = 0.53 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:29.757769: step 29350, loss = 0.65 (7650.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:30.084329: step 29360, loss = 0.77 (7789.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:30.406617: step 29370, loss = 0.65 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:30.726464: step 29380, loss = 0.73 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:31.048436: step 29390, loss = 0.78 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:31.367929: step 29400, loss = 0.67 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:31.829898: step 29410, loss = 0.71 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:32.153092: step 29420, loss = 0.81 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:32.473402: step 29430, loss = 0.73 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:32.793709: step 29440, loss = 0.65 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:33.116402: step 29450, loss = 0.66 (7812.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:33.436523: step 29460, loss = 0.75 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:33.756619: step 29470, loss = 0.56 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:34.078195: step 29480, loss = 0.61 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:34.402170: step 29490, loss = 0.74 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:34.725025: step 29500, loss = 0.75 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:35.191242: step 29510, loss = 0.71 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:35.513717: step 29520, loss = 0.52 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:35.836258: step 29530, loss = 0.59 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:36.155631: step 29540, loss = 0.69 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:36.474890: step 29550, loss = 0.78 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:36.796047: step 29560, loss = 0.57 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:37.116736: step 29570, loss = 0.65 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:37.439287: step 29580, loss = 0.50 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:37.762722: step 29590, loss = 0.59 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:38.085641: step 29600, loss = 0.63 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:38.551591: step 29610, loss = 0.61 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:38.873631: step 29620, loss = 0.52 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:39.194487: step 29630, loss = 0.50 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:39.518929: step 29640, loss = 0.82 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:39.843479: step 29650, loss = 0.58 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:40.166494: step 29660, loss = 0.67 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:40.487411: step 29670, loss = 0.62 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:40.809289: step 29680, loss = 0.54 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:41.129602: step 29690, loss = 0.78 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:41.454734: step 29700, loss = 0.62 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:41.924132: step 29710, loss = 0.72 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:42.245820: step 29720, loss = 0.50 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:42.565717: step 29730, loss = 0.72 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:42.887629: step 29740, loss = 0.61 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:43.209037: step 29750, loss = 0.56 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:43.530298: step 29760, loss = 0.59 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:43.853170: step 29770, loss = 0.68 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:44.176819: step 29780, loss = 0.72 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:44.501118: step 29790, loss = 0.77 (7916.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:44.823178: step 29800, loss = 0.66 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:45.289143: step 29810, loss = 0.74 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:45.610325: step 29820, loss = 0.83 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:45.930488: step 29830, loss = 0.67 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:46.253083: step 29840, loss = 0.63 (7774.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:46.574253: step 29850, loss = 0.68 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:46.902528: step 29860, loss = 0.49 (7530.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:47.224647: step 29870, loss = 0.56 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:47.548005: step 29880, loss = 0.65 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:47.868123: step 29890, loss = 0.93 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:48.188120: step 29900, loss = 0.66 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:48.641500: step 29910, loss = 0.82 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:48.964719: step 29920, loss = 0.73 (7812.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:49.286529: step 29930, loss = 0.64 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:49.612702: step 29940, loss = 0.67 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:49.935569: step 29950, loss = 0.60 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:50.256415: step 29960, loss = 0.54 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:50.581824: step 29970, loss = 0.68 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:50.903788: step 29980, loss = 0.65 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:51.228512: step 29990, loss = 0.59 (7850.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:51.550823: step 30000, loss = 0.67 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:52.158054: step 30010, loss = 0.73 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:52.480992: step 30020, loss = 0.64 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:52.801629: step 30030, loss = 0.44 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:53.128546: step 30040, loss = 0.68 (7395.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:53.451482: step 30050, loss = 0.56 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:53.771897: step 30060, loss = 0.69 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:54.093046: step 30070, loss = 0.67 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:54.414548: step 30080, loss = 0.61 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:54.734661: step 30090, loss = 0.80 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:55.057219: step 30100, loss = 0.68 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:55.511453: step 30110, loss = 0.78 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:55.830560: step 30120, loss = 0.74 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:56.153217: step 30130, loss = 0.68 (7868.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:56.476386: step 30140, loss = 0.70 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:56.799944: step 30150, loss = 0.60 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:57.121057: step 30160, loss = 0.72 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:57.444900: step 30170, loss = 0.72 (7850.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:57.773641: step 30180, loss = 0.56 (7582.4 examples/sec; 0.017 sec/batch)
2017-09-16 15:56:58.095843: step 30190, loss = 0.73 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:58.418461: step 30200, loss = 0.66 (7780.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:58.880297: step 30210, loss = 0.59 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:59.205095: step 30220, loss = 0.71 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:59.525839: step 30230, loss = 0.73 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:56:59.850778: step 30240, loss = 0.81 (7703.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:00.170918: step 30250, loss = 0.63 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:00.491833: step 30260, loss = 0.71 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:00.814457: step 30270, loss = 0.62 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:01.136141: step 30280, loss = 0.67 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:01.459777: step 30290, loss = 0.55 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:01.781635: step 30300, loss = 0.71 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:02.233962: step 30310, loss = 0.60 (7898.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:02.557791: step 30320, loss = 0.72 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:02.878189: step 30330, loss = 0.69 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:03.198287: step 30340, loss = 0.67 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:03.520772: step 30350, loss = 0.58 (7747.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:03.845406: step 30360, loss = 0.72 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:04.165743: step 30370, loss = 0.64 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:04.485827: step 30380, loss = 0.64 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:04.813302: step 30390, loss = 0.58 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:05.134839: step 30400, loss = 0.74 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:05.592328: step 30410, loss = 0.70 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:05.911914: step 30420, loss = 0.51 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:06.232737: step 30430, loss = 0.62 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:06.554207: step 30440, loss = 0.74 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:06.875284: step 30450, loss = 0.72 (8151.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:07.197192: step 30460, loss = 0.85 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:07.519957: step 30470, loss = 0.68 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:07.842572: step 30480, loss = 0.71 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:08.164548: step 30490, loss = 0.57 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:08.486256: step 30500, loss = 0.58 (8121.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:08.952529: step 30510, loss = 0.56 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:09.276396: step 30520, loss = 0.64 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:09.599167: step 30530, loss = 0.63 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:09.923646: step 30540, loss = 0.83 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:10.247394: step 30550, loss = 0.65 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:10.570248: step 30560, loss = 0.54 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:10.891041: step 30570, loss = 0.85 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:11.210928: step 30580, loss = 0.71 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:11.529927: step 30590, loss = 0.76 (7849.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:11.849808: step 30600, loss = 0.73 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:12.313131: step 30610, loss = 0.75 (8152.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:12.632355: step 30620, loss = 0.55 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:12.951405: step 30630, loss = 0.63 (8156.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:13.272069: step 30640, loss = 0.60 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:13.593985: step 30650, loss = 0.61 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:13.914499: step 30660, loss = 0.68 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:14.233621: step 30670, loss = 0.64 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:14.555390: step 30680, loss = 0.73 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:14.876319: step 30690, loss = 0.64 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:15.197078: step 30700, loss = 0.69 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:15.646943: step 30710, loss = 0.66 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:15.967608: step 30720, loss = 0.60 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:16.289951: step 30730, loss = 0.61 (7893.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:16.611885: step 30740, loss = 0.58 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:16.932828: step 30750, loss = 0.61 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:17.256476: step 30760, loss = 0.85 (7796.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:17.579748: step 30770, loss = 0.76 (7811.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:17.900510: step 30780, loss = 0.69 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:18.227010: step 30790, loss = 0.74 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:18.549477: step 30800, loss = 0.70 (7847.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:19.005026: step 30810, loss = 0.74 (8108.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:19.324935: step 30820, loss = 0.59 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:19.646455: step 30830, loss = 0.68 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:19.967460: step 30840, loss = 0.54 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:20.287798: step 30850, loss = 0.54 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:20.609273: step 30860, loss = 0.55 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:20.936025: step 30870, loss = 0.58 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:21.257982: step 30880, loss = 0.67 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:21.577212: step 30890, loss = 0.92 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:21.899464: step 30900, loss = 0.69 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:22.363863: step 30910, loss = 0.66 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:22.688364: step 30920, loss = 0.61 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:23.009281: step 30930, loss = 0.63 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:23.331081: step 30940, loss = 0.62 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:23.653502: step 30950, loss = 0.66 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:23.976854: step 30960, loss = 0.69 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:24.298062: step 30970, loss = 0.66 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:24.619096: step 30980, loss = 0.81 (7970.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:24.941590: step 30990, loss = 0.57 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:25.265436: step 31000, loss = 0.76 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:25.824804: step 31010, loss = 0.67 (7866.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:26.148535: step 31020, loss = 0.55 (7730.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:26.471308: step 31030, loss = 0.57 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:26.792088: step 31040, loss = 0.60 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:27.112406: step 31050, loss = 0.68 (7795.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:27.435361: step 31060, loss = 0.67 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:27.755785: step 31070, loss = 0.73 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:28.076681: step 31080, loss = 0.63 (7788.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:28.397819: step 31090, loss = 0.74 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:28.720692: step 31100, loss = 0.66 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:29.183969: step 31110, loss = 0.62 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:29.506346: step 31120, loss = 0.66 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:29.828607: step 31130, loss = 0.74 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:30.149589: step 31140, loss = 0.63 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:30.475863: step 31150, loss = 0.78 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:30.802166: step 31160, loss = 0.61 (7822.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:31.124292: step 31170, loss = 0.61 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:31.446884: step 31180, loss = 0.72 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:31.768225: step 31190, loss = 0.72 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:32.089907: step 31200, loss = 0.69 (7828.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:32.541655: step 31210, loss = 0.59 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:32.861705: step 31220, loss = 0.70 (8138.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:33.181005: step 31230, loss = 0.66 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:33.501384: step 31240, loss = 0.65 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:33.822815: step 31250, loss = 0.69 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:34.143145: step 31260, loss = 0.50 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:34.464838: step 31270, loss = 0.59 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:34.784418: step 31280, loss = 0.62 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:35.103512: step 31290, loss = 0.66 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:35.422012: step 31300, loss = 0.71 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:35.891834: step 31310, loss = 0.63 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:36.214002: step 31320, loss = 0.87 (7825.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:36.534797: step 31330, loss = 0.63 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:36.853995: step 31340, loss = 0.67 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:37.175375: step 31350, loss = 0.54 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:37.499213: step 31360, loss = 0.63 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:37.820316: step 31370, loss = 0.55 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:38.142098: step 31380, loss = 0.73 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:38.462556: step 31390, loss = 0.63 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:38.782440: step 31400, loss = 0.75 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:39.253132: step 31410, loss = 0.80 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:39.575524: step 31420, loss = 0.57 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:39.895558: step 31430, loss = 0.62 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:40.217023: step 31440, loss = 0.59 (7784.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:40.539775: step 31450, loss = 0.75 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:40.863843: step 31460, loss = 0.57 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:41.185697: step 31470, loss = 0.65 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:41.508925: step 31480, loss = 0.71 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:41.829844: step 31490, loss = 0.68 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:42.149305: step 31500, loss = 0.54 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:42.611302: step 31510, loss = 0.62 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:42.937108: step 31520, loss = 0.63 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:43.258229: step 31530, loss = 0.67 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:43.579335: step 31540, loss = 0.65 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:43.900678: step 31550, loss = 0.63 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:44.224331: step 31560, loss = 0.62 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:44.543870: step 31570, loss = 0.63 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:44.864729: step 31580, loss = 0.79 (7824.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:45.187764: step 31590, loss = 0.57 (7809.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:45.514301: step 31600, loss = 0.64 (7622.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:45.972126: step 31610, loss = 0.60 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:46.294218: step 31620, loss = 0.67 (7913.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:46.615097: step 31630, loss = 0.64 (7721.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:46.937899: step 31640, loss = 0.74 (7612.5 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:47.258234: step 31650, loss = 0.65 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:47.583559: step 31660, loss = 0.59 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:47.907460: step 31670, loss = 0.61 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:48.228675: step 31680, loss = 0.71 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:48.551631: step 31690, loss = 0.77 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:48.871309: step 31700, loss = 0.63 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:49.325249: step 31710, loss = 0.57 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:49.647160: step 31720, loss = 0.73 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:49.969941: step 31730, loss = 0.59 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:50.291737: step 31740, loss = 0.68 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:50.612210: step 31750, loss = 0.57 (7968.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:50.934215: step 31760, loss = 0.55 (7826.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:51.255907: step 31770, loss = 0.63 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:51.576577: step 31780, loss = 0.79 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:51.898056: step 31790, loss = 0.63 (8018.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:52.220532: step 31800, loss = 0.60 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:52.669932: step 31810, loss = 0.69 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:52.990206: step 31820, loss = 0.72 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:53.309452: step 31830, loss = 0.68 (8107.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:53.630826: step 31840, loss = 0.76 (7982.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:53.953922: step 31850, loss = 0.69 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:54.274726: step 31860, loss = 0.54 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:54.596657: step 31870, loss = 0.61 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:54.917258: step 31880, loss = 0.62 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:55.238155: step 31890, loss = 0.67 (7855.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:55.559338: step 31900, loss = 0.79 (7891.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:56.027920: step 31910, loss = 0.70 (7427.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:56.348952: step 31920, loss = 0.49 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:56.671790: step 31930, loss = 0.66 (7831.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:56.994289: step 31940, loss = 0.63 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:57.315396: step 31950, loss = 0.79 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:57.636031: step 31960, loss = 0.72 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:57.956409: step 31970, loss = 0.70 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:58.281396: step 31980, loss = 0.58 (7340.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:57:58.601383: step 31990, loss = 0.67 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:58.922597: step 32000, loss = 0.64 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:59.486571: step 32010, loss = 0.65 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:57:59.809454: step 32020, loss = 0.61 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:00.129730: step 32030, loss = 0.59 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:00.451926: step 32040, loss = 0.65 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:00.771875: step 32050, loss = 0.75 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:01.091083: step 32060, loss = 0.51 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:01.414283: step 32070, loss = 0.58 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:01.737229: step 32080, loss = 0.60 (7568.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:02.059755: step 32090, loss = 0.60 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:02.381169: step 32100, loss = 0.73 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:02.851685: step 32110, loss = 0.94 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:03.173468: step 32120, loss = 0.53 (8085.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:03.496148: step 32130, loss = 0.64 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:03.817210: step 32140, loss = 0.69 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:04.137182: step 32150, loss = 0.64 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:04.457434: step 32160, loss = 0.69 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:04.778477: step 32170, loss = 0.63 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:05.099109: step 32180, loss = 0.57 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:05.421163: step 32190, loss = 0.57 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:05.741555: step 32200, loss = 0.70 (7821.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:06.203285: step 32210, loss = 0.63 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:06.524960: step 32220, loss = 0.62 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:06.844308: step 32230, loss = 0.83 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:07.164631: step 32240, loss = 0.73 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:07.486146: step 32250, loss = 0.64 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:07.807048: step 32260, loss = 0.68 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:08.129537: step 32270, loss = 0.64 (7900.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:08.449417: step 32280, loss = 0.59 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:08.770871: step 32290, loss = 0.55 (8037.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:09.090750: step 32300, loss = 0.92 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:09.556884: step 32310, loss = 0.81 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:09.876414: step 32320, loss = 0.69 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:10.195705: step 32330, loss = 0.71 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:10.517346: step 32340, loss = 0.58 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:10.837837: step 32350, loss = 0.61 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:11.158517: step 32360, loss = 0.67 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:11.480182: step 32370, loss = 0.57 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:11.799530: step 32380, loss = 0.58 (8036.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:12.121240: step 32390, loss = 0.68 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:12.441967: step 32400, loss = 0.73 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:12.901392: step 32410, loss = 0.81 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:13.221374: step 32420, loss = 0.65 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:13.541773: step 32430, loss = 0.68 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:13.865488: step 32440, loss = 0.75 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:14.185464: step 32450, loss = 0.71 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:14.505688: step 32460, loss = 0.62 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:14.829334: step 32470, loss = 0.71 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:15.150250: step 32480, loss = 0.80 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:15.472137: step 32490, loss = 0.81 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:15.794189: step 32500, loss = 0.54 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:16.257295: step 32510, loss = 0.77 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:16.576196: step 32520, loss = 0.59 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:16.898254: step 32530, loss = 0.71 (7841.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:17.217985: step 32540, loss = 0.65 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:17.538489: step 32550, loss = 0.66 (8077.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:17.862470: step 32560, loss = 0.65 (7506.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:18.187658: step 32570, loss = 0.60 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:18.511179: step 32580, loss = 0.75 (7722.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:18.833849: step 32590, loss = 0.63 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:19.154550: step 32600, loss = 0.65 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:19.608574: step 32610, loss = 0.60 (7525.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:19.933313: step 32620, loss = 0.70 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:20.256046: step 32630, loss = 0.54 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:20.578257: step 32640, loss = 0.70 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:20.899187: step 32650, loss = 0.68 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:21.219909: step 32660, loss = 0.69 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:21.550562: step 32670, loss = 0.74 (6168.5 examples/sec; 0.021 sec/batch)
2017-09-16 15:58:21.873398: step 32680, loss = 0.70 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:22.193394: step 32690, loss = 0.73 (8147.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:22.515834: step 32700, loss = 0.64 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:22.968234: step 32710, loss = 0.66 (8142.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:23.290318: step 32720, loss = 0.69 (7476.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:23.615942: step 32730, loss = 0.61 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:23.937393: step 32740, loss = 0.74 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:24.258125: step 32750, loss = 0.53 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:24.582871: step 32760, loss = 0.77 (7712.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:24.902609: step 32770, loss = 0.69 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:25.225354: step 32780, loss = 0.64 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:25.548554: step 32790, loss = 0.74 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:25.872375: step 32800, loss = 0.59 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:26.343341: step 32810, loss = 0.67 (7820.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:26.665516: step 32820, loss = 0.66 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:26.986618: step 32830, loss = 0.66 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:27.307074: step 32840, loss = 0.60 (7926.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:27.627783: step 32850, loss = 0.60 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:27.948552: step 32860, loss = 0.65 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:28.271205: step 32870, loss = 0.69 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:28.592512: step 32880, loss = 0.66 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:28.913649: step 32890, loss = 0.76 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:29.234579: step 32900, loss = 0.59 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:29.697061: step 32910, loss = 0.57 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:30.017326: step 32920, loss = 0.61 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:30.338306: step 32930, loss = 0.74 (7953.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:30.659838: step 32940, loss = 0.71 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:30.982259: step 32950, loss = 0.60 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:31.301677: step 32960, loss = 0.78 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:31.624601: step 32970, loss = 0.62 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:31.943524: step 32980, loss = 0.47 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:32.266008: step 32990, loss = 0.71 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:32.587202: step 33000, loss = 0.60 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:33.133333: step 33010, loss = 0.63 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:33.455046: step 33020, loss = 0.74 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:33.777296: step 33030, loss = 0.66 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:34.098377: step 33040, loss = 0.54 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:34.420331: step 33050, loss = 0.69 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:34.741725: step 33060, loss = 0.71 (7963.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:35.065466: step 33070, loss = 0.67 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:35.386823: step 33080, loss = 0.61 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:35.708606: step 33090, loss = 0.58 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:36.029478: step 33100, loss = 0.90 (7897.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:36.488026: step 33110, loss = 0.56 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:36.812816: step 33120, loss = 0.70 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:37.136885: step 33130, loss = 0.66 (8047.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:37.460994: step 33140, loss = 0.61 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:37.786321: step 33150, loss = 0.68 (7914.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:38.108100: step 33160, loss = 0.65 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:38.428703: step 33170, loss = 0.70 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:38.751736: step 33180, loss = 0.70 (7823.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:39.075621: step 33190, loss = 0.73 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:39.403149: step 33200, loss = 0.57 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:39.860808: step 33210, loss = 0.78 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:40.187172: step 33220, loss = 0.71 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:40.510427: step 33230, loss = 0.65 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:40.830736: step 33240, loss = 0.63 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:41.154617: step 33250, loss = 0.56 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:41.476240: step 33260, loss = 0.61 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:41.797360: step 33270, loss = 0.78 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:42.118875: step 33280, loss = 0.73 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:42.442523: step 33290, loss = 0.64 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:42.763548: step 33300, loss = 0.75 (7847.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:43.230801: step 33310, loss = 0.72 (7579.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:43.554647: step 33320, loss = 0.67 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:43.875410: step 33330, loss = 0.55 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:44.196614: step 33340, loss = 0.77 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:44.514992: step 33350, loss = 0.79 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:44.839709: step 33360, loss = 0.68 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:45.161455: step 33370, loss = 0.69 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:45.483459: step 33380, loss = 0.62 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:45.804986: step 33390, loss = 0.54 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:46.126979: step 33400, loss = 0.70 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:46.590299: step 33410, loss = 0.68 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:46.913650: step 33420, loss = 0.57 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:47.234324: step 33430, loss = 0.61 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:47.562107: step 33440, loss = 0.79 (7368.2 examples/sec; 0.017 sec/batch)
2017-09-16 15:58:47.884643: step 33450, loss = 0.63 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:48.207604: step 33460, loss = 0.72 (7872.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:48.529187: step 33470, loss = 0.55 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:48.849202: step 33480, loss = 0.69 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:49.169785: step 33490, loss = 0.56 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:49.492182: step 33500, loss = 0.66 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:49.955638: step 33510, loss = 0.68 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:50.275920: step 33520, loss = 0.65 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:50.594990: step 33530, loss = 0.59 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:50.915599: step 33540, loss = 0.74 (7818.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:51.237099: step 33550, loss = 0.65 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:51.557410: step 33560, loss = 0.69 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:51.881278: step 33570, loss = 0.64 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:52.204044: step 33580, loss = 0.64 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:52.527495: step 33590, loss = 0.54 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:52.850358: step 33600, loss = 0.60 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:53.300906: step 33610, loss = 0.66 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:53.621411: step 33620, loss = 0.62 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:53.941889: step 33630, loss = 0.66 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:54.265180: step 33640, loss = 0.56 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:54.587325: step 33650, loss = 0.56 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:54.908590: step 33660, loss = 0.77 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:55.230200: step 33670, loss = 0.61 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:55.551579: step 33680, loss = 0.64 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:55.872207: step 33690, loss = 0.60 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:56.192101: step 33700, loss = 0.62 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:56.649121: step 33710, loss = 0.79 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:56.970536: step 33720, loss = 0.67 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:57.294633: step 33730, loss = 0.58 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:57.616137: step 33740, loss = 0.70 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:57.936502: step 33750, loss = 0.62 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:58.258370: step 33760, loss = 0.70 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:58.580551: step 33770, loss = 0.59 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:58.905034: step 33780, loss = 0.76 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:59.227415: step 33790, loss = 0.62 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:58:59.550896: step 33800, loss = 0.76 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:00.015928: step 33810, loss = 0.52 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:00.340380: step 33820, loss = 0.61 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:00.661397: step 33830, loss = 0.62 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:00.984614: step 33840, loss = 0.63 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:01.307775: step 33850, loss = 0.76 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:01.630886: step 33860, loss = 0.66 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:01.952263: step 33870, loss = 0.52 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:02.274704: step 33880, loss = 0.76 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:02.602069: step 33890, loss = 0.54 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:02.926921: step 33900, loss = 0.60 (8149.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:03.382164: step 33910, loss = 0.60 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:03.703719: step 33920, loss = 0.62 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:04.026700: step 33930, loss = 0.53 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:04.348924: step 33940, loss = 0.72 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:04.671210: step 33950, loss = 0.58 (7954.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:04.993100: step 33960, loss = 0.66 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:05.314252: step 33970, loss = 0.54 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:05.634324: step 33980, loss = 0.64 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:05.956206: step 33990, loss = 0.78 (7846.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:06.277756: step 34000, loss = 0.63 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:06.835038: step 34010, loss = 0.64 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:07.155321: step 34020, loss = 0.63 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:07.481304: step 34030, loss = 0.68 (7339.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:07.804222: step 34040, loss = 0.54 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:08.125238: step 34050, loss = 0.52 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:08.447603: step 34060, loss = 0.63 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:08.772231: step 34070, loss = 0.60 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:09.096096: step 34080, loss = 0.81 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:09.417460: step 34090, loss = 0.62 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:09.739913: step 34100, loss = 0.60 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:10.194296: step 34110, loss = 0.71 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:10.514290: step 34120, loss = 0.64 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:10.837418: step 34130, loss = 0.56 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:11.160025: step 34140, loss = 0.57 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:11.480994: step 34150, loss = 0.81 (7858.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:11.803267: step 34160, loss = 0.71 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:12.125096: step 34170, loss = 0.53 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:12.447616: step 34180, loss = 0.67 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:12.773466: step 34190, loss = 0.66 (7334.0 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:13.097184: step 34200, loss = 0.63 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:13.553429: step 34210, loss = 0.55 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:13.876724: step 34220, loss = 0.60 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:14.197461: step 34230, loss = 0.70 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:14.518417: step 34240, loss = 0.55 (7894.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:14.839706: step 34250, loss = 0.71 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:15.160324: step 34260, loss = 0.65 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:15.484770: step 34270, loss = 0.64 (8014.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:15.805124: step 34280, loss = 0.84 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:16.125528: step 34290, loss = 0.58 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:16.446429: step 34300, loss = 0.65 (7872.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:16.900972: step 34310, loss = 0.56 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:17.221362: step 34320, loss = 0.65 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:17.541646: step 34330, loss = 0.60 (7903.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:17.864797: step 34340, loss = 0.74 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:18.186944: step 34350, loss = 0.56 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:18.508804: step 34360, loss = 0.67 (7834.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:18.831480: step 34370, loss = 0.55 (7757.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:19.153428: step 34380, loss = 0.82 (7761.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:19.475367: step 34390, loss = 0.64 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:19.796234: step 34400, loss = 0.73 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:20.260557: step 34410, loss = 0.59 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:20.579828: step 34420, loss = 0.59 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:20.903547: step 34430, loss = 0.71 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:21.227022: step 34440, loss = 0.72 (7803.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:21.547578: step 34450, loss = 0.64 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:21.869673: step 34460, loss = 0.62 (7814.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:22.194428: step 34470, loss = 0.64 (8080.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:22.514392: step 34480, loss = 0.63 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:22.838685: step 34490, loss = 0.75 (8145.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:23.163410: step 34500, loss = 0.70 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:23.619554: step 34510, loss = 0.65 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:23.942413: step 34520, loss = 0.84 (7652.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:24.262384: step 34530, loss = 0.55 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:24.583435: step 34540, loss = 0.71 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:24.908837: step 34550, loss = 0.57 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:25.231183: step 34560, loss = 0.53 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:25.553730: step 34570, loss = 0.62 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:25.874856: step 34580, loss = 0.71 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:26.193682: step 34590, loss = 0.63 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:26.514146: step 34600, loss = 0.58 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:26.970865: step 34610, loss = 0.57 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:27.292259: step 34620, loss = 0.70 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:27.614180: step 34630, loss = 0.61 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:27.936269: step 34640, loss = 0.69 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:28.258766: step 34650, loss = 0.65 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:28.581489: step 34660, loss = 0.76 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:28.904043: step 34670, loss = 0.61 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:29.225026: step 34680, loss = 0.64 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:29.545232: step 34690, loss = 0.56 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:29.866815: step 34700, loss = 0.74 (8157.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:30.327866: step 34710, loss = 0.62 (7658.7 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:30.651692: step 34720, loss = 0.71 (8134.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:30.975613: step 34730, loss = 0.78 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:31.299833: step 34740, loss = 0.77 (8158.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:31.620668: step 34750, loss = 0.53 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:31.941727: step 34760, loss = 0.79 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:32.263771: step 34770, loss = 0.84 (7866.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:32.589114: step 34780, loss = 0.62 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:32.911651: step 34790, loss = 0.52 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:33.240712: step 34800, loss = 0.56 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:33.707468: step 34810, loss = 0.62 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:34.027996: step 34820, loss = 0.61 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:34.349191: step 34830, loss = 0.55 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:34.668643: step 34840, loss = 0.73 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:34.990714: step 34850, loss = 0.63 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:35.315429: step 34860, loss = 0.67 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:35.643342: step 34870, loss = 0.52 (7834.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:35.966101: step 34880, loss = 0.71 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:36.286643: step 34890, loss = 0.68 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:36.614698: step 34900, loss = 0.65 (7818.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:37.073626: step 34910, loss = 0.64 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:37.395911: step 34920, loss = 0.81 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:37.716810: step 34930, loss = 0.61 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:38.038094: step 34940, loss = 0.80 (7963.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:38.361513: step 34950, loss = 0.52 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:38.685509: step 34960, loss = 0.61 (7484.3 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:39.004145: step 34970, loss = 0.70 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:39.327250: step 34980, loss = 0.80 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:39.649433: step 34990, loss = 0.57 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:39.973513: step 35000, loss = 0.80 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:40.536030: step 35010, loss = 0.58 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:40.856982: step 35020, loss = 0.61 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:41.179803: step 35030, loss = 0.66 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:41.502320: step 35040, loss = 0.72 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:41.828383: step 35050, loss = 0.78 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:42.151303: step 35060, loss = 0.62 (8128.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:42.473679: step 35070, loss = 0.63 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:42.796319: step 35080, loss = 0.84 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:43.118166: step 35090, loss = 0.62 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:43.437895: step 35100, loss = 0.56 (8135.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:43.899901: step 35110, loss = 0.56 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:44.221656: step 35120, loss = 0.74 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:44.541354: step 35130, loss = 0.69 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:44.867437: step 35140, loss = 0.57 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:45.191035: step 35150, loss = 0.55 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:45.512238: step 35160, loss = 0.80 (8175.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:45.832723: step 35170, loss = 0.67 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:46.154371: step 35180, loss = 0.67 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:46.475860: step 35190, loss = 0.71 (7690.9 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:46.796593: step 35200, loss = 0.93 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:47.262579: step 35210, loss = 0.69 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:47.584391: step 35220, loss = 0.68 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:47.906118: step 35230, loss = 0.65 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:48.227024: step 35240, loss = 0.57 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:48.549468: step 35250, loss = 0.74 (8175.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:48.873271: step 35260, loss = 0.64 (7287.6 examples/sec; 0.018 sec/batch)
2017-09-16 15:59:49.200677: step 35270, loss = 0.63 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:49.520280: step 35280, loss = 0.58 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:49.842015: step 35290, loss = 0.56 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:50.162815: step 35300, loss = 0.67 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:50.615477: step 35310, loss = 0.55 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:50.937501: step 35320, loss = 0.70 (8102.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:51.259380: step 35330, loss = 0.71 (7473.8 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:51.583899: step 35340, loss = 0.69 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:51.907038: step 35350, loss = 0.70 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:52.228769: step 35360, loss = 0.70 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:52.548442: step 35370, loss = 0.63 (7793.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:52.869067: step 35380, loss = 0.56 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:53.191130: step 35390, loss = 0.60 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:53.517187: step 35400, loss = 0.55 (7594.6 examples/sec; 0.017 sec/batch)
2017-09-16 15:59:53.971781: step 35410, loss = 0.69 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:54.292912: step 35420, loss = 0.63 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:54.615772: step 35430, loss = 0.68 (7823.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:54.936725: step 35440, loss = 0.64 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:55.258571: step 35450, loss = 0.68 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:55.578064: step 35460, loss = 0.67 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:55.899916: step 35470, loss = 0.62 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:56.224042: step 35480, loss = 0.60 (7784.2 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:56.549400: step 35490, loss = 0.67 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:56.871518: step 35500, loss = 0.60 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:57.337883: step 35510, loss = 0.71 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:57.655333: step 35520, loss = 0.60 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:57.978102: step 35530, loss = 0.71 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:58.298859: step 35540, loss = 0.69 (7808.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:58.622400: step 35550, loss = 0.63 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:58.949726: step 35560, loss = 0.70 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:59.269740: step 35570, loss = 0.57 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 15:59:59.594875: step 35580, loss = 0.72 (7069.9 examples/sec; 0.018 sec/batch)
2017-09-16 15:59:59.918054: step 35590, loss = 0.54 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:00.239366: step 35600, loss = 0.71 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:00.701325: step 35610, loss = 0.59 (7563.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:01.021194: step 35620, loss = 0.61 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:01.343360: step 35630, loss = 0.55 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:01.668012: step 35640, loss = 0.77 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:01.988821: step 35650, loss = 0.54 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:02.308515: step 35660, loss = 0.61 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:02.630062: step 35670, loss = 0.57 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:02.951503: step 35680, loss = 0.75 (7632.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:03.274846: step 35690, loss = 0.62 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:03.593362: step 35700, loss = 0.64 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:04.043538: step 35710, loss = 0.58 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:04.365055: step 35720, loss = 0.62 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:04.687532: step 35730, loss = 0.68 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:05.012030: step 35740, loss = 0.60 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:05.331322: step 35750, loss = 0.68 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:05.651848: step 35760, loss = 0.58 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:05.978048: step 35770, loss = 0.51 (7804.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:06.299631: step 35780, loss = 0.60 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:06.617917: step 35790, loss = 0.72 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:06.939239: step 35800, loss = 0.61 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:07.404287: step 35810, loss = 0.54 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:07.725798: step 35820, loss = 0.54 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:08.049013: step 35830, loss = 0.61 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:08.367834: step 35840, loss = 0.76 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:08.688733: step 35850, loss = 0.57 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:09.008745: step 35860, loss = 0.63 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:09.328662: step 35870, loss = 0.78 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:09.650083: step 35880, loss = 0.58 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:09.970917: step 35890, loss = 0.67 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:10.291972: step 35900, loss = 0.65 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:10.756565: step 35910, loss = 0.61 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:11.078441: step 35920, loss = 0.67 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:11.397921: step 35930, loss = 0.69 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:11.719243: step 35940, loss = 0.62 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:12.039249: step 35950, loss = 0.77 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:12.361499: step 35960, loss = 0.67 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:12.684087: step 35970, loss = 0.74 (7961.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:13.009230: step 35980, loss = 0.71 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:13.333175: step 35990, loss = 0.53 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:13.653173: step 36000, loss = 0.75 (7836.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:14.211681: step 36010, loss = 0.58 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:14.535063: step 36020, loss = 0.66 (7581.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:14.855211: step 36030, loss = 0.55 (8043.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:15.177631: step 36040, loss = 0.70 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:15.500618: step 36050, loss = 0.57 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:15.822753: step 36060, loss = 0.65 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:16.145748: step 36070, loss = 0.61 (7701.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:16.468936: step 36080, loss = 0.66 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:16.789539: step 36090, loss = 0.70 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:17.110897: step 36100, loss = 0.69 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:17.575181: step 36110, loss = 0.50 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:17.895331: step 36120, loss = 0.68 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:18.216390: step 36130, loss = 0.65 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:18.536615: step 36140, loss = 0.71 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:18.859546: step 36150, loss = 0.59 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:19.179409: step 36160, loss = 0.60 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:19.500225: step 36170, loss = 0.63 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:19.826320: step 36180, loss = 0.54 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:20.146013: step 36190, loss = 0.70 (8043.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:20.467651: step 36200, loss = 0.61 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:20.922158: step 36210, loss = 0.67 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:21.243133: step 36220, loss = 0.68 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:21.564118: step 36230, loss = 0.67 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:21.886161: step 36240, loss = 0.61 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:22.210371: step 36250, loss = 0.60 (7829.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:22.531665: step 36260, loss = 0.70 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:22.852613: step 36270, loss = 0.76 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:23.172445: step 36280, loss = 0.60 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:23.496878: step 36290, loss = 0.60 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:23.822423: step 36300, loss = 0.73 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:24.288224: step 36310, loss = 0.71 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:24.606962: step 36320, loss = 0.66 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:24.926875: step 36330, loss = 0.55 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:25.245328: step 36340, loss = 0.62 (8129.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:25.565935: step 36350, loss = 0.68 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:25.886259: step 36360, loss = 0.63 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:26.207045: step 36370, loss = 0.60 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:26.527834: step 36380, loss = 0.72 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:26.851638: step 36390, loss = 0.62 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:27.170833: step 36400, loss = 0.64 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:27.632664: step 36410, loss = 0.65 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:27.956248: step 36420, loss = 0.68 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:28.276460: step 36430, loss = 0.64 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:28.598404: step 36440, loss = 0.66 (7934.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:28.920745: step 36450, loss = 0.49 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:29.241991: step 36460, loss = 0.50 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:29.563727: step 36470, loss = 0.65 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:29.884566: step 36480, loss = 0.56 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:30.203886: step 36490, loss = 0.63 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:30.524393: step 36500, loss = 0.51 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:30.990056: step 36510, loss = 0.64 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:31.313014: step 36520, loss = 0.62 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:31.633585: step 36530, loss = 0.62 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:31.952141: step 36540, loss = 0.55 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:32.274418: step 36550, loss = 0.70 (7774.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:32.598211: step 36560, loss = 0.57 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:32.920755: step 36570, loss = 0.60 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:33.245695: step 36580, loss = 0.52 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:33.570617: step 36590, loss = 0.59 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:33.891918: step 36600, loss = 0.66 (7840.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:34.342300: step 36610, loss = 0.84 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:34.662380: step 36620, loss = 0.50 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:34.983722: step 36630, loss = 0.48 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:35.304840: step 36640, loss = 0.59 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:35.627503: step 36650, loss = 0.63 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:35.949853: step 36660, loss = 0.69 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:36.271274: step 36670, loss = 0.53 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:36.591335: step 36680, loss = 0.73 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:36.914315: step 36690, loss = 0.57 (7826.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:37.237919: step 36700, loss = 0.71 (7615.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:37.693739: step 36710, loss = 0.59 (8122.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:38.014550: step 36720, loss = 0.64 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:38.338084: step 36730, loss = 0.55 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:38.658905: step 36740, loss = 0.68 (7905.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:38.979244: step 36750, loss = 0.60 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:39.298014: step 36760, loss = 0.54 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:39.619158: step 36770, loss = 0.58 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:39.939445: step 36780, loss = 0.67 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:40.260939: step 36790, loss = 0.61 (7841.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:40.584051: step 36800, loss = 0.51 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:41.043547: step 36810, loss = 0.67 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:41.364344: step 36820, loss = 0.64 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:41.685603: step 36830, loss = 0.57 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:42.006000: step 36840, loss = 0.64 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:42.329116: step 36850, loss = 0.74 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:42.650377: step 36860, loss = 0.57 (7813.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:42.970792: step 36870, loss = 0.83 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:43.294896: step 36880, loss = 0.87 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:43.617414: step 36890, loss = 0.61 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:43.939499: step 36900, loss = 0.63 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:44.401005: step 36910, loss = 0.64 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:44.724932: step 36920, loss = 0.74 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:45.048163: step 36930, loss = 0.56 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:45.375432: step 36940, loss = 0.70 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:45.697035: step 36950, loss = 0.57 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:46.021470: step 36960, loss = 0.68 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:46.343169: step 36970, loss = 0.64 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:46.666698: step 36980, loss = 0.62 (7904.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:46.988815: step 36990, loss = 0.69 (8089.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:47.309866: step 37000, loss = 0.74 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:47.862951: step 37010, loss = 0.72 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:48.187658: step 37020, loss = 0.71 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:48.505813: step 37030, loss = 0.72 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:48.826149: step 37040, loss = 0.68 (7963.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:49.148730: step 37050, loss = 0.49 (7815.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:49.468888: step 37060, loss = 0.63 (8140.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:49.790255: step 37070, loss = 0.63 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:50.112650: step 37080, loss = 0.84 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:50.433746: step 37090, loss = 0.58 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:50.760784: step 37100, loss = 0.68 (7458.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:51.223132: step 37110, loss = 0.61 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:51.547298: step 37120, loss = 0.69 (7660.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:00:51.867592: step 37130, loss = 0.60 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:52.193630: step 37140, loss = 0.58 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:52.515907: step 37150, loss = 0.67 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:52.836392: step 37160, loss = 0.58 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:53.157812: step 37170, loss = 0.65 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:53.480121: step 37180, loss = 0.66 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:53.801623: step 37190, loss = 0.58 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:54.122159: step 37200, loss = 0.72 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:54.585526: step 37210, loss = 0.48 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:54.911615: step 37220, loss = 0.57 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:55.232259: step 37230, loss = 0.66 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:55.553470: step 37240, loss = 0.75 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:55.873636: step 37250, loss = 0.51 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:56.195365: step 37260, loss = 0.57 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:56.516966: step 37270, loss = 0.72 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:56.839926: step 37280, loss = 0.58 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:57.160844: step 37290, loss = 0.72 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:57.484036: step 37300, loss = 0.61 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:57.950479: step 37310, loss = 0.64 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:58.271040: step 37320, loss = 0.67 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:58.591919: step 37330, loss = 0.70 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:58.911456: step 37340, loss = 0.60 (8138.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:59.232572: step 37350, loss = 0.68 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:59.553653: step 37360, loss = 0.68 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:00:59.875907: step 37370, loss = 0.78 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:00.197774: step 37380, loss = 0.60 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:00.519740: step 37390, loss = 0.65 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:00.842494: step 37400, loss = 0.75 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:01.302988: step 37410, loss = 0.54 (7928.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:01.625390: step 37420, loss = 0.57 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:01.947414: step 37430, loss = 0.62 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:02.268638: step 37440, loss = 0.69 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:02.590753: step 37450, loss = 0.66 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:02.915774: step 37460, loss = 0.71 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:03.235888: step 37470, loss = 0.60 (8018.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:03.557446: step 37480, loss = 0.62 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:03.880731: step 37490, loss = 0.69 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:04.202056: step 37500, loss = 0.64 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:04.669659: step 37510, loss = 0.66 (7556.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:04.992134: step 37520, loss = 0.54 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:05.317489: step 37530, loss = 0.68 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:05.638750: step 37540, loss = 0.58 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:05.963796: step 37550, loss = 0.57 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:06.284284: step 37560, loss = 0.56 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:06.606405: step 37570, loss = 0.50 (7834.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:06.927016: step 37580, loss = 0.65 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:07.247677: step 37590, loss = 0.58 (8156.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:07.573844: step 37600, loss = 0.52 (7407.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:08.040802: step 37610, loss = 0.64 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:08.366336: step 37620, loss = 0.62 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:08.688891: step 37630, loss = 0.61 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:09.012833: step 37640, loss = 0.69 (7866.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:09.336061: step 37650, loss = 0.73 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:09.657645: step 37660, loss = 0.68 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:09.977841: step 37670, loss = 0.73 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:10.299613: step 37680, loss = 0.69 (8030.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:10.623328: step 37690, loss = 0.52 (7840.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:10.952767: step 37700, loss = 0.66 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:11.406968: step 37710, loss = 0.71 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:11.728142: step 37720, loss = 0.67 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:12.050610: step 37730, loss = 0.63 (7843.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:12.371572: step 37740, loss = 0.59 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:12.691036: step 37750, loss = 0.63 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:13.013394: step 37760, loss = 0.50 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:13.336447: step 37770, loss = 0.81 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:13.658041: step 37780, loss = 0.62 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:13.980673: step 37790, loss = 0.59 (7554.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:14.303600: step 37800, loss = 0.53 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:14.769263: step 37810, loss = 0.60 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:15.090978: step 37820, loss = 0.84 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:15.415655: step 37830, loss = 0.80 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:15.737922: step 37840, loss = 0.63 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:16.060242: step 37850, loss = 0.61 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:16.379778: step 37860, loss = 0.79 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:16.702333: step 37870, loss = 0.74 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:17.025038: step 37880, loss = 0.71 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:17.346056: step 37890, loss = 0.53 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:17.666681: step 37900, loss = 0.60 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:18.137108: step 37910, loss = 0.67 (7495.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:18.459202: step 37920, loss = 0.57 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:18.783064: step 37930, loss = 0.65 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:19.105390: step 37940, loss = 0.59 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:19.427387: step 37950, loss = 0.71 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:19.750453: step 37960, loss = 0.67 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:20.070412: step 37970, loss = 0.76 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:20.391020: step 37980, loss = 0.65 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:20.710236: step 37990, loss = 0.60 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:21.031790: step 38000, loss = 0.60 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:21.630713: step 38010, loss = 0.81 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:21.949481: step 38020, loss = 0.55 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:22.268990: step 38030, loss = 0.52 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:22.591635: step 38040, loss = 0.61 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:22.912206: step 38050, loss = 0.60 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:23.233900: step 38060, loss = 0.64 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:23.554045: step 38070, loss = 0.64 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:23.877025: step 38080, loss = 0.57 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:24.200210: step 38090, loss = 0.72 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:24.520566: step 38100, loss = 0.65 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:24.978711: step 38110, loss = 0.75 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:25.302069: step 38120, loss = 0.68 (7716.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:25.622218: step 38130, loss = 0.58 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:25.946880: step 38140, loss = 0.64 (7415.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:26.268091: step 38150, loss = 0.67 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:26.587468: step 38160, loss = 0.56 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:26.908130: step 38170, loss = 0.61 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:27.231576: step 38180, loss = 0.69 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:27.554477: step 38190, loss = 0.61 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:27.876599: step 38200, loss = 0.61 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:28.332822: step 38210, loss = 0.74 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:28.654068: step 38220, loss = 0.68 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:28.976698: step 38230, loss = 0.56 (8137.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:29.300979: step 38240, loss = 0.63 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:29.620995: step 38250, loss = 0.62 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:29.941492: step 38260, loss = 0.60 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:30.263316: step 38270, loss = 0.66 (8134.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:30.583265: step 38280, loss = 0.66 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:30.904143: step 38290, loss = 0.55 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:31.227480: step 38300, loss = 0.68 (7935.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:31.688243: step 38310, loss = 0.64 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:32.009414: step 38320, loss = 0.75 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:32.329787: step 38330, loss = 0.68 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:32.651209: step 38340, loss = 0.62 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:32.971115: step 38350, loss = 0.78 (7886.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:33.293019: step 38360, loss = 0.74 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:33.613697: step 38370, loss = 0.76 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:33.935040: step 38380, loss = 0.84 (7777.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:34.257241: step 38390, loss = 0.65 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:34.576197: step 38400, loss = 0.60 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:35.034392: step 38410, loss = 0.60 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:35.355489: step 38420, loss = 0.67 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:35.674460: step 38430, loss = 0.70 (8174.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:35.995718: step 38440, loss = 0.67 (7856.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:36.317127: step 38450, loss = 0.60 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:36.638237: step 38460, loss = 0.73 (7891.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:36.957177: step 38470, loss = 0.61 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:37.276859: step 38480, loss = 0.73 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:37.596885: step 38490, loss = 0.71 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:37.916876: step 38500, loss = 0.75 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:38.368148: step 38510, loss = 0.63 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:38.690966: step 38520, loss = 0.94 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:39.010361: step 38530, loss = 0.58 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:39.335628: step 38540, loss = 0.66 (8159.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:39.655270: step 38550, loss = 0.61 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:39.975483: step 38560, loss = 0.52 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:40.294714: step 38570, loss = 0.60 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:40.615507: step 38580, loss = 0.76 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:40.937380: step 38590, loss = 0.72 (7496.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:41.261229: step 38600, loss = 0.69 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:41.727360: step 38610, loss = 0.67 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:42.046289: step 38620, loss = 0.65 (8165.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:42.367181: step 38630, loss = 0.55 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:42.688681: step 38640, loss = 0.65 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:43.009525: step 38650, loss = 0.71 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:43.329292: step 38660, loss = 0.75 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:43.650200: step 38670, loss = 0.68 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:43.973869: step 38680, loss = 0.60 (7861.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:44.294759: step 38690, loss = 0.57 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:44.615654: step 38700, loss = 0.49 (8062.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:45.072446: step 38710, loss = 0.69 (8199.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:45.395857: step 38720, loss = 0.70 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:45.716837: step 38730, loss = 0.64 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:46.037640: step 38740, loss = 0.80 (7785.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:46.356492: step 38750, loss = 0.56 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:46.678509: step 38760, loss = 0.72 (7581.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:01:46.999885: step 38770, loss = 0.66 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:47.322313: step 38780, loss = 0.58 (8177.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:47.642391: step 38790, loss = 0.77 (8183.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:47.965393: step 38800, loss = 0.77 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:48.420683: step 38810, loss = 0.74 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:48.739088: step 38820, loss = 0.66 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:49.060091: step 38830, loss = 0.72 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:49.386381: step 38840, loss = 0.68 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:49.709133: step 38850, loss = 0.53 (8167.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:50.029199: step 38860, loss = 0.65 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:50.349306: step 38870, loss = 0.61 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:50.668623: step 38880, loss = 0.61 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:50.990470: step 38890, loss = 0.63 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:51.311188: step 38900, loss = 0.64 (7828.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:51.776546: step 38910, loss = 0.77 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:52.097005: step 38920, loss = 0.61 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:52.418958: step 38930, loss = 0.66 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:52.743150: step 38940, loss = 0.67 (7791.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:53.067553: step 38950, loss = 0.62 (7758.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:53.387918: step 38960, loss = 0.65 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:53.709542: step 38970, loss = 0.55 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:54.030491: step 38980, loss = 0.66 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:54.354705: step 38990, loss = 0.71 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:54.677339: step 39000, loss = 0.57 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:55.240807: step 39010, loss = 0.64 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:55.562305: step 39020, loss = 0.69 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:55.885133: step 39030, loss = 0.68 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:56.207675: step 39040, loss = 0.56 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:56.530374: step 39050, loss = 0.72 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:56.852751: step 39060, loss = 0.69 (7809.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:57.173092: step 39070, loss = 0.64 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:57.492921: step 39080, loss = 0.55 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:57.819086: step 39090, loss = 0.83 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:58.141075: step 39100, loss = 0.53 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:58.604311: step 39110, loss = 0.70 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:58.925254: step 39120, loss = 0.74 (7793.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:59.245539: step 39130, loss = 0.56 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:59.568339: step 39140, loss = 0.64 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:01:59.891848: step 39150, loss = 0.61 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:00.214292: step 39160, loss = 0.67 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:00.535506: step 39170, loss = 0.63 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:00.855834: step 39180, loss = 0.66 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:01.179752: step 39190, loss = 0.50 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:01.505160: step 39200, loss = 0.68 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:01.972871: step 39210, loss = 0.55 (7658.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:02.293693: step 39220, loss = 0.66 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:02.614034: step 39230, loss = 0.60 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:02.933836: step 39240, loss = 0.64 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:03.260101: step 39250, loss = 0.64 (7493.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:03.580580: step 39260, loss = 0.67 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:03.903116: step 39270, loss = 0.70 (7781.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:04.226052: step 39280, loss = 0.68 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:04.551388: step 39290, loss = 0.54 (7992.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:04.874052: step 39300, loss = 0.56 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:05.332950: step 39310, loss = 0.67 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:05.653474: step 39320, loss = 0.68 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:05.974296: step 39330, loss = 0.64 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:06.295388: step 39340, loss = 0.64 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:06.616152: step 39350, loss = 0.66 (7814.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:06.935438: step 39360, loss = 0.66 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:07.258428: step 39370, loss = 0.60 (7372.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:07.579416: step 39380, loss = 0.64 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:07.902130: step 39390, loss = 0.79 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:08.224324: step 39400, loss = 0.63 (8166.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:08.686813: step 39410, loss = 0.78 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:09.011073: step 39420, loss = 0.59 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:09.336911: step 39430, loss = 0.59 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:09.656102: step 39440, loss = 0.67 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:09.978739: step 39450, loss = 0.54 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:10.301705: step 39460, loss = 0.43 (7763.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:10.621841: step 39470, loss = 0.57 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:10.941340: step 39480, loss = 0.67 (7935.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:11.261997: step 39490, loss = 0.60 (7798.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:11.582289: step 39500, loss = 0.73 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:12.052137: step 39510, loss = 0.71 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:12.375439: step 39520, loss = 0.57 (7316.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:12.701510: step 39530, loss = 0.62 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:13.023486: step 39540, loss = 0.60 (7917.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:13.345176: step 39550, loss = 0.58 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:13.663448: step 39560, loss = 0.59 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:13.986572: step 39570, loss = 0.70 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:14.308253: step 39580, loss = 0.67 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:14.628776: step 39590, loss = 0.64 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:14.951360: step 39600, loss = 0.67 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:15.405095: step 39610, loss = 0.63 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:15.725907: step 39620, loss = 0.61 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:16.046894: step 39630, loss = 0.71 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:16.367215: step 39640, loss = 0.68 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:16.688912: step 39650, loss = 0.52 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:17.007831: step 39660, loss = 0.73 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:17.330353: step 39670, loss = 0.78 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:17.654763: step 39680, loss = 0.53 (8141.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:17.974981: step 39690, loss = 0.55 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:18.298013: step 39700, loss = 0.60 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:18.749990: step 39710, loss = 0.64 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:19.071156: step 39720, loss = 0.78 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:19.391483: step 39730, loss = 0.57 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:19.714818: step 39740, loss = 0.60 (7606.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:20.037511: step 39750, loss = 0.70 (7460.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:20.363165: step 39760, loss = 0.70 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:20.685738: step 39770, loss = 0.66 (7533.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:21.006345: step 39780, loss = 0.53 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:21.327006: step 39790, loss = 0.68 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:21.646876: step 39800, loss = 0.50 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:22.102749: step 39810, loss = 0.55 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:22.423386: step 39820, loss = 0.60 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:22.744840: step 39830, loss = 0.56 (7890.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:23.065025: step 39840, loss = 0.62 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:23.387332: step 39850, loss = 0.67 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:23.709943: step 39860, loss = 0.59 (7853.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:24.030083: step 39870, loss = 0.88 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:24.354942: step 39880, loss = 0.54 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:24.677123: step 39890, loss = 0.60 (8072.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:25.000219: step 39900, loss = 0.71 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:25.463957: step 39910, loss = 0.74 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:25.783917: step 39920, loss = 0.60 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:26.108531: step 39930, loss = 0.64 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:26.427786: step 39940, loss = 0.70 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:26.746591: step 39950, loss = 0.74 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:27.066627: step 39960, loss = 0.69 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:27.391296: step 39970, loss = 0.55 (7555.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:27.712329: step 39980, loss = 0.72 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:28.031869: step 39990, loss = 0.64 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:28.351970: step 40000, loss = 0.64 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:28.919317: step 40010, loss = 0.72 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:29.244568: step 40020, loss = 0.67 (7941.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:29.568303: step 40030, loss = 0.69 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:29.889753: step 40040, loss = 0.61 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:30.212695: step 40050, loss = 0.61 (7636.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:30.534994: step 40060, loss = 0.55 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:30.857571: step 40070, loss = 0.60 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:31.180797: step 40080, loss = 0.57 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:31.503146: step 40090, loss = 0.66 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:31.822073: step 40100, loss = 0.70 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:32.277622: step 40110, loss = 0.61 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:32.600851: step 40120, loss = 0.48 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:32.921544: step 40130, loss = 0.66 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:33.242141: step 40140, loss = 0.53 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:33.564917: step 40150, loss = 0.52 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:33.884531: step 40160, loss = 0.60 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:34.206612: step 40170, loss = 0.77 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:34.527725: step 40180, loss = 0.64 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:34.847806: step 40190, loss = 0.70 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:35.170087: step 40200, loss = 0.58 (7793.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:35.628903: step 40210, loss = 0.77 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:35.949830: step 40220, loss = 0.56 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:36.270473: step 40230, loss = 0.61 (7922.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:36.592054: step 40240, loss = 0.64 (7900.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:36.914303: step 40250, loss = 0.68 (7854.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:37.236084: step 40260, loss = 0.60 (7692.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:37.556962: step 40270, loss = 0.55 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:37.880750: step 40280, loss = 0.81 (7785.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:38.200873: step 40290, loss = 0.54 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:38.522827: step 40300, loss = 0.50 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:38.986666: step 40310, loss = 0.71 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:39.307161: step 40320, loss = 0.58 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:39.627172: step 40330, loss = 0.58 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:39.949610: step 40340, loss = 0.78 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:40.269521: step 40350, loss = 0.57 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:40.589925: step 40360, loss = 0.59 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:40.917087: step 40370, loss = 0.87 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:41.241365: step 40380, loss = 0.59 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:41.564488: step 40390, loss = 0.64 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:41.885824: step 40400, loss = 0.61 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:42.349324: step 40410, loss = 0.65 (7857.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:42.671121: step 40420, loss = 0.64 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:42.990612: step 40430, loss = 0.48 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:43.314774: step 40440, loss = 0.63 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:43.639307: step 40450, loss = 0.58 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:43.961980: step 40460, loss = 0.66 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:44.282453: step 40470, loss = 0.53 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:44.603898: step 40480, loss = 0.62 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:44.926373: step 40490, loss = 0.73 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:45.247592: step 40500, loss = 0.61 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:45.710091: step 40510, loss = 0.57 (7954.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:46.032878: step 40520, loss = 0.65 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:46.354991: step 40530, loss = 0.54 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:46.675188: step 40540, loss = 0.52 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:46.996538: step 40550, loss = 0.67 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:47.321438: step 40560, loss = 0.63 (8034.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:47.641542: step 40570, loss = 0.64 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:47.961684: step 40580, loss = 0.60 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:48.284019: step 40590, loss = 0.70 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:48.603923: step 40600, loss = 0.49 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:49.052955: step 40610, loss = 0.67 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:49.373676: step 40620, loss = 0.53 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:49.694660: step 40630, loss = 0.60 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:50.016518: step 40640, loss = 0.52 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:50.337288: step 40650, loss = 0.76 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:50.657805: step 40660, loss = 0.67 (7849.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:50.975967: step 40670, loss = 0.64 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:51.296219: step 40680, loss = 0.73 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:51.619423: step 40690, loss = 0.64 (7853.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:51.939405: step 40700, loss = 0.59 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:52.397782: step 40710, loss = 0.60 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:52.718331: step 40720, loss = 0.66 (8164.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:53.039745: step 40730, loss = 0.64 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:53.362339: step 40740, loss = 0.79 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:53.682882: step 40750, loss = 0.58 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:54.006067: step 40760, loss = 0.59 (7884.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:54.326335: step 40770, loss = 0.57 (8006.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:54.646624: step 40780, loss = 0.59 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:54.969798: step 40790, loss = 0.60 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:55.294565: step 40800, loss = 0.66 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:55.747934: step 40810, loss = 0.59 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:56.068241: step 40820, loss = 0.73 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:56.391116: step 40830, loss = 0.70 (7949.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:56.710837: step 40840, loss = 0.59 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:57.033486: step 40850, loss = 0.63 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:57.356101: step 40860, loss = 0.65 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:57.679037: step 40870, loss = 0.75 (7506.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:02:58.002081: step 40880, loss = 0.76 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:58.323540: step 40890, loss = 0.62 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:58.645659: step 40900, loss = 0.76 (8108.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:59.096354: step 40910, loss = 0.63 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:59.419661: step 40920, loss = 0.61 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:02:59.741048: step 40930, loss = 0.73 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:00.062878: step 40940, loss = 0.63 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:00.382606: step 40950, loss = 0.70 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:00.707064: step 40960, loss = 0.71 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:01.029823: step 40970, loss = 0.74 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:01.351803: step 40980, loss = 0.72 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:01.675335: step 40990, loss = 0.78 (7774.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:01.995534: step 41000, loss = 0.50 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:02.554916: step 41010, loss = 0.74 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:02.878356: step 41020, loss = 0.64 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:03.198948: step 41030, loss = 0.72 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:03.519536: step 41040, loss = 0.66 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:03.841197: step 41050, loss = 0.60 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:04.163342: step 41060, loss = 0.65 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:04.483647: step 41070, loss = 0.63 (7944.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:04.806076: step 41080, loss = 0.73 (7915.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:05.126626: step 41090, loss = 0.60 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:05.447942: step 41100, loss = 0.55 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:05.913964: step 41110, loss = 0.53 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:06.236972: step 41120, loss = 0.64 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:06.558864: step 41130, loss = 0.66 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:06.881328: step 41140, loss = 0.53 (7605.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:07.203730: step 41150, loss = 0.62 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:07.524950: step 41160, loss = 0.49 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:07.846755: step 41170, loss = 0.60 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:08.168358: step 41180, loss = 0.68 (7580.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:08.489747: step 41190, loss = 0.64 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:08.813951: step 41200, loss = 0.61 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:09.269045: step 41210, loss = 0.56 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:09.591704: step 41220, loss = 0.74 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:09.914639: step 41230, loss = 0.52 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:10.234624: step 41240, loss = 0.66 (8152.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:10.555142: step 41250, loss = 0.62 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:10.876589: step 41260, loss = 0.58 (7775.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:11.198228: step 41270, loss = 0.68 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:11.520908: step 41280, loss = 0.78 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:11.842351: step 41290, loss = 0.67 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:12.163055: step 41300, loss = 0.60 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:12.626687: step 41310, loss = 0.64 (7887.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:12.949862: step 41320, loss = 0.67 (7256.6 examples/sec; 0.018 sec/batch)
2017-09-16 16:03:13.271244: step 41330, loss = 0.59 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:13.596571: step 41340, loss = 0.61 (7793.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:13.918080: step 41350, loss = 0.57 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:14.240245: step 41360, loss = 0.65 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:14.560704: step 41370, loss = 0.63 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:14.880352: step 41380, loss = 0.55 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:15.203324: step 41390, loss = 0.55 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:15.525846: step 41400, loss = 0.69 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:15.988028: step 41410, loss = 0.61 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:16.310108: step 41420, loss = 0.63 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:16.629971: step 41430, loss = 0.50 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:16.952302: step 41440, loss = 0.56 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:17.274901: step 41450, loss = 0.59 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:17.597407: step 41460, loss = 0.70 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:17.919292: step 41470, loss = 0.68 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:18.237443: step 41480, loss = 0.64 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:18.557761: step 41490, loss = 0.74 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:18.877945: step 41500, loss = 0.51 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:19.334342: step 41510, loss = 0.64 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:19.654620: step 41520, loss = 0.60 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:19.975800: step 41530, loss = 0.63 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:20.296216: step 41540, loss = 0.70 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:20.616978: step 41550, loss = 0.59 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:20.939497: step 41560, loss = 0.55 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:21.259203: step 41570, loss = 0.82 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:21.579184: step 41580, loss = 0.72 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:21.901614: step 41590, loss = 0.58 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:22.222756: step 41600, loss = 0.74 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:22.681240: step 41610, loss = 0.55 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:23.002413: step 41620, loss = 0.61 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:23.322047: step 41630, loss = 0.82 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:23.643912: step 41640, loss = 0.80 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:23.965939: step 41650, loss = 0.47 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:24.287341: step 41660, loss = 0.57 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:24.608248: step 41670, loss = 0.72 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:24.928161: step 41680, loss = 0.60 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:25.248909: step 41690, loss = 0.72 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:25.568889: step 41700, loss = 0.71 (7889.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:26.027246: step 41710, loss = 0.59 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:26.348831: step 41720, loss = 0.65 (7774.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:26.670027: step 41730, loss = 0.66 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:26.991149: step 41740, loss = 0.57 (8067.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:27.311948: step 41750, loss = 0.62 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:27.631483: step 41760, loss = 0.57 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:27.951743: step 41770, loss = 0.63 (7825.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:28.271276: step 41780, loss = 0.60 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:28.594350: step 41790, loss = 0.73 (7793.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:28.915944: step 41800, loss = 0.48 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:29.378786: step 41810, loss = 0.77 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:29.700408: step 41820, loss = 0.57 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:30.020455: step 41830, loss = 0.43 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:30.341974: step 41840, loss = 0.68 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:30.662790: step 41850, loss = 0.59 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:30.983945: step 41860, loss = 0.64 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:31.303803: step 41870, loss = 0.56 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:31.627463: step 41880, loss = 0.64 (7596.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:31.949781: step 41890, loss = 0.74 (7612.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:32.271912: step 41900, loss = 0.69 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:32.722189: step 41910, loss = 0.60 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:33.043262: step 41920, loss = 0.74 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:33.366856: step 41930, loss = 0.62 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:33.688210: step 41940, loss = 0.53 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:34.010123: step 41950, loss = 0.62 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:34.332269: step 41960, loss = 0.55 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:34.651574: step 41970, loss = 0.69 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:34.973720: step 41980, loss = 0.48 (7960.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:35.296177: step 41990, loss = 0.55 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:35.619680: step 42000, loss = 0.71 (7748.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:36.172284: step 42010, loss = 0.70 (8116.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:36.494205: step 42020, loss = 0.70 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:36.813007: step 42030, loss = 0.52 (8014.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:37.134831: step 42040, loss = 0.70 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:37.459033: step 42050, loss = 0.73 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:37.783839: step 42060, loss = 0.62 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:38.103223: step 42070, loss = 0.59 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:38.423611: step 42080, loss = 0.65 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:38.748723: step 42090, loss = 0.70 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:39.069546: step 42100, loss = 0.66 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:39.524171: step 42110, loss = 0.55 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:39.848392: step 42120, loss = 0.73 (7831.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:40.170869: step 42130, loss = 0.65 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:40.494681: step 42140, loss = 0.75 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:40.820104: step 42150, loss = 0.71 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:41.141015: step 42160, loss = 0.66 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:41.461367: step 42170, loss = 0.74 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:41.783278: step 42180, loss = 0.62 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:42.109112: step 42190, loss = 0.81 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:42.430861: step 42200, loss = 0.73 (7877.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:42.884905: step 42210, loss = 0.50 (7837.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:43.207141: step 42220, loss = 0.62 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:43.530529: step 42230, loss = 0.57 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:43.852858: step 42240, loss = 0.63 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:44.172950: step 42250, loss = 0.59 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:44.491558: step 42260, loss = 0.58 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:44.811611: step 42270, loss = 0.57 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:45.134343: step 42280, loss = 0.60 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:45.455441: step 42290, loss = 0.62 (7779.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:45.775068: step 42300, loss = 0.67 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:46.237488: step 42310, loss = 0.58 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:46.557382: step 42320, loss = 0.63 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:46.886235: step 42330, loss = 0.66 (7742.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:47.211775: step 42340, loss = 0.57 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:47.533469: step 42350, loss = 0.61 (7806.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:47.855072: step 42360, loss = 0.70 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:48.174041: step 42370, loss = 0.56 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:48.497271: step 42380, loss = 0.72 (7877.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:48.822297: step 42390, loss = 0.60 (7552.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:49.146223: step 42400, loss = 0.74 (7782.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:49.600061: step 42410, loss = 0.65 (7827.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:49.920846: step 42420, loss = 0.69 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:50.239926: step 42430, loss = 0.65 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:50.561250: step 42440, loss = 0.67 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:50.882961: step 42450, loss = 0.47 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:51.206479: step 42460, loss = 0.62 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:51.529101: step 42470, loss = 0.54 (8095.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:51.854018: step 42480, loss = 0.61 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:52.178368: step 42490, loss = 0.61 (7941.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:52.499999: step 42500, loss = 0.60 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:52.954882: step 42510, loss = 0.53 (7851.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:53.277592: step 42520, loss = 0.72 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:53.601653: step 42530, loss = 0.61 (7367.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:03:53.929813: step 42540, loss = 0.59 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:54.250781: step 42550, loss = 0.61 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:54.571151: step 42560, loss = 0.66 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:54.892374: step 42570, loss = 0.57 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:55.214405: step 42580, loss = 0.60 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:55.535075: step 42590, loss = 0.62 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:55.855857: step 42600, loss = 0.70 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:56.319776: step 42610, loss = 0.62 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:56.641956: step 42620, loss = 0.58 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:56.973185: step 42630, loss = 0.67 (7798.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:57.298917: step 42640, loss = 0.64 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:57.622378: step 42650, loss = 0.70 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:57.943599: step 42660, loss = 0.64 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:58.266954: step 42670, loss = 0.58 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:58.586036: step 42680, loss = 0.56 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:58.906623: step 42690, loss = 0.58 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:59.231169: step 42700, loss = 0.62 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:03:59.694242: step 42710, loss = 0.86 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:00.017256: step 42720, loss = 0.52 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:00.338087: step 42730, loss = 0.75 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:00.658552: step 42740, loss = 0.53 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:00.981421: step 42750, loss = 0.86 (7772.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:01.303028: step 42760, loss = 0.76 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:01.627392: step 42770, loss = 0.73 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:01.949412: step 42780, loss = 0.52 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:02.268790: step 42790, loss = 0.52 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:02.587537: step 42800, loss = 0.57 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:03.038730: step 42810, loss = 0.59 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:03.357537: step 42820, loss = 0.52 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:03.677802: step 42830, loss = 0.67 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:03.998565: step 42840, loss = 0.73 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:04.322989: step 42850, loss = 0.70 (7454.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:04.648828: step 42860, loss = 0.61 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:04.970854: step 42870, loss = 0.56 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:05.290611: step 42880, loss = 0.72 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:05.611352: step 42890, loss = 0.69 (8057.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:05.933044: step 42900, loss = 0.56 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:06.390423: step 42910, loss = 0.60 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:06.711924: step 42920, loss = 0.62 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:07.033284: step 42930, loss = 0.64 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:07.357068: step 42940, loss = 0.80 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:07.678997: step 42950, loss = 0.64 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:08.000992: step 42960, loss = 0.53 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:08.321660: step 42970, loss = 0.60 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:08.644235: step 42980, loss = 0.51 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:08.970005: step 42990, loss = 0.55 (8185.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:09.301223: step 43000, loss = 0.58 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:09.861766: step 43010, loss = 0.82 (7872.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:10.186909: step 43020, loss = 0.63 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:10.508575: step 43030, loss = 0.56 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:10.831075: step 43040, loss = 0.65 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:11.151844: step 43050, loss = 0.58 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:11.476441: step 43060, loss = 0.66 (7747.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:11.799279: step 43070, loss = 0.65 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:12.119600: step 43080, loss = 0.62 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:12.438551: step 43090, loss = 0.64 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:12.760947: step 43100, loss = 0.68 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:13.209973: step 43110, loss = 0.65 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:13.529660: step 43120, loss = 0.65 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:13.849088: step 43130, loss = 0.56 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:14.173125: step 43140, loss = 0.58 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:14.494014: step 43150, loss = 0.49 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:14.815611: step 43160, loss = 0.52 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:15.136706: step 43170, loss = 0.66 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:15.457032: step 43180, loss = 0.63 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:15.779740: step 43190, loss = 0.68 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:16.105346: step 43200, loss = 0.65 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:16.572169: step 43210, loss = 0.73 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:16.895416: step 43220, loss = 0.63 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:17.215395: step 43230, loss = 0.67 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:17.535104: step 43240, loss = 0.74 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:17.856308: step 43250, loss = 0.72 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:18.178275: step 43260, loss = 0.66 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:18.500017: step 43270, loss = 0.63 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:18.825055: step 43280, loss = 0.70 (7240.7 examples/sec; 0.018 sec/batch)
2017-09-16 16:04:19.144408: step 43290, loss = 0.59 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:19.465965: step 43300, loss = 0.69 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:19.931219: step 43310, loss = 0.50 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:20.255368: step 43320, loss = 0.65 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:20.578590: step 43330, loss = 0.65 (7867.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:20.898814: step 43340, loss = 0.67 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:21.218895: step 43350, loss = 0.68 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:21.541510: step 43360, loss = 0.59 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:21.863225: step 43370, loss = 0.68 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:22.187369: step 43380, loss = 0.60 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:22.507405: step 43390, loss = 0.60 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:22.827756: step 43400, loss = 0.51 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:23.292578: step 43410, loss = 0.52 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:23.615296: step 43420, loss = 0.78 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:23.936938: step 43430, loss = 0.56 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:24.258749: step 43440, loss = 0.67 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:24.578405: step 43450, loss = 0.62 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:24.898942: step 43460, loss = 0.58 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:25.220702: step 43470, loss = 0.70 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:25.541342: step 43480, loss = 0.51 (8005.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:25.861544: step 43490, loss = 0.69 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:26.182028: step 43500, loss = 0.61 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:26.646876: step 43510, loss = 0.61 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:26.967638: step 43520, loss = 0.63 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:27.289075: step 43530, loss = 0.56 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:27.608461: step 43540, loss = 0.52 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:27.931343: step 43550, loss = 0.55 (7658.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:28.251078: step 43560, loss = 0.52 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:28.575636: step 43570, loss = 0.55 (7890.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:28.895817: step 43580, loss = 0.70 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:29.218520: step 43590, loss = 0.64 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:29.540572: step 43600, loss = 0.66 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:30.008821: step 43610, loss = 0.58 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:30.330066: step 43620, loss = 0.52 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:30.649001: step 43630, loss = 0.60 (7824.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:30.971909: step 43640, loss = 0.81 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:31.291913: step 43650, loss = 0.76 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:31.614532: step 43660, loss = 0.67 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:31.936085: step 43670, loss = 0.65 (7887.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:32.257565: step 43680, loss = 0.72 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:32.577125: step 43690, loss = 0.55 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:32.898321: step 43700, loss = 0.83 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:33.358188: step 43710, loss = 0.59 (8062.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:33.684092: step 43720, loss = 0.67 (7800.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:34.004877: step 43730, loss = 0.55 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:34.327745: step 43740, loss = 0.73 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:34.648857: step 43750, loss = 0.53 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:34.968777: step 43760, loss = 0.66 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:35.291367: step 43770, loss = 0.56 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:35.613736: step 43780, loss = 0.64 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:35.936944: step 43790, loss = 0.68 (7584.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:36.259112: step 43800, loss = 0.52 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:36.720176: step 43810, loss = 0.63 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:37.041399: step 43820, loss = 0.72 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:37.362777: step 43830, loss = 0.64 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:37.684840: step 43840, loss = 0.62 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:38.005935: step 43850, loss = 0.55 (7982.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:38.327123: step 43860, loss = 0.74 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:38.648181: step 43870, loss = 0.67 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:38.969007: step 43880, loss = 0.61 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:39.289052: step 43890, loss = 0.73 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:39.612239: step 43900, loss = 0.61 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:40.077701: step 43910, loss = 0.55 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:40.397025: step 43920, loss = 0.61 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:40.718178: step 43930, loss = 0.50 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:41.039176: step 43940, loss = 0.54 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:41.358904: step 43950, loss = 0.68 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:41.679679: step 43960, loss = 0.67 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:42.000932: step 43970, loss = 0.49 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:42.322467: step 43980, loss = 0.77 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:42.641813: step 43990, loss = 0.61 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:42.962882: step 44000, loss = 0.51 (7950.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:43.518106: step 44010, loss = 0.75 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:43.840139: step 44020, loss = 0.60 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:44.162058: step 44030, loss = 0.66 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:44.485463: step 44040, loss = 0.64 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:44.807048: step 44050, loss = 0.64 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:45.128432: step 44060, loss = 0.67 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:45.450494: step 44070, loss = 0.57 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:45.769629: step 44080, loss = 0.57 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:46.089723: step 44090, loss = 0.68 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:46.410853: step 44100, loss = 0.61 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:46.872673: step 44110, loss = 0.67 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:47.194395: step 44120, loss = 0.55 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:47.517229: step 44130, loss = 0.60 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:47.837894: step 44140, loss = 0.67 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:48.159367: step 44150, loss = 0.59 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:48.479028: step 44160, loss = 0.59 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:48.800290: step 44170, loss = 0.59 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:49.122204: step 44180, loss = 0.45 (7577.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:49.446992: step 44190, loss = 0.63 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:49.770515: step 44200, loss = 0.60 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:50.231846: step 44210, loss = 0.70 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:50.552585: step 44220, loss = 0.61 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:50.875728: step 44230, loss = 0.76 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:51.197194: step 44240, loss = 0.57 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:51.518065: step 44250, loss = 0.64 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:51.836622: step 44260, loss = 0.60 (8171.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:52.156295: step 44270, loss = 0.71 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:52.476265: step 44280, loss = 0.54 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:52.796800: step 44290, loss = 0.70 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:53.119490: step 44300, loss = 0.60 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:53.575911: step 44310, loss = 0.54 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:53.898299: step 44320, loss = 0.68 (7571.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:54.218424: step 44330, loss = 0.60 (7722.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:04:54.538782: step 44340, loss = 0.64 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:54.860321: step 44350, loss = 0.65 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:55.182932: step 44360, loss = 0.65 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:55.504193: step 44370, loss = 0.65 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:55.825006: step 44380, loss = 0.69 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:56.145640: step 44390, loss = 0.59 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:56.466200: step 44400, loss = 0.70 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:56.930990: step 44410, loss = 0.61 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:57.258066: step 44420, loss = 0.64 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:57.580350: step 44430, loss = 0.60 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:57.902365: step 44440, loss = 0.45 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:58.220898: step 44450, loss = 0.71 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:58.542911: step 44460, loss = 0.58 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:58.864316: step 44470, loss = 0.60 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:59.188524: step 44480, loss = 0.63 (7769.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:59.511325: step 44490, loss = 0.53 (7782.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:04:59.835985: step 44500, loss = 0.68 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:00.298847: step 44510, loss = 0.55 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:00.620204: step 44520, loss = 0.65 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:00.939690: step 44530, loss = 0.57 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:01.259263: step 44540, loss = 0.59 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:01.581747: step 44550, loss = 0.58 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:01.907776: step 44560, loss = 0.57 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:02.228808: step 44570, loss = 0.69 (8150.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:02.551483: step 44580, loss = 0.60 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:02.876359: step 44590, loss = 0.66 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:03.199509: step 44600, loss = 0.69 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:03.665219: step 44610, loss = 0.65 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:03.988651: step 44620, loss = 0.77 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:04.311610: step 44630, loss = 0.44 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:04.633870: step 44640, loss = 0.55 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:04.957987: step 44650, loss = 0.76 (7743.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:05:05.280363: step 44660, loss = 0.52 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:05.600765: step 44670, loss = 0.64 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:05.923221: step 44680, loss = 0.49 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:06.243011: step 44690, loss = 0.65 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:06.564264: step 44700, loss = 0.67 (8121.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:07.025153: step 44710, loss = 0.58 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:07.346747: step 44720, loss = 0.69 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:07.670600: step 44730, loss = 0.53 (7455.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:05:07.994953: step 44740, loss = 0.56 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:08.317108: step 44750, loss = 0.55 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:08.636963: step 44760, loss = 0.74 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:08.957584: step 44770, loss = 0.60 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:09.279443: step 44780, loss = 0.75 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:09.600586: step 44790, loss = 0.56 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:09.921637: step 44800, loss = 0.59 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:10.384964: step 44810, loss = 0.54 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:10.704344: step 44820, loss = 0.60 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:11.023566: step 44830, loss = 0.64 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:11.345177: step 44840, loss = 0.61 (7941.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:11.671309: step 44850, loss = 0.58 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:11.992882: step 44860, loss = 0.59 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:12.314651: step 44870, loss = 1.01 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:12.634841: step 44880, loss = 0.56 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:12.957291: step 44890, loss = 0.67 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:13.277130: step 44900, loss = 0.64 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:13.734068: step 44910, loss = 0.71 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:14.056106: step 44920, loss = 0.66 (7912.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:14.376024: step 44930, loss = 0.59 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:14.698431: step 44940, loss = 0.70 (7591.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:05:15.022266: step 44950, loss = 0.63 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:15.342753: step 44960, loss = 0.68 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:15.666911: step 44970, loss = 0.60 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:15.988233: step 44980, loss = 0.63 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:16.308414: step 44990, loss = 0.59 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:16.628278: step 45000, loss = 0.66 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:17.239112: step 45010, loss = 0.67 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:17.568855: step 45020, loss = 0.62 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:17.888025: step 45030, loss = 0.62 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:18.209096: step 45040, loss = 0.62 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:18.529266: step 45050, loss = 0.65 (7937.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:18.850885: step 45060, loss = 0.53 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:19.170900: step 45070, loss = 0.54 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:19.491672: step 45080, loss = 0.77 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:19.811933: step 45090, loss = 0.64 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:20.136238: step 45100, loss = 0.61 (7760.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:20.583584: step 45110, loss = 0.59 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:20.906494: step 45120, loss = 0.70 (8003.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:21.228424: step 45130, loss = 0.68 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:21.553185: step 45140, loss = 0.66 (7892.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:21.875516: step 45150, loss = 0.51 (7819.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:22.196565: step 45160, loss = 0.71 (8095.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:22.520150: step 45170, loss = 0.60 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:22.841730: step 45180, loss = 0.67 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:23.160788: step 45190, loss = 0.60 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:23.481399: step 45200, loss = 0.62 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:23.931629: step 45210, loss = 0.58 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:24.253386: step 45220, loss = 0.52 (7814.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:24.575861: step 45230, loss = 0.68 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:24.899331: step 45240, loss = 0.67 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:25.219083: step 45250, loss = 0.59 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:25.541946: step 45260, loss = 0.78 (7877.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:25.862467: step 45270, loss = 0.55 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:26.183518: step 45280, loss = 0.63 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:26.506070: step 45290, loss = 0.76 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:26.828884: step 45300, loss = 0.67 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:27.295113: step 45310, loss = 0.57 (8134.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:27.615932: step 45320, loss = 0.62 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:27.941075: step 45330, loss = 0.69 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:28.262147: step 45340, loss = 0.71 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:28.585310: step 45350, loss = 0.54 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:28.905962: step 45360, loss = 0.62 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:29.230320: step 45370, loss = 0.60 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:29.551476: step 45380, loss = 0.65 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:29.872386: step 45390, loss = 0.61 (7927.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:30.192988: step 45400, loss = 0.68 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:30.654385: step 45410, loss = 0.65 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:30.975729: step 45420, loss = 0.59 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:31.300236: step 45430, loss = 0.64 (7656.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:05:31.621757: step 45440, loss = 0.66 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:31.942439: step 45450, loss = 0.61 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:32.264376: step 45460, loss = 0.58 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:32.585740: step 45470, loss = 0.51 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:32.905854: step 45480, loss = 0.74 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:33.226413: step 45490, loss = 0.60 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:33.548266: step 45500, loss = 0.62 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:34.008448: step 45510, loss = 0.64 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:34.330011: step 45520, loss = 0.57 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:34.650789: step 45530, loss = 0.68 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:34.970181: step 45540, loss = 0.50 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:35.293937: step 45550, loss = 0.60 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:35.613856: step 45560, loss = 0.60 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:35.933522: step 45570, loss = 0.66 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:36.252531: step 45580, loss = 0.71 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:36.574558: step 45590, loss = 0.62 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:36.895367: step 45600, loss = 0.56 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:37.357581: step 45610, loss = 0.64 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:37.677965: step 45620, loss = 0.58 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:37.998800: step 45630, loss = 0.60 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:38.319445: step 45640, loss = 0.58 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:38.641362: step 45650, loss = 0.71 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:38.961664: step 45660, loss = 0.56 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:39.283040: step 45670, loss = 0.76 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:39.610453: step 45680, loss = 0.79 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:39.936824: step 45690, loss = 0.54 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:40.257026: step 45700, loss = 0.77 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:40.721918: step 45710, loss = 0.67 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:41.042729: step 45720, loss = 0.59 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:41.368654: step 45730, loss = 0.62 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:41.691696: step 45740, loss = 0.57 (7806.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:42.014413: step 45750, loss = 0.58 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:42.336961: step 45760, loss = 0.70 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:42.658134: step 45770, loss = 0.69 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:42.979412: step 45780, loss = 0.57 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:43.300563: step 45790, loss = 0.70 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:43.623255: step 45800, loss = 0.50 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:44.086530: step 45810, loss = 0.67 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:44.408837: step 45820, loss = 0.56 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:44.730592: step 45830, loss = 0.67 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:45.052041: step 45840, loss = 0.65 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:45.374122: step 45850, loss = 0.61 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:45.695451: step 45860, loss = 0.61 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:46.017885: step 45870, loss = 0.64 (8013.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:46.339137: step 45880, loss = 0.65 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:46.659302: step 45890, loss = 0.46 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:46.977997: step 45900, loss = 0.65 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:47.433677: step 45910, loss = 0.51 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:47.752251: step 45920, loss = 0.53 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:48.073405: step 45930, loss = 0.63 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:48.394594: step 45940, loss = 0.48 (8038.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:48.714469: step 45950, loss = 0.64 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:49.035953: step 45960, loss = 0.60 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:49.356814: step 45970, loss = 0.60 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:49.676193: step 45980, loss = 0.54 (8140.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:49.997372: step 45990, loss = 0.63 (7771.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:50.320263: step 46000, loss = 0.56 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:50.863328: step 46010, loss = 0.56 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:51.185038: step 46020, loss = 0.59 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:51.506193: step 46030, loss = 0.59 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:51.828048: step 46040, loss = 0.55 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:52.149890: step 46050, loss = 0.80 (7876.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:52.471590: step 46060, loss = 0.75 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:52.794508: step 46070, loss = 0.72 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:53.115329: step 46080, loss = 0.75 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:53.436594: step 46090, loss = 0.74 (7889.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:53.760212: step 46100, loss = 0.64 (7790.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:54.214116: step 46110, loss = 0.75 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:54.533748: step 46120, loss = 0.59 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:54.857560: step 46130, loss = 0.56 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:55.179576: step 46140, loss = 0.46 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:55.500770: step 46150, loss = 0.75 (7869.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:55.821730: step 46160, loss = 0.81 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:56.142507: step 46170, loss = 0.62 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:56.468142: step 46180, loss = 0.57 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:56.792743: step 46190, loss = 0.53 (7884.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:57.112774: step 46200, loss = 0.52 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:57.573865: step 46210, loss = 0.84 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:57.894787: step 46220, loss = 0.67 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:58.216320: step 46230, loss = 0.64 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:58.536295: step 46240, loss = 0.56 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:58.855236: step 46250, loss = 0.67 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:59.174771: step 46260, loss = 0.55 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:59.496903: step 46270, loss = 0.58 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:05:59.817131: step 46280, loss = 0.54 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:00.139672: step 46290, loss = 0.69 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:00.460484: step 46300, loss = 0.53 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:00.930162: step 46310, loss = 0.56 (7768.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:01.249327: step 46320, loss = 0.63 (8141.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:01.568805: step 46330, loss = 0.55 (8060.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:01.889247: step 46340, loss = 0.63 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:02.208987: step 46350, loss = 0.56 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:02.529319: step 46360, loss = 0.83 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:02.852985: step 46370, loss = 0.53 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:03.176519: step 46380, loss = 0.68 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:03.497979: step 46390, loss = 0.60 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:03.818795: step 46400, loss = 0.67 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:04.275189: step 46410, loss = 0.61 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:04.598175: step 46420, loss = 0.59 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:04.917216: step 46430, loss = 0.49 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:05.237092: step 46440, loss = 0.56 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:05.564913: step 46450, loss = 0.55 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:05.890725: step 46460, loss = 0.57 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:06.214323: step 46470, loss = 0.65 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:06.538318: step 46480, loss = 0.59 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:06.861389: step 46490, loss = 0.62 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:07.183326: step 46500, loss = 0.53 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:07.648279: step 46510, loss = 0.56 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:07.977301: step 46520, loss = 0.63 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:08.302556: step 46530, loss = 0.67 (7483.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:06:08.626594: step 46540, loss = 0.69 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:08.946079: step 46550, loss = 0.61 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:09.269699: step 46560, loss = 0.73 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:09.593132: step 46570, loss = 0.72 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:09.913756: step 46580, loss = 0.56 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:10.236592: step 46590, loss = 0.66 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:10.558167: step 46600, loss = 0.60 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:11.017590: step 46610, loss = 0.58 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:11.341105: step 46620, loss = 0.68 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:11.663005: step 46630, loss = 0.56 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:11.984202: step 46640, loss = 0.52 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:12.303515: step 46650, loss = 0.61 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:12.628742: step 46660, loss = 0.62 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:12.951642: step 46670, loss = 0.64 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:13.271805: step 46680, loss = 0.63 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:13.593399: step 46690, loss = 0.52 (8152.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:13.915322: step 46700, loss = 0.61 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:14.379910: step 46710, loss = 0.61 (7554.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:06:14.704734: step 46720, loss = 0.54 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:15.025631: step 46730, loss = 0.70 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:15.345780: step 46740, loss = 0.70 (7996.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:15.667279: step 46750, loss = 0.53 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:15.987532: step 46760, loss = 0.65 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:16.308621: step 46770, loss = 0.61 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:16.632054: step 46780, loss = 0.61 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:16.955448: step 46790, loss = 0.65 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:17.279220: step 46800, loss = 0.76 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:17.738179: step 46810, loss = 0.65 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:18.061750: step 46820, loss = 0.61 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:18.382195: step 46830, loss = 0.64 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:18.703018: step 46840, loss = 0.80 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:19.023922: step 46850, loss = 0.61 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:19.345680: step 46860, loss = 0.61 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:19.667541: step 46870, loss = 0.70 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:19.989090: step 46880, loss = 0.65 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:20.308058: step 46890, loss = 0.63 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:20.628897: step 46900, loss = 0.59 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:21.088044: step 46910, loss = 0.68 (7834.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:21.410484: step 46920, loss = 0.56 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:21.730877: step 46930, loss = 0.65 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:22.050515: step 46940, loss = 0.61 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:22.371321: step 46950, loss = 0.52 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:22.691420: step 46960, loss = 0.58 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:23.012993: step 46970, loss = 0.53 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:23.335980: step 46980, loss = 0.63 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:23.658910: step 46990, loss = 0.57 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:23.981146: step 47000, loss = 0.78 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:24.552911: step 47010, loss = 0.63 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:24.874181: step 47020, loss = 0.77 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:25.194316: step 47030, loss = 0.78 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:25.515036: step 47040, loss = 0.73 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:25.835588: step 47050, loss = 0.64 (8169.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:26.157439: step 47060, loss = 0.57 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:26.480062: step 47070, loss = 0.60 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:26.800839: step 47080, loss = 0.54 (7745.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:06:27.121038: step 47090, loss = 0.68 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:27.442643: step 47100, loss = 0.68 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:27.901195: step 47110, loss = 0.62 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:28.222088: step 47120, loss = 0.67 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:28.543254: step 47130, loss = 0.66 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:28.866315: step 47140, loss = 0.60 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:29.187602: step 47150, loss = 0.56 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:29.510596: step 47160, loss = 0.58 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:29.831869: step 47170, loss = 0.74 (8069.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:30.152222: step 47180, loss = 0.61 (7956.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:30.474380: step 47190, loss = 0.70 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:30.794695: step 47200, loss = 0.68 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:31.257386: step 47210, loss = 0.56 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:31.580742: step 47220, loss = 0.58 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:31.900008: step 47230, loss = 0.76 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:32.224996: step 47240, loss = 0.60 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:32.545489: step 47250, loss = 0.65 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:32.865418: step 47260, loss = 0.74 (8142.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:33.185341: step 47270, loss = 0.64 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:33.506857: step 47280, loss = 0.66 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:33.829741: step 47290, loss = 0.55 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:34.151471: step 47300, loss = 0.67 (8133.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:34.610405: step 47310, loss = 0.67 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:34.931876: step 47320, loss = 0.58 (7921.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:35.256380: step 47330, loss = 0.50 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:35.580115: step 47340, loss = 0.69 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:35.904111: step 47350, loss = 0.62 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:36.225064: step 47360, loss = 0.54 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:36.548622: step 47370, loss = 0.58 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:36.871513: step 47380, loss = 0.46 (7933.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:37.193164: step 47390, loss = 0.61 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:37.513204: step 47400, loss = 0.85 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:37.975212: step 47410, loss = 0.57 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:38.296464: step 47420, loss = 0.55 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:38.616081: step 47430, loss = 0.69 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:38.938688: step 47440, loss = 0.67 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:39.259711: step 47450, loss = 0.74 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:39.580218: step 47460, loss = 0.71 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:39.900435: step 47470, loss = 0.54 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:40.221852: step 47480, loss = 0.49 (7837.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:40.544709: step 47490, loss = 0.65 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:40.865981: step 47500, loss = 0.65 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:41.328434: step 47510, loss = 0.63 (7784.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:41.650826: step 47520, loss = 0.70 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:41.971848: step 47530, loss = 0.60 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:42.293080: step 47540, loss = 0.46 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:42.613877: step 47550, loss = 0.63 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:42.934851: step 47560, loss = 0.69 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:43.254692: step 47570, loss = 0.70 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:43.574018: step 47580, loss = 0.74 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:43.894519: step 47590, loss = 0.66 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:44.217576: step 47600, loss = 0.62 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:44.680947: step 47610, loss = 0.52 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:45.001558: step 47620, loss = 0.65 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:45.322186: step 47630, loss = 0.56 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:45.644169: step 47640, loss = 0.60 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:45.970838: step 47650, loss = 0.60 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:46.293657: step 47660, loss = 0.70 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:46.615562: step 47670, loss = 0.60 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:46.937439: step 47680, loss = 0.62 (7987.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:47.258397: step 47690, loss = 0.67 (7817.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:47.578154: step 47700, loss = 0.74 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:48.042018: step 47710, loss = 0.67 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:48.362329: step 47720, loss = 0.60 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:48.683486: step 47730, loss = 0.65 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:49.003138: step 47740, loss = 0.75 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:49.322848: step 47750, loss = 0.57 (7911.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:49.643447: step 47760, loss = 0.68 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:49.964199: step 47770, loss = 0.56 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:50.283334: step 47780, loss = 0.74 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:50.603059: step 47790, loss = 0.69 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:50.924959: step 47800, loss = 0.70 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:51.380877: step 47810, loss = 0.66 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:51.700742: step 47820, loss = 0.61 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:52.021302: step 47830, loss = 0.70 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:52.340950: step 47840, loss = 0.61 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:52.661219: step 47850, loss = 0.71 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:52.981411: step 47860, loss = 0.57 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:53.300302: step 47870, loss = 0.84 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:53.620157: step 47880, loss = 0.65 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:53.940720: step 47890, loss = 0.61 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:54.263296: step 47900, loss = 0.61 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:54.719254: step 47910, loss = 0.70 (7536.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:06:55.043527: step 47920, loss = 0.72 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:55.364623: step 47930, loss = 0.82 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:55.689145: step 47940, loss = 0.61 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:56.011394: step 47950, loss = 0.75 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:56.331263: step 47960, loss = 0.66 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:56.653558: step 47970, loss = 0.75 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:56.973568: step 47980, loss = 0.49 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:57.294700: step 47990, loss = 0.59 (7851.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:57.615713: step 48000, loss = 0.61 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:58.157773: step 48010, loss = 0.62 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:58.482681: step 48020, loss = 0.53 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:58.803743: step 48030, loss = 0.50 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:59.123795: step 48040, loss = 0.47 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:59.443169: step 48050, loss = 0.63 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:06:59.764682: step 48060, loss = 0.67 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:00.085806: step 48070, loss = 0.60 (7917.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:00.408000: step 48080, loss = 0.73 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:00.728892: step 48090, loss = 0.49 (7806.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:01.050360: step 48100, loss = 0.58 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:01.509379: step 48110, loss = 0.64 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:01.833716: step 48120, loss = 0.64 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:02.154512: step 48130, loss = 0.78 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:02.477132: step 48140, loss = 0.67 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:02.798389: step 48150, loss = 0.63 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:03.119798: step 48160, loss = 0.54 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:03.439536: step 48170, loss = 0.54 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:03.760348: step 48180, loss = 0.62 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:04.081704: step 48190, loss = 0.63 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:04.401873: step 48200, loss = 0.67 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:04.863514: step 48210, loss = 0.57 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:05.187216: step 48220, loss = 0.66 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:05.509262: step 48230, loss = 0.55 (8010.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:05.831258: step 48240, loss = 0.61 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:06.153481: step 48250, loss = 0.55 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:06.474583: step 48260, loss = 0.63 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:06.795492: step 48270, loss = 0.65 (7917.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:07.123372: step 48280, loss = 0.69 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:07.445045: step 48290, loss = 0.59 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:07.767438: step 48300, loss = 0.67 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:08.230370: step 48310, loss = 0.82 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:08.550490: step 48320, loss = 0.63 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:08.872535: step 48330, loss = 0.60 (7412.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:09.196582: step 48340, loss = 0.57 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:09.520644: step 48350, loss = 0.65 (7829.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:09.843106: step 48360, loss = 0.59 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:10.166045: step 48370, loss = 0.61 (7841.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:10.487707: step 48380, loss = 0.60 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:10.809884: step 48390, loss = 0.55 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:11.129015: step 48400, loss = 0.52 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:11.590218: step 48410, loss = 0.76 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:11.914544: step 48420, loss = 0.64 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:12.235774: step 48430, loss = 0.54 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:12.557515: step 48440, loss = 0.63 (8104.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:12.880604: step 48450, loss = 0.59 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:13.199263: step 48460, loss = 0.64 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:13.518938: step 48470, loss = 0.50 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:13.838661: step 48480, loss = 0.66 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:14.164003: step 48490, loss = 0.60 (7584.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:14.489224: step 48500, loss = 0.50 (7828.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:14.945900: step 48510, loss = 0.80 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:15.266701: step 48520, loss = 0.63 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:15.589314: step 48530, loss = 0.49 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:15.909114: step 48540, loss = 0.68 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:16.231894: step 48550, loss = 0.66 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:16.554047: step 48560, loss = 0.71 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:16.873676: step 48570, loss = 0.64 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:17.198057: step 48580, loss = 0.58 (7708.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:17.519244: step 48590, loss = 0.48 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:17.841138: step 48600, loss = 0.81 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:18.298787: step 48610, loss = 0.66 (7816.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:18.619430: step 48620, loss = 0.60 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:18.940201: step 48630, loss = 0.69 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:19.260431: step 48640, loss = 0.81 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:19.585559: step 48650, loss = 0.60 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:19.906473: step 48660, loss = 0.61 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:20.229628: step 48670, loss = 0.70 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:20.554299: step 48680, loss = 0.63 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:20.877638: step 48690, loss = 0.74 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:21.202814: step 48700, loss = 0.51 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:21.657827: step 48710, loss = 0.63 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:21.978717: step 48720, loss = 0.53 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:22.301549: step 48730, loss = 0.48 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:22.621130: step 48740, loss = 0.55 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:22.941512: step 48750, loss = 0.55 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:23.262548: step 48760, loss = 0.66 (7984.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:23.585578: step 48770, loss = 0.63 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:23.906673: step 48780, loss = 0.60 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:24.229370: step 48790, loss = 0.55 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:24.552000: step 48800, loss = 0.56 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:25.013924: step 48810, loss = 0.53 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:25.338610: step 48820, loss = 0.61 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:25.658662: step 48830, loss = 0.71 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:25.979248: step 48840, loss = 0.57 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:26.301215: step 48850, loss = 0.55 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:26.622728: step 48860, loss = 0.71 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:26.942328: step 48870, loss = 0.70 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:27.261283: step 48880, loss = 0.68 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:27.582722: step 48890, loss = 0.55 (7945.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:27.904693: step 48900, loss = 0.67 (7792.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:28.356978: step 48910, loss = 0.72 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:28.678474: step 48920, loss = 0.61 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:29.000092: step 48930, loss = 0.64 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:29.324579: step 48940, loss = 0.54 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:29.646883: step 48950, loss = 0.60 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:29.967182: step 48960, loss = 0.58 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:30.289660: step 48970, loss = 0.65 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:30.614996: step 48980, loss = 0.60 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:30.939469: step 48990, loss = 0.55 (7840.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:31.264053: step 49000, loss = 0.67 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:31.823239: step 49010, loss = 0.50 (7790.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:32.146485: step 49020, loss = 0.46 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:32.469652: step 49030, loss = 0.68 (7867.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:32.791436: step 49040, loss = 0.62 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:33.111965: step 49050, loss = 0.68 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:33.434477: step 49060, loss = 0.58 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:33.760951: step 49070, loss = 0.66 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:34.081421: step 49080, loss = 0.65 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:34.403430: step 49090, loss = 0.62 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:34.723624: step 49100, loss = 0.67 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:35.177827: step 49110, loss = 0.75 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:35.500976: step 49120, loss = 0.58 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:35.823684: step 49130, loss = 0.57 (8121.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:36.144350: step 49140, loss = 0.65 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:36.466333: step 49150, loss = 0.56 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:36.786435: step 49160, loss = 0.53 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:37.109771: step 49170, loss = 0.73 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:37.432980: step 49180, loss = 0.53 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:37.758048: step 49190, loss = 0.63 (7707.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:38.080945: step 49200, loss = 0.67 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:38.541334: step 49210, loss = 0.73 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:38.860327: step 49220, loss = 0.74 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:39.184409: step 49230, loss = 0.57 (8100.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:39.507654: step 49240, loss = 0.58 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:39.831011: step 49250, loss = 0.51 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:40.150828: step 49260, loss = 0.76 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:40.470498: step 49270, loss = 0.74 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:40.793297: step 49280, loss = 0.76 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:41.117364: step 49290, loss = 0.46 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:41.440727: step 49300, loss = 0.61 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:41.902120: step 49310, loss = 0.56 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:42.226524: step 49320, loss = 0.66 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:42.548333: step 49330, loss = 0.66 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:42.870229: step 49340, loss = 0.58 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:43.193468: step 49350, loss = 0.50 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:43.512869: step 49360, loss = 0.60 (8085.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:43.834560: step 49370, loss = 0.52 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:44.156110: step 49380, loss = 0.67 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:44.480488: step 49390, loss = 0.72 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:44.800249: step 49400, loss = 0.59 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:45.270288: step 49410, loss = 0.63 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:45.590312: step 49420, loss = 0.51 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:45.911279: step 49430, loss = 0.53 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:46.233361: step 49440, loss = 0.52 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:46.555709: step 49450, loss = 0.50 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:46.876689: step 49460, loss = 0.59 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:47.197096: step 49470, loss = 0.55 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:47.521050: step 49480, loss = 0.64 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:47.842027: step 49490, loss = 0.71 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:48.164868: step 49500, loss = 0.66 (7835.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:48.628600: step 49510, loss = 0.58 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:48.947894: step 49520, loss = 0.65 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:49.269665: step 49530, loss = 0.58 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:49.591009: step 49540, loss = 0.55 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:49.910685: step 49550, loss = 0.59 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:50.233316: step 49560, loss = 0.64 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:50.555711: step 49570, loss = 0.48 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:50.876335: step 49580, loss = 0.68 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:51.201834: step 49590, loss = 0.62 (7687.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:51.523487: step 49600, loss = 0.68 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:51.986907: step 49610, loss = 0.59 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:52.307136: step 49620, loss = 0.72 (7826.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:52.628479: step 49630, loss = 0.61 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:52.950271: step 49640, loss = 0.54 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:53.269989: step 49650, loss = 0.66 (8218.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:53.595718: step 49660, loss = 0.59 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:53.917815: step 49670, loss = 0.63 (7813.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:54.242223: step 49680, loss = 0.63 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:54.563164: step 49690, loss = 0.52 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:54.884785: step 49700, loss = 0.51 (7756.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:55.351902: step 49710, loss = 0.67 (7620.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:55.674105: step 49720, loss = 0.58 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:55.996721: step 49730, loss = 0.79 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:56.320778: step 49740, loss = 0.62 (8128.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:56.643544: step 49750, loss = 0.67 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:56.965487: step 49760, loss = 0.52 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:57.288177: step 49770, loss = 0.54 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:57.610840: step 49780, loss = 0.65 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:57.932061: step 49790, loss = 0.60 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:58.257677: step 49800, loss = 0.66 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:58.723914: step 49810, loss = 0.67 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:59.047958: step 49820, loss = 0.72 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:07:59.370401: step 49830, loss = 0.79 (7498.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:07:59.692079: step 49840, loss = 0.68 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:00.015497: step 49850, loss = 0.63 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:00.337707: step 49860, loss = 0.66 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:00.664271: step 49870, loss = 0.69 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:00.986719: step 49880, loss = 0.70 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:01.308436: step 49890, loss = 0.55 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:01.631461: step 49900, loss = 0.60 (7693.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:02.093720: step 49910, loss = 0.63 (7847.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:02.412147: step 49920, loss = 0.70 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:02.734881: step 49930, loss = 0.68 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:03.056808: step 49940, loss = 0.52 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:03.375385: step 49950, loss = 0.54 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:03.694798: step 49960, loss = 0.60 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:04.013750: step 49970, loss = 0.51 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:04.335624: step 49980, loss = 0.66 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:04.657691: step 49990, loss = 0.65 (7604.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:04.985300: step 50000, loss = 0.57 (7618.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:05.544970: step 50010, loss = 0.52 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:05.864550: step 50020, loss = 0.72 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:06.184699: step 50030, loss = 0.64 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:06.507285: step 50040, loss = 0.59 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:06.828533: step 50050, loss = 0.56 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:07.153892: step 50060, loss = 0.51 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:07.476252: step 50070, loss = 0.54 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:07.797402: step 50080, loss = 0.43 (8009.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:08.118880: step 50090, loss = 0.56 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:08.438558: step 50100, loss = 0.65 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:08.888721: step 50110, loss = 0.53 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:09.209766: step 50120, loss = 0.81 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:09.529420: step 50130, loss = 0.50 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:09.850290: step 50140, loss = 0.59 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:10.171455: step 50150, loss = 0.65 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:10.495369: step 50160, loss = 0.66 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:10.816440: step 50170, loss = 0.76 (8003.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:11.138709: step 50180, loss = 0.69 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:11.460714: step 50190, loss = 0.63 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:11.783529: step 50200, loss = 0.54 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:12.232793: step 50210, loss = 0.55 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:12.553941: step 50220, loss = 0.65 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:12.875633: step 50230, loss = 0.61 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:13.197804: step 50240, loss = 0.59 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:13.520560: step 50250, loss = 0.68 (7788.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:13.844723: step 50260, loss = 0.58 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:14.165750: step 50270, loss = 0.65 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:14.485596: step 50280, loss = 0.65 (8060.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:14.806573: step 50290, loss = 0.64 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:15.124782: step 50300, loss = 0.64 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:15.586631: step 50310, loss = 0.66 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:15.906169: step 50320, loss = 0.79 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:16.226288: step 50330, loss = 0.58 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:16.547315: step 50340, loss = 0.65 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:16.868064: step 50350, loss = 0.68 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:17.189343: step 50360, loss = 0.62 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:17.508570: step 50370, loss = 0.66 (7755.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:17.833001: step 50380, loss = 0.62 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:18.153812: step 50390, loss = 0.63 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:18.476017: step 50400, loss = 0.54 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:18.938723: step 50410, loss = 0.60 (7863.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:19.260099: step 50420, loss = 0.63 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:19.579847: step 50430, loss = 0.53 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:19.901245: step 50440, loss = 0.58 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:20.220170: step 50450, loss = 0.53 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:20.538715: step 50460, loss = 0.77 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:20.860565: step 50470, loss = 0.66 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:21.180725: step 50480, loss = 0.64 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:21.500762: step 50490, loss = 0.66 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:21.820915: step 50500, loss = 0.63 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:22.284573: step 50510, loss = 0.54 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:22.604034: step 50520, loss = 0.63 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:22.925709: step 50530, loss = 0.61 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:23.245293: step 50540, loss = 0.64 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:23.572099: step 50550, loss = 0.66 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:23.895003: step 50560, loss = 0.57 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:24.215700: step 50570, loss = 0.57 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:24.538114: step 50580, loss = 0.56 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:24.860978: step 50590, loss = 0.52 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:25.181661: step 50600, loss = 0.52 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:25.629285: step 50610, loss = 0.67 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:25.950752: step 50620, loss = 0.66 (7781.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:26.270597: step 50630, loss = 0.69 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:26.597395: step 50640, loss = 0.59 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:26.917928: step 50650, loss = 0.59 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:27.239640: step 50660, loss = 0.59 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:27.561232: step 50670, loss = 0.75 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:27.882083: step 50680, loss = 0.49 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:28.201801: step 50690, loss = 0.58 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:28.522723: step 50700, loss = 0.63 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:28.980298: step 50710, loss = 0.68 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:29.298159: step 50720, loss = 0.63 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:29.619516: step 50730, loss = 0.59 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:29.942194: step 50740, loss = 0.53 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:30.262314: step 50750, loss = 0.54 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:30.582838: step 50760, loss = 0.56 (8161.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:30.902337: step 50770, loss = 0.68 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:31.222325: step 50780, loss = 0.67 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:31.544067: step 50790, loss = 0.72 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:31.867286: step 50800, loss = 0.61 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:32.328062: step 50810, loss = 0.67 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:32.648834: step 50820, loss = 0.67 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:32.971123: step 50830, loss = 0.62 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:33.296714: step 50840, loss = 0.49 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:33.616387: step 50850, loss = 0.62 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:33.944509: step 50860, loss = 0.51 (7953.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:34.273927: step 50870, loss = 0.78 (7718.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:34.594149: step 50880, loss = 0.49 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:34.913570: step 50890, loss = 0.72 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:35.233871: step 50900, loss = 0.67 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:35.690004: step 50910, loss = 0.51 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:36.012123: step 50920, loss = 0.52 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:36.331548: step 50930, loss = 0.77 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:36.652935: step 50940, loss = 0.56 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:36.973021: step 50950, loss = 0.56 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:37.294262: step 50960, loss = 0.62 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:37.615056: step 50970, loss = 0.57 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:37.935030: step 50980, loss = 0.61 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:38.255930: step 50990, loss = 0.77 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:38.577161: step 51000, loss = 0.69 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:39.117407: step 51010, loss = 0.58 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:39.437202: step 51020, loss = 0.51 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:39.757386: step 51030, loss = 0.55 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:40.077490: step 51040, loss = 0.65 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:40.396665: step 51050, loss = 0.52 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:40.715008: step 51060, loss = 0.61 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:41.035416: step 51070, loss = 0.65 (7899.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:41.357429: step 51080, loss = 0.62 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:41.675523: step 51090, loss = 0.73 (8128.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:41.993532: step 51100, loss = 0.63 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:42.455394: step 51110, loss = 0.62 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:42.777821: step 51120, loss = 0.56 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:43.097277: step 51130, loss = 0.64 (7937.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:43.417956: step 51140, loss = 0.47 (7846.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:43.740225: step 51150, loss = 0.69 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:44.063075: step 51160, loss = 0.67 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:44.385980: step 51170, loss = 0.51 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:44.711955: step 51180, loss = 0.83 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:45.035317: step 51190, loss = 0.54 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:45.355382: step 51200, loss = 0.67 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:45.817419: step 51210, loss = 0.57 (7797.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:46.136877: step 51220, loss = 0.57 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:46.457988: step 51230, loss = 0.59 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:46.779301: step 51240, loss = 0.62 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:47.100573: step 51250, loss = 0.54 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:47.424870: step 51260, loss = 0.63 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:47.747248: step 51270, loss = 0.59 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:48.067571: step 51280, loss = 0.63 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:48.387354: step 51290, loss = 0.58 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:48.708092: step 51300, loss = 0.59 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:49.171729: step 51310, loss = 0.48 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:49.491212: step 51320, loss = 0.56 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:49.814465: step 51330, loss = 0.65 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:50.141201: step 51340, loss = 0.66 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:50.464389: step 51350, loss = 0.78 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:50.787739: step 51360, loss = 0.68 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:51.108476: step 51370, loss = 0.63 (7849.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:51.429143: step 51380, loss = 0.56 (8113.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:51.749261: step 51390, loss = 0.69 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:52.070845: step 51400, loss = 0.87 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:52.534888: step 51410, loss = 0.64 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:52.857151: step 51420, loss = 0.58 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:53.177138: step 51430, loss = 0.54 (8167.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:53.500967: step 51440, loss = 0.67 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:53.821786: step 51450, loss = 0.64 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:54.143748: step 51460, loss = 0.67 (7687.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:54.463579: step 51470, loss = 0.58 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:54.784412: step 51480, loss = 0.66 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:55.106574: step 51490, loss = 0.73 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:55.427177: step 51500, loss = 0.49 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:55.895504: step 51510, loss = 0.75 (7959.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:56.219014: step 51520, loss = 0.56 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:56.540300: step 51530, loss = 0.61 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:56.862624: step 51540, loss = 0.47 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:57.184848: step 51550, loss = 0.60 (7642.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:08:57.506540: step 51560, loss = 0.75 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:57.828341: step 51570, loss = 0.56 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:58.151890: step 51580, loss = 0.57 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:58.471687: step 51590, loss = 0.70 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:58.793873: step 51600, loss = 0.59 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:59.250579: step 51610, loss = 0.58 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:59.574509: step 51620, loss = 0.70 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:08:59.896971: step 51630, loss = 0.57 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:00.219229: step 51640, loss = 0.60 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:00.539320: step 51650, loss = 0.49 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:00.863502: step 51660, loss = 0.59 (8158.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:01.190771: step 51670, loss = 0.52 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:01.513239: step 51680, loss = 0.63 (7709.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:01.834940: step 51690, loss = 0.60 (7835.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:02.158759: step 51700, loss = 0.64 (7760.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:02.625665: step 51710, loss = 0.55 (7753.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:02.949444: step 51720, loss = 0.58 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:03.273632: step 51730, loss = 0.54 (8010.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:03.594794: step 51740, loss = 0.52 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:03.916996: step 51750, loss = 0.64 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:04.239043: step 51760, loss = 0.71 (7884.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:04.559197: step 51770, loss = 0.60 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:04.885511: step 51780, loss = 0.60 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:05.208374: step 51790, loss = 0.57 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:05.531114: step 51800, loss = 0.76 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:05.993083: step 51810, loss = 0.70 (7425.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:06.318526: step 51820, loss = 0.76 (7808.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:06.642344: step 51830, loss = 0.60 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:06.964720: step 51840, loss = 0.52 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:07.286410: step 51850, loss = 0.57 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:07.605791: step 51860, loss = 0.57 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:07.927067: step 51870, loss = 0.56 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:08.247224: step 51880, loss = 0.76 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:08.568796: step 51890, loss = 0.64 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:08.891795: step 51900, loss = 0.60 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:09.351049: step 51910, loss = 0.62 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:09.672372: step 51920, loss = 0.61 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:09.993980: step 51930, loss = 0.58 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:10.318341: step 51940, loss = 0.67 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:10.637718: step 51950, loss = 0.75 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:10.961902: step 51960, loss = 0.69 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:11.285451: step 51970, loss = 0.58 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:11.608459: step 51980, loss = 0.63 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:11.935424: step 51990, loss = 0.58 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:12.256085: step 52000, loss = 0.56 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:12.901239: step 52010, loss = 0.64 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:13.225247: step 52020, loss = 0.59 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:13.546940: step 52030, loss = 0.52 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:13.867658: step 52040, loss = 0.47 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:14.188824: step 52050, loss = 0.62 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:14.510368: step 52060, loss = 0.61 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:14.831642: step 52070, loss = 0.50 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:15.154359: step 52080, loss = 0.58 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:15.480960: step 52090, loss = 0.66 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:15.804198: step 52100, loss = 0.63 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:16.263575: step 52110, loss = 0.74 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:16.585721: step 52120, loss = 0.64 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:16.906203: step 52130, loss = 0.60 (7843.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:17.229092: step 52140, loss = 0.77 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:17.554475: step 52150, loss = 0.46 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:17.874202: step 52160, loss = 0.62 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:18.193396: step 52170, loss = 0.49 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:18.514970: step 52180, loss = 0.63 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:18.836103: step 52190, loss = 0.56 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:19.158808: step 52200, loss = 0.56 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:19.619252: step 52210, loss = 0.66 (7839.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:19.940307: step 52220, loss = 0.53 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:20.265721: step 52230, loss = 0.62 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:20.586729: step 52240, loss = 0.65 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:20.906858: step 52250, loss = 0.74 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:21.229917: step 52260, loss = 0.54 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:21.555201: step 52270, loss = 0.57 (7275.4 examples/sec; 0.018 sec/batch)
2017-09-16 16:09:21.874313: step 52280, loss = 0.61 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:22.195405: step 52290, loss = 0.49 (7818.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:22.516359: step 52300, loss = 0.63 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:22.962320: step 52310, loss = 0.66 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:23.283749: step 52320, loss = 0.80 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:23.604802: step 52330, loss = 0.63 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:23.930530: step 52340, loss = 0.70 (7660.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:24.251029: step 52350, loss = 0.62 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:24.570920: step 52360, loss = 0.56 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:24.890759: step 52370, loss = 0.57 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:25.210447: step 52380, loss = 0.73 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:25.530965: step 52390, loss = 0.59 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:25.851630: step 52400, loss = 0.63 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:26.316328: step 52410, loss = 0.59 (7836.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:26.637703: step 52420, loss = 0.64 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:26.958460: step 52430, loss = 0.68 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:27.279645: step 52440, loss = 0.72 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:27.599030: step 52450, loss = 0.55 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:27.920010: step 52460, loss = 0.60 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:28.242392: step 52470, loss = 0.64 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:28.563284: step 52480, loss = 0.64 (8034.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:28.884392: step 52490, loss = 0.78 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:29.205285: step 52500, loss = 0.63 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:29.663136: step 52510, loss = 0.65 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:29.984819: step 52520, loss = 0.52 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:30.304461: step 52530, loss = 0.65 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:30.628165: step 52540, loss = 0.56 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:30.948497: step 52550, loss = 0.57 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:31.269240: step 52560, loss = 0.63 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:31.590530: step 52570, loss = 0.59 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:31.909824: step 52580, loss = 0.52 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:32.229400: step 52590, loss = 0.55 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:32.550124: step 52600, loss = 0.56 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:33.012387: step 52610, loss = 0.70 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:33.334782: step 52620, loss = 0.65 (7610.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:33.658216: step 52630, loss = 0.62 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:33.979195: step 52640, loss = 0.58 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:34.300246: step 52650, loss = 0.57 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:34.621639: step 52660, loss = 0.65 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:34.941024: step 52670, loss = 0.77 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:35.261447: step 52680, loss = 0.68 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:35.581923: step 52690, loss = 0.64 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:35.902706: step 52700, loss = 0.61 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:36.371597: step 52710, loss = 0.63 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:36.694553: step 52720, loss = 0.69 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:37.018606: step 52730, loss = 0.85 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:37.339552: step 52740, loss = 0.66 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:37.660505: step 52750, loss = 0.51 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:37.981955: step 52760, loss = 0.58 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:38.303681: step 52770, loss = 0.67 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:38.629343: step 52780, loss = 0.69 (7457.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:38.952372: step 52790, loss = 0.55 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:39.275236: step 52800, loss = 0.58 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:39.724386: step 52810, loss = 0.67 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:40.044190: step 52820, loss = 0.63 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:40.365465: step 52830, loss = 0.61 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:40.685448: step 52840, loss = 0.55 (7838.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:41.007028: step 52850, loss = 0.62 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:41.329512: step 52860, loss = 0.51 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:41.652422: step 52870, loss = 0.67 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:41.972699: step 52880, loss = 0.58 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:42.293041: step 52890, loss = 0.65 (8095.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:42.612134: step 52900, loss = 0.64 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:43.074522: step 52910, loss = 0.66 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:43.395032: step 52920, loss = 0.68 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:43.716540: step 52930, loss = 0.65 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:44.038252: step 52940, loss = 0.75 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:44.358522: step 52950, loss = 0.76 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:44.677787: step 52960, loss = 0.73 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:45.000033: step 52970, loss = 0.59 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:45.325409: step 52980, loss = 0.63 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:45.651083: step 52990, loss = 0.55 (7827.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:45.975037: step 53000, loss = 0.73 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:46.536221: step 53010, loss = 0.65 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:46.858897: step 53020, loss = 0.68 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:47.180717: step 53030, loss = 0.75 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:47.505304: step 53040, loss = 0.50 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:47.827266: step 53050, loss = 0.57 (7807.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:48.149039: step 53060, loss = 0.81 (7645.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:48.469958: step 53070, loss = 0.58 (8127.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:48.791691: step 53080, loss = 0.59 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:49.113227: step 53090, loss = 0.61 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:49.433305: step 53100, loss = 0.67 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:49.889423: step 53110, loss = 0.54 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:50.210738: step 53120, loss = 0.54 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:50.533450: step 53130, loss = 0.75 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:50.856510: step 53140, loss = 0.52 (7814.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:51.178882: step 53150, loss = 0.77 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:51.501389: step 53160, loss = 0.58 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:51.824233: step 53170, loss = 0.63 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:52.146109: step 53180, loss = 0.68 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:52.465848: step 53190, loss = 0.53 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:52.789944: step 53200, loss = 0.65 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:53.250031: step 53210, loss = 0.75 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:53.570852: step 53220, loss = 0.53 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:53.892919: step 53230, loss = 0.64 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:54.218153: step 53240, loss = 0.53 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:54.545868: step 53250, loss = 0.68 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:54.869588: step 53260, loss = 0.59 (8034.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:55.192608: step 53270, loss = 0.61 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:55.511694: step 53280, loss = 0.58 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:55.833187: step 53290, loss = 0.60 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:56.156055: step 53300, loss = 0.60 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:56.616368: step 53310, loss = 0.60 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:56.936138: step 53320, loss = 0.58 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:57.260830: step 53330, loss = 0.69 (7602.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:09:57.581331: step 53340, loss = 0.64 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:57.904120: step 53350, loss = 0.53 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:58.224303: step 53360, loss = 0.65 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:58.545728: step 53370, loss = 0.53 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:58.868285: step 53380, loss = 0.71 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:59.187632: step 53390, loss = 0.67 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:59.509738: step 53400, loss = 0.71 (7937.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:09:59.971165: step 53410, loss = 0.61 (8082.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:00.291964: step 53420, loss = 0.58 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:00.612810: step 53430, loss = 0.66 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:00.931896: step 53440, loss = 0.68 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:01.252017: step 53450, loss = 0.68 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:01.572219: step 53460, loss = 0.73 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:01.892633: step 53470, loss = 0.55 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:02.223881: step 53480, loss = 0.70 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:02.543433: step 53490, loss = 0.51 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:02.864657: step 53500, loss = 0.51 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:03.327160: step 53510, loss = 0.63 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:03.648685: step 53520, loss = 0.63 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:03.974910: step 53530, loss = 0.79 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:04.294376: step 53540, loss = 0.66 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:04.615407: step 53550, loss = 0.69 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:04.936734: step 53560, loss = 0.50 (7776.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:05.256421: step 53570, loss = 0.64 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:05.577597: step 53580, loss = 0.59 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:05.899867: step 53590, loss = 0.55 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:06.220253: step 53600, loss = 0.52 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:06.684612: step 53610, loss = 0.57 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:07.006268: step 53620, loss = 0.53 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:07.327375: step 53630, loss = 0.71 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:07.647000: step 53640, loss = 0.63 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:07.968545: step 53650, loss = 0.62 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:08.289237: step 53660, loss = 0.59 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:08.612207: step 53670, loss = 0.67 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:08.932428: step 53680, loss = 0.58 (7928.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:09.253345: step 53690, loss = 0.51 (7945.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:09.574243: step 53700, loss = 0.75 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:10.036474: step 53710, loss = 0.60 (7908.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:10.361734: step 53720, loss = 0.73 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:10.681338: step 53730, loss = 0.54 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:11.002508: step 53740, loss = 0.58 (7833.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:11.321454: step 53750, loss = 0.51 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:11.641501: step 53760, loss = 0.78 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:11.960630: step 53770, loss = 0.53 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:12.279684: step 53780, loss = 0.69 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:12.599486: step 53790, loss = 0.52 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:12.919272: step 53800, loss = 0.51 (7811.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:13.385021: step 53810, loss = 0.68 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:13.707107: step 53820, loss = 0.50 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:14.027195: step 53830, loss = 0.49 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:14.347994: step 53840, loss = 0.53 (8077.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:14.670230: step 53850, loss = 0.62 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:14.988478: step 53860, loss = 0.67 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:15.311060: step 53870, loss = 0.55 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:15.633838: step 53880, loss = 0.64 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:15.954275: step 53890, loss = 0.84 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:16.274565: step 53900, loss = 0.58 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:16.732564: step 53910, loss = 0.54 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:17.053281: step 53920, loss = 0.59 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:17.371979: step 53930, loss = 0.63 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:17.690289: step 53940, loss = 0.54 (8208.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:18.013176: step 53950, loss = 0.66 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:18.333284: step 53960, loss = 0.58 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:18.657044: step 53970, loss = 0.65 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:18.979143: step 53980, loss = 0.92 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:19.297848: step 53990, loss = 0.56 (8151.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:19.617881: step 54000, loss = 0.57 (7835.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:20.190818: step 54010, loss = 0.57 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:20.510253: step 54020, loss = 0.57 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:20.830944: step 54030, loss = 0.83 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:21.155121: step 54040, loss = 0.67 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:21.477469: step 54050, loss = 0.66 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:21.797010: step 54060, loss = 0.58 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:22.116183: step 54070, loss = 0.56 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:22.436274: step 54080, loss = 0.68 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:22.756872: step 54090, loss = 0.58 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:23.077882: step 54100, loss = 0.73 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:23.532858: step 54110, loss = 0.62 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:23.854830: step 54120, loss = 0.79 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:24.179375: step 54130, loss = 0.50 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:24.501219: step 54140, loss = 0.65 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:24.822416: step 54150, loss = 0.56 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:25.143840: step 54160, loss = 0.50 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:25.464584: step 54170, loss = 0.68 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:25.788740: step 54180, loss = 0.57 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:26.109432: step 54190, loss = 0.52 (7884.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:26.431615: step 54200, loss = 0.53 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:26.897163: step 54210, loss = 0.62 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:27.217201: step 54220, loss = 0.59 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:27.538731: step 54230, loss = 0.60 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:27.858624: step 54240, loss = 0.60 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:28.177675: step 54250, loss = 0.74 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:28.502143: step 54260, loss = 0.57 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:28.821585: step 54270, loss = 0.67 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:29.141959: step 54280, loss = 0.67 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:29.465361: step 54290, loss = 0.65 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:29.784764: step 54300, loss = 0.59 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:30.236078: step 54310, loss = 0.57 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:30.555472: step 54320, loss = 0.65 (8143.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:30.876877: step 54330, loss = 0.63 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:31.195844: step 54340, loss = 0.49 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:31.515769: step 54350, loss = 0.63 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:31.834969: step 54360, loss = 0.56 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:32.154133: step 54370, loss = 0.71 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:32.473568: step 54380, loss = 0.59 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:32.796066: step 54390, loss = 0.64 (7654.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:10:33.115782: step 54400, loss = 0.83 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:33.578464: step 54410, loss = 0.55 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:33.899301: step 54420, loss = 0.60 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:34.219927: step 54430, loss = 0.60 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:34.539649: step 54440, loss = 0.61 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:34.860822: step 54450, loss = 0.55 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:35.181287: step 54460, loss = 0.53 (8127.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:35.502106: step 54470, loss = 0.49 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:35.827846: step 54480, loss = 0.54 (7488.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:10:36.147236: step 54490, loss = 0.48 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:36.466952: step 54500, loss = 0.55 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:36.916561: step 54510, loss = 0.73 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:37.239871: step 54520, loss = 0.55 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:37.559394: step 54530, loss = 0.65 (7920.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:37.884154: step 54540, loss = 0.66 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:38.208736: step 54550, loss = 0.62 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:38.529112: step 54560, loss = 0.51 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:38.849620: step 54570, loss = 0.60 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:39.171121: step 54580, loss = 0.65 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:39.492341: step 54590, loss = 0.58 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:39.817970: step 54600, loss = 0.61 (7510.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:10:40.274261: step 54610, loss = 0.69 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:40.594557: step 54620, loss = 0.74 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:40.916709: step 54630, loss = 0.75 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:41.235369: step 54640, loss = 0.66 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:41.555579: step 54650, loss = 0.80 (7956.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:41.875531: step 54660, loss = 0.63 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:42.197607: step 54670, loss = 0.70 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:42.516214: step 54680, loss = 0.71 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:42.836213: step 54690, loss = 0.60 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:43.160038: step 54700, loss = 0.58 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:43.622049: step 54710, loss = 0.62 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:43.942139: step 54720, loss = 0.63 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:44.263539: step 54730, loss = 0.45 (7849.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:44.585387: step 54740, loss = 0.52 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:44.912127: step 54750, loss = 0.57 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:45.233889: step 54760, loss = 0.57 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:45.559201: step 54770, loss = 0.62 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:45.882496: step 54780, loss = 0.53 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:46.201139: step 54790, loss = 0.59 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:46.522156: step 54800, loss = 0.60 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:46.983666: step 54810, loss = 0.47 (8154.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:47.302305: step 54820, loss = 0.69 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:47.624359: step 54830, loss = 0.57 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:47.943625: step 54840, loss = 0.67 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:48.265410: step 54850, loss = 0.74 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:48.585144: step 54860, loss = 0.75 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:48.903710: step 54870, loss = 0.58 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:49.222468: step 54880, loss = 0.53 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:49.544668: step 54890, loss = 0.54 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:49.866553: step 54900, loss = 0.49 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:50.330071: step 54910, loss = 0.68 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:50.649401: step 54920, loss = 0.55 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:50.969912: step 54930, loss = 0.57 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:51.289433: step 54940, loss = 0.73 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:51.609036: step 54950, loss = 0.56 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:51.928602: step 54960, loss = 0.63 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:52.256485: step 54970, loss = 0.72 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:52.577136: step 54980, loss = 0.55 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:52.897667: step 54990, loss = 0.54 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:53.218939: step 55000, loss = 0.64 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:53.776751: step 55010, loss = 0.59 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:54.096260: step 55020, loss = 0.56 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:54.416229: step 55030, loss = 0.56 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:54.737393: step 55040, loss = 0.52 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:55.057283: step 55050, loss = 0.63 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:55.375849: step 55060, loss = 0.55 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:55.695022: step 55070, loss = 0.61 (7942.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:56.017516: step 55080, loss = 0.56 (7802.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:56.339263: step 55090, loss = 0.61 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:56.660286: step 55100, loss = 0.69 (7807.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:57.123154: step 55110, loss = 0.68 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:57.442128: step 55120, loss = 0.70 (8094.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:57.764034: step 55130, loss = 0.60 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:58.084145: step 55140, loss = 0.55 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:58.406543: step 55150, loss = 0.55 (7944.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:58.727054: step 55160, loss = 0.52 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:59.048983: step 55170, loss = 0.59 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:59.369197: step 55180, loss = 0.73 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:10:59.690192: step 55190, loss = 0.73 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:00.011203: step 55200, loss = 0.63 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:00.470423: step 55210, loss = 0.72 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:00.790659: step 55220, loss = 0.52 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:01.113341: step 55230, loss = 0.55 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:01.435321: step 55240, loss = 0.59 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:01.756884: step 55250, loss = 0.87 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:02.076393: step 55260, loss = 0.59 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:02.398160: step 55270, loss = 0.76 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:02.718613: step 55280, loss = 0.61 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:03.038524: step 55290, loss = 0.56 (7820.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:03.359487: step 55300, loss = 0.59 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:03.822358: step 55310, loss = 0.56 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:04.145437: step 55320, loss = 0.74 (7875.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:04.466625: step 55330, loss = 0.67 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:04.787758: step 55340, loss = 0.71 (7808.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:05.110752: step 55350, loss = 0.69 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:05.432761: step 55360, loss = 0.67 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:05.756215: step 55370, loss = 0.60 (7318.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:11:06.076881: step 55380, loss = 0.64 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:06.397165: step 55390, loss = 0.72 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:06.715398: step 55400, loss = 0.58 (8149.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:07.168407: step 55410, loss = 0.69 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:07.490350: step 55420, loss = 0.77 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:07.809891: step 55430, loss = 0.58 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:08.129825: step 55440, loss = 0.57 (8140.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:08.450783: step 55450, loss = 0.72 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:08.769032: step 55460, loss = 0.57 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:09.090712: step 55470, loss = 0.75 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:09.410529: step 55480, loss = 0.64 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:09.730202: step 55490, loss = 0.56 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:10.052073: step 55500, loss = 0.77 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:10.502108: step 55510, loss = 0.66 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:10.819373: step 55520, loss = 0.61 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:11.140956: step 55530, loss = 0.54 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:11.462286: step 55540, loss = 0.67 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:11.782509: step 55550, loss = 0.56 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:12.102179: step 55560, loss = 0.61 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:12.422399: step 55570, loss = 0.66 (8083.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:12.744508: step 55580, loss = 0.60 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:13.063853: step 55590, loss = 0.74 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:13.385782: step 55600, loss = 0.74 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:13.844142: step 55610, loss = 0.42 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:14.165914: step 55620, loss = 0.68 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:14.487313: step 55630, loss = 0.67 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:14.807767: step 55640, loss = 0.69 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:15.128362: step 55650, loss = 0.45 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:15.449294: step 55660, loss = 0.57 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:15.768499: step 55670, loss = 0.56 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:16.088595: step 55680, loss = 0.73 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:16.407672: step 55690, loss = 0.71 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:16.728254: step 55700, loss = 0.63 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:17.189568: step 55710, loss = 0.63 (7659.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:11:17.514818: step 55720, loss = 0.63 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:17.836469: step 55730, loss = 0.68 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:18.155433: step 55740, loss = 0.73 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:18.476483: step 55750, loss = 0.66 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:18.796535: step 55760, loss = 0.52 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:19.119575: step 55770, loss = 0.61 (7768.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:19.441311: step 55780, loss = 0.66 (8154.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:19.762051: step 55790, loss = 0.58 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:20.083167: step 55800, loss = 0.60 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:20.535221: step 55810, loss = 0.73 (8125.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:20.856670: step 55820, loss = 0.56 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:21.181782: step 55830, loss = 0.77 (7610.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:11:21.502694: step 55840, loss = 0.49 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:21.825852: step 55850, loss = 0.64 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:22.146576: step 55860, loss = 0.65 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:22.467096: step 55870, loss = 0.64 (7824.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:22.786280: step 55880, loss = 0.52 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:23.108432: step 55890, loss = 0.53 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:23.428826: step 55900, loss = 0.62 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:23.893085: step 55910, loss = 0.69 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:24.212428: step 55920, loss = 0.67 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:24.532836: step 55930, loss = 0.51 (8155.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:24.854686: step 55940, loss = 0.69 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:25.176057: step 55950, loss = 0.66 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:25.496994: step 55960, loss = 0.56 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:25.815747: step 55970, loss = 0.74 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:26.135146: step 55980, loss = 0.51 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:26.453333: step 55990, loss = 0.67 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:26.773922: step 56000, loss = 0.58 (7839.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:27.333054: step 56010, loss = 0.56 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:27.654303: step 56020, loss = 0.56 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:27.973279: step 56030, loss = 0.64 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:28.294612: step 56040, loss = 0.44 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:28.615551: step 56050, loss = 0.54 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:28.938138: step 56060, loss = 0.71 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:29.258985: step 56070, loss = 0.64 (7987.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:29.579502: step 56080, loss = 0.62 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:29.899175: step 56090, loss = 0.65 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:30.220766: step 56100, loss = 0.56 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:30.677054: step 56110, loss = 0.55 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:30.998319: step 56120, loss = 0.57 (7809.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:31.317673: step 56130, loss = 0.59 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:31.640696: step 56140, loss = 0.59 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:31.961090: step 56150, loss = 0.64 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:32.281432: step 56160, loss = 0.62 (7861.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:32.600620: step 56170, loss = 0.63 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:32.924003: step 56180, loss = 0.65 (7809.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:33.244982: step 56190, loss = 0.57 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:33.567898: step 56200, loss = 0.61 (7818.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:34.027440: step 56210, loss = 0.53 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:34.348350: step 56220, loss = 0.87 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:34.667570: step 56230, loss = 0.70 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:34.987620: step 56240, loss = 0.65 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:35.306356: step 56250, loss = 0.63 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:35.635032: step 56260, loss = 0.74 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:35.956357: step 56270, loss = 0.62 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:36.277333: step 56280, loss = 0.52 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:36.599745: step 56290, loss = 0.65 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:36.919831: step 56300, loss = 0.57 (7972.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:37.392231: step 56310, loss = 0.70 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:37.712660: step 56320, loss = 0.64 (7937.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:38.032576: step 56330, loss = 0.51 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:38.352491: step 56340, loss = 0.84 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:38.675796: step 56350, loss = 0.58 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:38.998508: step 56360, loss = 0.70 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:39.324005: step 56370, loss = 0.52 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:39.643382: step 56380, loss = 0.62 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:39.965464: step 56390, loss = 0.67 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:40.286450: step 56400, loss = 0.56 (7887.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:40.746019: step 56410, loss = 0.70 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:41.066696: step 56420, loss = 0.67 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:41.387508: step 56430, loss = 0.63 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:41.707209: step 56440, loss = 0.49 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:42.029051: step 56450, loss = 0.57 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:42.347592: step 56460, loss = 0.65 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:42.669311: step 56470, loss = 0.66 (7798.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:42.991330: step 56480, loss = 0.60 (7807.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:43.312431: step 56490, loss = 0.62 (7884.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:43.632684: step 56500, loss = 0.64 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:44.096088: step 56510, loss = 0.51 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:44.419975: step 56520, loss = 0.49 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:44.742375: step 56530, loss = 0.59 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:45.064064: step 56540, loss = 0.69 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:45.391137: step 56550, loss = 0.66 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:45.715553: step 56560, loss = 0.58 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:46.036631: step 56570, loss = 0.63 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:46.355251: step 56580, loss = 0.49 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:46.679113: step 56590, loss = 0.58 (7668.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:11:46.999548: step 56600, loss = 0.64 (7805.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:47.468007: step 56610, loss = 0.60 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:47.789026: step 56620, loss = 0.55 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:48.110282: step 56630, loss = 0.69 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:48.430055: step 56640, loss = 0.62 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:48.750950: step 56650, loss = 0.49 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:49.072082: step 56660, loss = 0.77 (8026.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:49.397116: step 56670, loss = 0.61 (7488.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:11:49.720104: step 56680, loss = 0.60 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:50.039931: step 56690, loss = 0.57 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:50.361842: step 56700, loss = 0.66 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:50.819455: step 56710, loss = 0.49 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:51.142181: step 56720, loss = 0.64 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:51.464100: step 56730, loss = 0.78 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:51.787096: step 56740, loss = 0.56 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:52.108011: step 56750, loss = 0.69 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:52.428237: step 56760, loss = 0.47 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:52.749901: step 56770, loss = 0.53 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:53.070758: step 56780, loss = 0.55 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:53.391528: step 56790, loss = 0.62 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:53.712606: step 56800, loss = 0.55 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:54.169558: step 56810, loss = 0.52 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:54.490121: step 56820, loss = 0.65 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:54.811497: step 56830, loss = 0.53 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:55.136345: step 56840, loss = 0.67 (7963.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:55.457806: step 56850, loss = 0.72 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:55.780429: step 56860, loss = 0.61 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:56.101949: step 56870, loss = 0.65 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:56.422374: step 56880, loss = 0.49 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:56.742831: step 56890, loss = 0.51 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:57.066407: step 56900, loss = 0.64 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:57.528061: step 56910, loss = 0.50 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:57.850468: step 56920, loss = 0.50 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:58.171394: step 56930, loss = 0.53 (8121.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:58.493076: step 56940, loss = 0.65 (8050.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:58.812612: step 56950, loss = 0.57 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:59.133943: step 56960, loss = 0.57 (7813.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:59.456028: step 56970, loss = 0.65 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:11:59.775642: step 56980, loss = 0.61 (7789.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:00.095184: step 56990, loss = 0.63 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:00.414964: step 57000, loss = 0.65 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:00.962361: step 57010, loss = 0.58 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:01.282900: step 57020, loss = 0.68 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:01.603642: step 57030, loss = 0.57 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:01.924854: step 57040, loss = 0.62 (7736.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:02.244933: step 57050, loss = 0.61 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:02.565695: step 57060, loss = 0.78 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:02.888654: step 57070, loss = 0.55 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:03.208031: step 57080, loss = 0.59 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:03.529047: step 57090, loss = 0.57 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:03.849275: step 57100, loss = 0.59 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:04.309441: step 57110, loss = 0.66 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:04.630495: step 57120, loss = 0.52 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:04.954264: step 57130, loss = 0.53 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:05.273657: step 57140, loss = 0.61 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:05.594295: step 57150, loss = 0.56 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:05.916804: step 57160, loss = 0.68 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:06.238330: step 57170, loss = 0.75 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:06.556609: step 57180, loss = 0.62 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:06.878368: step 57190, loss = 0.64 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:07.199623: step 57200, loss = 0.52 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:07.669709: step 57210, loss = 0.58 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:07.992684: step 57220, loss = 0.57 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:08.314371: step 57230, loss = 0.59 (7762.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:08.633793: step 57240, loss = 0.68 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:08.956332: step 57250, loss = 0.55 (7774.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:09.277562: step 57260, loss = 0.60 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:09.597090: step 57270, loss = 0.59 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:09.920267: step 57280, loss = 0.58 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:10.241224: step 57290, loss = 0.75 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:10.561951: step 57300, loss = 0.59 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:11.022828: step 57310, loss = 0.41 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:11.343321: step 57320, loss = 0.54 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:11.664572: step 57330, loss = 0.62 (8077.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:11.982818: step 57340, loss = 0.72 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:12.302652: step 57350, loss = 0.64 (7919.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:12.621706: step 57360, loss = 0.70 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:12.942842: step 57370, loss = 0.60 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:13.264410: step 57380, loss = 0.61 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:13.586300: step 57390, loss = 0.71 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:13.905432: step 57400, loss = 0.62 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:14.368661: step 57410, loss = 0.56 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:14.690581: step 57420, loss = 0.65 (7830.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:15.012699: step 57430, loss = 0.64 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:15.332752: step 57440, loss = 0.69 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:15.653489: step 57450, loss = 0.63 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:15.973158: step 57460, loss = 0.59 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:16.294993: step 57470, loss = 0.77 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:16.616334: step 57480, loss = 0.61 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:16.940712: step 57490, loss = 0.50 (7509.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:17.263901: step 57500, loss = 0.55 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:17.715442: step 57510, loss = 0.60 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:18.035156: step 57520, loss = 0.70 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:18.355617: step 57530, loss = 0.56 (8120.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:18.677094: step 57540, loss = 0.57 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:18.997891: step 57550, loss = 0.66 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:19.318519: step 57560, loss = 0.54 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:19.638501: step 57570, loss = 0.58 (8153.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:19.957454: step 57580, loss = 0.62 (8219.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:20.278935: step 57590, loss = 0.70 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:20.599195: step 57600, loss = 0.64 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:21.056918: step 57610, loss = 0.53 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:21.377091: step 57620, loss = 0.59 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:21.696172: step 57630, loss = 0.69 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:22.015182: step 57640, loss = 0.64 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:22.337438: step 57650, loss = 0.51 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:22.656760: step 57660, loss = 0.66 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:22.980582: step 57670, loss = 0.68 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:23.303482: step 57680, loss = 0.63 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:23.622584: step 57690, loss = 0.69 (8130.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:23.942097: step 57700, loss = 0.62 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:24.398812: step 57710, loss = 0.52 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:24.720018: step 57720, loss = 0.51 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:25.041538: step 57730, loss = 0.67 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:25.363396: step 57740, loss = 0.55 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:25.681442: step 57750, loss = 0.68 (7973.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:26.001962: step 57760, loss = 0.70 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:26.322727: step 57770, loss = 0.70 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:26.645062: step 57780, loss = 0.63 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:26.963906: step 57790, loss = 0.72 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:27.286164: step 57800, loss = 0.72 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:27.736761: step 57810, loss = 0.54 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:28.058511: step 57820, loss = 0.60 (8153.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:28.379343: step 57830, loss = 0.63 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:28.700421: step 57840, loss = 0.66 (7666.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:29.022930: step 57850, loss = 0.56 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:29.343933: step 57860, loss = 0.64 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:29.664501: step 57870, loss = 0.56 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:29.987361: step 57880, loss = 0.64 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:30.307100: step 57890, loss = 0.51 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:30.627282: step 57900, loss = 0.61 (7608.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:31.103162: step 57910, loss = 0.65 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:31.427186: step 57920, loss = 0.73 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:31.746475: step 57930, loss = 0.58 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:32.067071: step 57940, loss = 0.69 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:32.387821: step 57950, loss = 0.63 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:32.708550: step 57960, loss = 0.54 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:33.028971: step 57970, loss = 0.63 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:33.347226: step 57980, loss = 0.56 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:33.667006: step 57990, loss = 0.70 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:33.989574: step 58000, loss = 0.62 (8033.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:34.554783: step 58010, loss = 0.67 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:34.876278: step 58020, loss = 0.59 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:35.197777: step 58030, loss = 0.68 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:35.519634: step 58040, loss = 0.55 (7859.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:35.839682: step 58050, loss = 0.65 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:36.162263: step 58060, loss = 0.66 (7972.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:36.481717: step 58070, loss = 0.61 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:36.801671: step 58080, loss = 0.70 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:37.120549: step 58090, loss = 0.52 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:37.447551: step 58100, loss = 0.62 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:37.912282: step 58110, loss = 0.61 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:38.235951: step 58120, loss = 0.67 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:38.555514: step 58130, loss = 0.63 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:38.880673: step 58140, loss = 0.58 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:39.201190: step 58150, loss = 0.65 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:39.525918: step 58160, loss = 0.55 (7955.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:39.849309: step 58170, loss = 0.55 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:40.169844: step 58180, loss = 0.58 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:40.492585: step 58190, loss = 0.61 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:40.815572: step 58200, loss = 0.62 (7767.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:41.285853: step 58210, loss = 0.66 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:41.607669: step 58220, loss = 0.60 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:41.927358: step 58230, loss = 0.53 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:42.249038: step 58240, loss = 0.69 (7848.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:42.571985: step 58250, loss = 0.60 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:42.890862: step 58260, loss = 0.55 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:43.211180: step 58270, loss = 0.61 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:43.530807: step 58280, loss = 0.59 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:43.850582: step 58290, loss = 0.63 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:44.172559: step 58300, loss = 0.63 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:44.635459: step 58310, loss = 0.75 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:44.957389: step 58320, loss = 0.57 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:45.276837: step 58330, loss = 0.46 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:45.600358: step 58340, loss = 0.47 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:45.920236: step 58350, loss = 0.70 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:46.242163: step 58360, loss = 0.65 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:46.562097: step 58370, loss = 0.63 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:46.883555: step 58380, loss = 0.48 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:47.201365: step 58390, loss = 0.64 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:47.524807: step 58400, loss = 0.48 (7790.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:47.979445: step 58410, loss = 0.63 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:48.300566: step 58420, loss = 0.78 (8096.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:48.620662: step 58430, loss = 0.69 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:48.943279: step 58440, loss = 0.59 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:49.265229: step 58450, loss = 0.64 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:49.586160: step 58460, loss = 0.58 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:49.907632: step 58470, loss = 0.58 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:50.228742: step 58480, loss = 0.58 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:50.548950: step 58490, loss = 0.71 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:50.869225: step 58500, loss = 0.64 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:51.324517: step 58510, loss = 0.65 (7528.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:51.647929: step 58520, loss = 0.53 (7519.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:51.969463: step 58530, loss = 0.54 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:52.289143: step 58540, loss = 0.63 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:52.608952: step 58550, loss = 0.54 (7973.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:52.931201: step 58560, loss = 0.53 (7668.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:12:53.252600: step 58570, loss = 0.57 (7808.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:53.571919: step 58580, loss = 0.66 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:53.891982: step 58590, loss = 0.61 (7855.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:54.212195: step 58600, loss = 0.59 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:54.681146: step 58610, loss = 0.61 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:55.000151: step 58620, loss = 0.69 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:55.319978: step 58630, loss = 0.55 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:55.640363: step 58640, loss = 0.71 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:55.961945: step 58650, loss = 0.74 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:56.281854: step 58660, loss = 0.66 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:56.602836: step 58670, loss = 0.72 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:56.924366: step 58680, loss = 0.68 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:57.244962: step 58690, loss = 0.68 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:57.565298: step 58700, loss = 0.69 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:58.025502: step 58710, loss = 0.59 (8144.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:58.346290: step 58720, loss = 0.55 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:58.667435: step 58730, loss = 0.74 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:58.986673: step 58740, loss = 0.54 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:59.306218: step 58750, loss = 0.73 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:59.628855: step 58760, loss = 0.58 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:12:59.947793: step 58770, loss = 0.63 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:00.270725: step 58780, loss = 0.70 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:00.591634: step 58790, loss = 0.68 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:00.914837: step 58800, loss = 0.63 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:01.379998: step 58810, loss = 0.66 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:01.700716: step 58820, loss = 0.56 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:02.022866: step 58830, loss = 0.86 (7853.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:02.343607: step 58840, loss = 0.58 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:02.666487: step 58850, loss = 0.71 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:02.988604: step 58860, loss = 0.61 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:03.311164: step 58870, loss = 0.56 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:03.638254: step 58880, loss = 0.69 (7766.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:03.961646: step 58890, loss = 0.66 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:04.282874: step 58900, loss = 0.75 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:04.748751: step 58910, loss = 0.66 (7723.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:13:05.070224: step 58920, loss = 0.47 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:05.391869: step 58930, loss = 0.66 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:05.714246: step 58940, loss = 0.54 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:06.035126: step 58950, loss = 0.43 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:06.354500: step 58960, loss = 0.66 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:06.676583: step 58970, loss = 0.56 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:06.997189: step 58980, loss = 0.65 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:07.323076: step 58990, loss = 0.57 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:07.643291: step 59000, loss = 0.52 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:08.193764: step 59010, loss = 0.56 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:08.515161: step 59020, loss = 0.63 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:08.834873: step 59030, loss = 0.66 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:09.156154: step 59040, loss = 0.56 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:09.476806: step 59050, loss = 0.61 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:09.795953: step 59060, loss = 0.52 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:10.115776: step 59070, loss = 0.59 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:10.439298: step 59080, loss = 0.57 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:10.758528: step 59090, loss = 0.56 (7913.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:11.080473: step 59100, loss = 0.63 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:11.533597: step 59110, loss = 0.53 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:11.855455: step 59120, loss = 0.64 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:12.175211: step 59130, loss = 0.52 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:12.496374: step 59140, loss = 0.51 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:12.816828: step 59150, loss = 0.62 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:13.141433: step 59160, loss = 0.62 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:13.464533: step 59170, loss = 0.49 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:13.786430: step 59180, loss = 0.78 (7862.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:14.106924: step 59190, loss = 0.58 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:14.427293: step 59200, loss = 0.57 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:14.881739: step 59210, loss = 0.60 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:15.202733: step 59220, loss = 0.58 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:15.524702: step 59230, loss = 0.63 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:15.844587: step 59240, loss = 0.68 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:16.163538: step 59250, loss = 0.57 (7894.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:16.482384: step 59260, loss = 0.72 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:16.801839: step 59270, loss = 0.52 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:17.120444: step 59280, loss = 0.63 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:17.440091: step 59290, loss = 0.71 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:17.762064: step 59300, loss = 0.74 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:18.227664: step 59310, loss = 0.62 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:18.547953: step 59320, loss = 0.66 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:18.870808: step 59330, loss = 0.67 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:19.190964: step 59340, loss = 0.51 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:19.510713: step 59350, loss = 0.49 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:19.832344: step 59360, loss = 0.53 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:20.152676: step 59370, loss = 0.60 (7806.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:20.472280: step 59380, loss = 0.64 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:20.794508: step 59390, loss = 0.57 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:21.112750: step 59400, loss = 0.55 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:21.574655: step 59410, loss = 0.55 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:21.896768: step 59420, loss = 0.62 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:22.218325: step 59430, loss = 0.71 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:22.538455: step 59440, loss = 0.54 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:22.860162: step 59450, loss = 0.71 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:23.178396: step 59460, loss = 0.58 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:23.497938: step 59470, loss = 0.66 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:23.818182: step 59480, loss = 0.75 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:24.138544: step 59490, loss = 0.65 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:24.459379: step 59500, loss = 0.49 (7924.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:24.919086: step 59510, loss = 0.65 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:25.240955: step 59520, loss = 0.58 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:25.562932: step 59530, loss = 0.66 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:25.881872: step 59540, loss = 0.51 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:26.200048: step 59550, loss = 0.62 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:26.519547: step 59560, loss = 0.56 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:26.839947: step 59570, loss = 0.60 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:27.159981: step 59580, loss = 0.58 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:27.481085: step 59590, loss = 0.63 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:27.800670: step 59600, loss = 0.50 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:28.269211: step 59610, loss = 0.53 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:28.589189: step 59620, loss = 0.54 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:28.909340: step 59630, loss = 0.76 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:29.229464: step 59640, loss = 0.52 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:29.549915: step 59650, loss = 0.56 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:29.871322: step 59660, loss = 0.47 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:30.195501: step 59670, loss = 0.61 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:30.518330: step 59680, loss = 0.58 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:30.838957: step 59690, loss = 0.57 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:31.157348: step 59700, loss = 0.54 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:31.616712: step 59710, loss = 0.58 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:31.937365: step 59720, loss = 0.85 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:32.256325: step 59730, loss = 0.78 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:32.579979: step 59740, loss = 0.71 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:32.904055: step 59750, loss = 0.50 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:33.224494: step 59760, loss = 0.60 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:33.543939: step 59770, loss = 0.61 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:33.862124: step 59780, loss = 0.46 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:34.182117: step 59790, loss = 0.54 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:34.503738: step 59800, loss = 0.51 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:34.964207: step 59810, loss = 0.67 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:35.285438: step 59820, loss = 0.58 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:35.608059: step 59830, loss = 0.69 (7481.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:13:35.931183: step 59840, loss = 0.62 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:36.252168: step 59850, loss = 0.71 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:36.573574: step 59860, loss = 0.54 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:36.894568: step 59870, loss = 0.60 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:37.215055: step 59880, loss = 0.57 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:37.536662: step 59890, loss = 0.59 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:37.858641: step 59900, loss = 0.53 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:38.308129: step 59910, loss = 0.55 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:38.627847: step 59920, loss = 0.49 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:38.947207: step 59930, loss = 0.66 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:39.267149: step 59940, loss = 0.69 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:39.586924: step 59950, loss = 0.66 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:39.907959: step 59960, loss = 0.67 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:40.227228: step 59970, loss = 0.49 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:40.547108: step 59980, loss = 0.57 (8172.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:40.866487: step 59990, loss = 0.58 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:41.186262: step 60000, loss = 0.57 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:41.806831: step 60010, loss = 0.58 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:42.124765: step 60020, loss = 0.71 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:42.447056: step 60030, loss = 0.57 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:42.766403: step 60040, loss = 0.57 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:43.087280: step 60050, loss = 0.64 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:43.406397: step 60060, loss = 0.77 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:43.726483: step 60070, loss = 0.65 (7652.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:13:44.045707: step 60080, loss = 0.51 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:44.366134: step 60090, loss = 0.62 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:44.690553: step 60100, loss = 0.66 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:45.143348: step 60110, loss = 0.63 (8181.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:45.470060: step 60120, loss = 0.52 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:45.791816: step 60130, loss = 0.62 (7917.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:46.112462: step 60140, loss = 0.62 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:46.432608: step 60150, loss = 0.58 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:46.753248: step 60160, loss = 0.58 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:47.073787: step 60170, loss = 0.63 (7793.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:47.396747: step 60180, loss = 0.65 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:47.721325: step 60190, loss = 0.62 (6989.6 examples/sec; 0.018 sec/batch)
2017-09-16 16:13:48.044604: step 60200, loss = 0.54 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:48.509451: step 60210, loss = 0.58 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:48.828353: step 60220, loss = 0.64 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:49.147610: step 60230, loss = 0.55 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:49.467854: step 60240, loss = 0.67 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:49.788487: step 60250, loss = 0.56 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:50.110218: step 60260, loss = 0.54 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:50.433225: step 60270, loss = 0.60 (7383.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:13:50.752323: step 60280, loss = 0.76 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:51.075503: step 60290, loss = 0.65 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:51.395589: step 60300, loss = 0.60 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:51.861317: step 60310, loss = 0.61 (8127.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:52.181446: step 60320, loss = 0.59 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:52.500905: step 60330, loss = 0.62 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:52.821403: step 60340, loss = 0.56 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:53.146497: step 60350, loss = 0.70 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:53.468398: step 60360, loss = 0.53 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:53.792001: step 60370, loss = 0.49 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:54.113849: step 60380, loss = 0.55 (7909.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:54.434239: step 60390, loss = 0.68 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:54.756010: step 60400, loss = 0.71 (7791.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:55.205480: step 60410, loss = 0.58 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:55.530585: step 60420, loss = 0.50 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:55.848916: step 60430, loss = 0.65 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:56.171210: step 60440, loss = 0.61 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:56.493544: step 60450, loss = 0.62 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:56.812936: step 60460, loss = 0.68 (7829.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:57.131790: step 60470, loss = 0.78 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:57.451892: step 60480, loss = 0.59 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:57.780051: step 60490, loss = 0.74 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:58.102448: step 60500, loss = 0.69 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:58.552193: step 60510, loss = 0.54 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:58.876085: step 60520, loss = 0.69 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:59.195568: step 60530, loss = 0.54 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:59.515968: step 60540, loss = 0.56 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:13:59.836199: step 60550, loss = 0.61 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:00.154605: step 60560, loss = 0.50 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:00.474245: step 60570, loss = 0.54 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:00.792526: step 60580, loss = 0.68 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:01.113055: step 60590, loss = 0.52 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:01.433172: step 60600, loss = 0.56 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:01.879632: step 60610, loss = 0.61 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:02.200553: step 60620, loss = 0.61 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:02.520199: step 60630, loss = 0.60 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:02.839112: step 60640, loss = 0.66 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:03.158058: step 60650, loss = 0.61 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:03.477887: step 60660, loss = 0.63 (7899.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:03.799129: step 60670, loss = 0.56 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:04.119051: step 60680, loss = 0.63 (7894.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:04.439146: step 60690, loss = 0.59 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:04.759246: step 60700, loss = 0.51 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:05.217574: step 60710, loss = 0.71 (8120.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:05.536483: step 60720, loss = 0.59 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:05.858739: step 60730, loss = 0.66 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:06.177869: step 60740, loss = 0.47 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:06.501697: step 60750, loss = 0.51 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:06.822515: step 60760, loss = 0.69 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:07.141887: step 60770, loss = 0.71 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:07.463597: step 60780, loss = 0.58 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:07.782598: step 60790, loss = 0.59 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:08.102028: step 60800, loss = 0.52 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:08.563077: step 60810, loss = 0.62 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:08.884980: step 60820, loss = 0.51 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:09.206217: step 60830, loss = 0.59 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:09.525593: step 60840, loss = 0.63 (7644.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:14:09.852874: step 60850, loss = 0.60 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:10.172885: step 60860, loss = 0.61 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:10.496221: step 60870, loss = 0.48 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:10.815683: step 60880, loss = 0.65 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:11.137244: step 60890, loss = 0.66 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:11.457956: step 60900, loss = 0.58 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:11.919315: step 60910, loss = 0.68 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:12.239755: step 60920, loss = 0.49 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:12.560758: step 60930, loss = 0.56 (7964.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:12.880160: step 60940, loss = 0.67 (7974.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:13.198830: step 60950, loss = 0.66 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:13.520312: step 60960, loss = 0.64 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:13.840333: step 60970, loss = 0.69 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:14.160981: step 60980, loss = 0.57 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:14.479239: step 60990, loss = 0.56 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:14.799007: step 61000, loss = 0.51 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:15.363464: step 61010, loss = 0.67 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:15.685787: step 61020, loss = 0.61 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:16.008447: step 61030, loss = 0.55 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:16.331090: step 61040, loss = 0.62 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:16.652711: step 61050, loss = 0.63 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:16.973112: step 61060, loss = 0.61 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:17.296278: step 61070, loss = 0.85 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:17.618024: step 61080, loss = 0.74 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:17.938090: step 61090, loss = 0.60 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:18.258999: step 61100, loss = 0.63 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:18.722810: step 61110, loss = 0.47 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:19.044001: step 61120, loss = 0.61 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:19.366518: step 61130, loss = 0.61 (7843.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:19.688015: step 61140, loss = 0.76 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:20.011171: step 61150, loss = 0.60 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:20.334366: step 61160, loss = 0.51 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:20.656893: step 61170, loss = 0.64 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:20.976195: step 61180, loss = 0.66 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:21.296761: step 61190, loss = 0.47 (7825.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:21.620345: step 61200, loss = 0.70 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:22.084647: step 61210, loss = 0.59 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:22.408748: step 61220, loss = 0.58 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:22.731366: step 61230, loss = 0.57 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:23.052355: step 61240, loss = 0.56 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:23.372308: step 61250, loss = 0.60 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:23.691937: step 61260, loss = 0.55 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:24.011971: step 61270, loss = 0.67 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:24.330880: step 61280, loss = 0.63 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:24.654587: step 61290, loss = 0.53 (7565.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:14:24.973874: step 61300, loss = 0.57 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:25.423450: step 61310, loss = 0.51 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:25.743545: step 61320, loss = 0.53 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:26.065303: step 61330, loss = 0.67 (7926.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:26.384982: step 61340, loss = 0.73 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:26.705370: step 61350, loss = 0.54 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:27.027158: step 61360, loss = 0.56 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:27.352417: step 61370, loss = 0.56 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:27.677275: step 61380, loss = 0.69 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:27.998425: step 61390, loss = 0.48 (7821.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:28.326540: step 61400, loss = 0.61 (7884.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:28.779024: step 61410, loss = 0.54 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:29.101182: step 61420, loss = 0.55 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:29.420375: step 61430, loss = 0.73 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:29.741219: step 61440, loss = 0.57 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:30.063066: step 61450, loss = 0.59 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:30.384461: step 61460, loss = 0.57 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:30.703487: step 61470, loss = 0.62 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:31.023771: step 61480, loss = 0.69 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:31.344787: step 61490, loss = 0.42 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:31.664490: step 61500, loss = 0.71 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:32.118781: step 61510, loss = 0.60 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:32.441725: step 61520, loss = 0.68 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:32.764230: step 61530, loss = 0.58 (7384.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:14:33.085568: step 61540, loss = 0.47 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:33.407147: step 61550, loss = 0.64 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:33.727930: step 61560, loss = 0.64 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:34.047415: step 61570, loss = 0.73 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:34.366278: step 61580, loss = 0.56 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:34.686515: step 61590, loss = 0.57 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:35.006608: step 61600, loss = 0.78 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:35.474550: step 61610, loss = 0.76 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:35.794871: step 61620, loss = 0.54 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:36.114094: step 61630, loss = 0.65 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:36.433566: step 61640, loss = 0.64 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:36.753880: step 61650, loss = 0.55 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:37.073405: step 61660, loss = 0.65 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:37.393870: step 61670, loss = 0.63 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:37.714173: step 61680, loss = 0.73 (7976.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:38.031630: step 61690, loss = 0.54 (8145.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:38.349244: step 61700, loss = 0.54 (8136.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:38.812679: step 61710, loss = 0.66 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:39.131577: step 61720, loss = 0.53 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:39.451685: step 61730, loss = 0.58 (8153.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:39.771260: step 61740, loss = 0.64 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:40.090584: step 61750, loss = 0.60 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:40.412491: step 61760, loss = 0.57 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:40.733079: step 61770, loss = 0.78 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:41.052451: step 61780, loss = 0.66 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:41.370598: step 61790, loss = 0.53 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:41.690912: step 61800, loss = 0.61 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:42.136875: step 61810, loss = 0.49 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:42.458382: step 61820, loss = 0.58 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:42.776787: step 61830, loss = 0.66 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:43.098153: step 61840, loss = 0.56 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:43.417467: step 61850, loss = 0.74 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:43.736252: step 61860, loss = 0.59 (8142.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:44.061362: step 61870, loss = 0.61 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:44.381659: step 61880, loss = 0.58 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:44.703309: step 61890, loss = 0.56 (7787.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:45.022703: step 61900, loss = 0.61 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:45.482183: step 61910, loss = 0.64 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:45.800685: step 61920, loss = 0.65 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:46.122975: step 61930, loss = 0.63 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:46.443737: step 61940, loss = 0.60 (8165.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:46.762395: step 61950, loss = 0.77 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:47.084407: step 61960, loss = 0.76 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:47.404587: step 61970, loss = 0.50 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:47.724608: step 61980, loss = 0.68 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:48.043901: step 61990, loss = 0.55 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:48.363991: step 62000, loss = 0.56 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:48.915773: step 62010, loss = 0.53 (8103.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:49.235328: step 62020, loss = 0.62 (8156.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:49.556558: step 62030, loss = 0.58 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:49.875982: step 62040, loss = 0.50 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:50.194820: step 62050, loss = 0.64 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:50.516155: step 62060, loss = 0.61 (7954.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:50.837000: step 62070, loss = 0.74 (7694.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:14:51.156697: step 62080, loss = 0.64 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:51.477573: step 62090, loss = 0.56 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:51.797285: step 62100, loss = 0.55 (8130.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:52.261876: step 62110, loss = 0.74 (8044.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:52.583581: step 62120, loss = 0.77 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:52.903562: step 62130, loss = 0.50 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:53.224169: step 62140, loss = 0.54 (7876.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:53.544388: step 62150, loss = 0.66 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:53.866821: step 62160, loss = 0.64 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:54.188586: step 62170, loss = 0.68 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:54.508496: step 62180, loss = 0.51 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:54.830103: step 62190, loss = 0.64 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:55.155080: step 62200, loss = 0.63 (7594.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:14:55.617162: step 62210, loss = 0.57 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:55.938324: step 62220, loss = 0.59 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:56.258606: step 62230, loss = 0.60 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:56.579067: step 62240, loss = 0.62 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:56.900237: step 62250, loss = 0.55 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:57.221928: step 62260, loss = 0.65 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:57.550541: step 62270, loss = 0.59 (7844.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:57.872989: step 62280, loss = 0.63 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:58.193529: step 62290, loss = 0.64 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:58.515968: step 62300, loss = 0.59 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:58.976849: step 62310, loss = 0.58 (7843.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:59.297841: step 62320, loss = 0.59 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:59.620686: step 62330, loss = 0.61 (7836.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:14:59.940671: step 62340, loss = 0.59 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:00.263352: step 62350, loss = 0.60 (7535.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:00.583384: step 62360, loss = 0.60 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:00.903993: step 62370, loss = 0.53 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:01.222428: step 62380, loss = 0.71 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:01.542011: step 62390, loss = 0.73 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:01.861602: step 62400, loss = 0.64 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:02.320844: step 62410, loss = 0.50 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:02.644374: step 62420, loss = 0.54 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:02.965174: step 62430, loss = 0.50 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:03.285032: step 62440, loss = 0.65 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:03.605086: step 62450, loss = 0.65 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:03.927540: step 62460, loss = 0.45 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:04.248686: step 62470, loss = 0.66 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:04.568506: step 62480, loss = 0.70 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:04.897632: step 62490, loss = 0.61 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:05.218878: step 62500, loss = 0.58 (7641.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:05.682193: step 62510, loss = 0.63 (7874.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:06.001773: step 62520, loss = 0.54 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:06.321762: step 62530, loss = 0.57 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:06.639459: step 62540, loss = 0.58 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:06.957844: step 62550, loss = 0.72 (8151.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:07.280175: step 62560, loss = 0.79 (7925.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:07.600481: step 62570, loss = 0.74 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:07.920838: step 62580, loss = 0.71 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:08.242818: step 62590, loss = 0.54 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:08.567376: step 62600, loss = 0.61 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:09.024123: step 62610, loss = 0.57 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:09.344731: step 62620, loss = 0.62 (7787.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:09.664039: step 62630, loss = 0.54 (8138.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:09.983405: step 62640, loss = 0.63 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:10.302247: step 62650, loss = 0.71 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:10.622015: step 62660, loss = 0.73 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:10.943341: step 62670, loss = 0.56 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:11.263176: step 62680, loss = 0.62 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:11.583050: step 62690, loss = 0.56 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:11.902646: step 62700, loss = 0.57 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:12.348314: step 62710, loss = 0.59 (8176.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:12.667370: step 62720, loss = 0.64 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:12.988220: step 62730, loss = 0.76 (7810.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:13.308689: step 62740, loss = 0.58 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:13.629128: step 62750, loss = 0.62 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:13.949514: step 62760, loss = 0.57 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:14.270558: step 62770, loss = 0.62 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:14.592506: step 62780, loss = 0.62 (7788.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:14.914294: step 62790, loss = 0.75 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:15.235186: step 62800, loss = 0.52 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:15.693898: step 62810, loss = 0.74 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:16.018095: step 62820, loss = 0.65 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:16.337997: step 62830, loss = 0.62 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:16.655829: step 62840, loss = 0.63 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:16.977552: step 62850, loss = 0.59 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:17.297840: step 62860, loss = 0.58 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:17.619359: step 62870, loss = 0.61 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:17.941406: step 62880, loss = 0.63 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:18.262532: step 62890, loss = 0.52 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:18.585556: step 62900, loss = 0.54 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:19.050246: step 62910, loss = 0.65 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:19.370353: step 62920, loss = 0.60 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:19.691952: step 62930, loss = 0.61 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:20.011412: step 62940, loss = 0.58 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:20.331936: step 62950, loss = 0.64 (7830.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:20.654449: step 62960, loss = 0.63 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:20.976177: step 62970, loss = 0.68 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:21.300600: step 62980, loss = 0.54 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:21.623061: step 62990, loss = 0.69 (7472.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:21.942682: step 63000, loss = 0.52 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:22.499826: step 63010, loss = 0.59 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:22.818287: step 63020, loss = 0.66 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:23.138880: step 63030, loss = 0.55 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:23.461216: step 63040, loss = 0.60 (7547.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:23.781703: step 63050, loss = 0.72 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:24.101811: step 63060, loss = 0.50 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:24.421789: step 63070, loss = 0.61 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:24.743127: step 63080, loss = 0.42 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:25.063707: step 63090, loss = 0.64 (7834.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:25.384207: step 63100, loss = 0.54 (8016.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:25.834236: step 63110, loss = 0.61 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:26.155286: step 63120, loss = 0.59 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:26.475725: step 63130, loss = 0.58 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:26.797492: step 63140, loss = 0.65 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:27.121756: step 63150, loss = 0.56 (7961.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:27.441705: step 63160, loss = 0.60 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:27.761353: step 63170, loss = 0.56 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:28.082559: step 63180, loss = 0.53 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:28.404292: step 63190, loss = 0.71 (7867.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:28.722792: step 63200, loss = 0.51 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:29.185687: step 63210, loss = 0.60 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:29.509879: step 63220, loss = 0.48 (7313.5 examples/sec; 0.018 sec/batch)
2017-09-16 16:15:29.829196: step 63230, loss = 0.44 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:30.152272: step 63240, loss = 0.57 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:30.473883: step 63250, loss = 0.54 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:30.793480: step 63260, loss = 0.58 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:31.114445: step 63270, loss = 0.59 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:31.436109: step 63280, loss = 0.55 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:31.757297: step 63290, loss = 0.52 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:32.076643: step 63300, loss = 0.81 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:32.542225: step 63310, loss = 0.54 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:32.863601: step 63320, loss = 0.87 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:33.183805: step 63330, loss = 0.61 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:33.502732: step 63340, loss = 0.53 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:33.824639: step 63350, loss = 0.57 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:34.144613: step 63360, loss = 0.59 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:34.464118: step 63370, loss = 0.61 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:34.783010: step 63380, loss = 0.77 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:35.107855: step 63390, loss = 0.62 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:35.430112: step 63400, loss = 0.60 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:35.877402: step 63410, loss = 0.58 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:36.198654: step 63420, loss = 0.46 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:36.517783: step 63430, loss = 0.60 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:36.838633: step 63440, loss = 0.61 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:37.157541: step 63450, loss = 0.44 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:37.478250: step 63460, loss = 0.48 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:37.797745: step 63470, loss = 0.57 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:38.117280: step 63480, loss = 0.63 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:38.437881: step 63490, loss = 0.68 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:38.758430: step 63500, loss = 0.57 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:39.223286: step 63510, loss = 0.56 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:39.542927: step 63520, loss = 0.59 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:39.864432: step 63530, loss = 0.62 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:40.185658: step 63540, loss = 0.58 (7900.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:40.508266: step 63550, loss = 0.68 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:40.827377: step 63560, loss = 0.62 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:41.146807: step 63570, loss = 0.61 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:41.469195: step 63580, loss = 0.65 (7828.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:41.794425: step 63590, loss = 0.56 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:42.113914: step 63600, loss = 0.52 (7887.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:42.568763: step 63610, loss = 0.65 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:42.891486: step 63620, loss = 0.63 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:43.212692: step 63630, loss = 0.61 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:43.531624: step 63640, loss = 0.77 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:43.855829: step 63650, loss = 0.51 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:44.181357: step 63660, loss = 0.73 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:44.499399: step 63670, loss = 0.58 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:44.819830: step 63680, loss = 0.61 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:45.139688: step 63690, loss = 0.54 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:45.460328: step 63700, loss = 0.54 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:45.927987: step 63710, loss = 0.58 (7643.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:46.247982: step 63720, loss = 0.58 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:46.566816: step 63730, loss = 0.55 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:46.888120: step 63740, loss = 0.53 (7497.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:47.208898: step 63750, loss = 0.69 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:47.531281: step 63760, loss = 0.72 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:47.853777: step 63770, loss = 0.52 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:48.173083: step 63780, loss = 0.69 (8059.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:48.494860: step 63790, loss = 0.51 (7679.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:48.813275: step 63800, loss = 0.51 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:49.268747: step 63810, loss = 0.52 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:49.590458: step 63820, loss = 0.48 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:49.912484: step 63830, loss = 0.58 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:50.233238: step 63840, loss = 0.58 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:50.552700: step 63850, loss = 0.57 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:50.873604: step 63860, loss = 0.55 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:51.192141: step 63870, loss = 0.58 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:51.512971: step 63880, loss = 0.62 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:51.832243: step 63890, loss = 0.56 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:52.151181: step 63900, loss = 0.57 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:52.607409: step 63910, loss = 0.54 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:52.926340: step 63920, loss = 0.59 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:53.247373: step 63930, loss = 0.61 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:53.567069: step 63940, loss = 0.60 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:53.888396: step 63950, loss = 0.63 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:54.211552: step 63960, loss = 0.75 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:54.534331: step 63970, loss = 0.67 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:54.855055: step 63980, loss = 0.55 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:55.175554: step 63990, loss = 0.49 (8143.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:55.496236: step 64000, loss = 0.46 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:56.054358: step 64010, loss = 0.47 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:56.375585: step 64020, loss = 0.66 (7657.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:15:56.694804: step 64030, loss = 0.75 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:57.020404: step 64040, loss = 0.47 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:57.343480: step 64050, loss = 0.66 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:57.664586: step 64060, loss = 0.56 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:57.983581: step 64070, loss = 0.64 (8151.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:58.306139: step 64080, loss = 0.58 (7798.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:58.625226: step 64090, loss = 0.57 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:58.945828: step 64100, loss = 0.71 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:59.403250: step 64110, loss = 0.47 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:15:59.726317: step 64120, loss = 0.68 (8152.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:00.052568: step 64130, loss = 0.56 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:00.373370: step 64140, loss = 0.54 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:00.693817: step 64150, loss = 0.59 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:01.012673: step 64160, loss = 0.58 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:01.332886: step 64170, loss = 0.61 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:01.652791: step 64180, loss = 0.62 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:01.975838: step 64190, loss = 0.57 (7467.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:02.300077: step 64200, loss = 0.57 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:02.750826: step 64210, loss = 0.61 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:03.070953: step 64220, loss = 0.56 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:03.392668: step 64230, loss = 0.59 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:03.711703: step 64240, loss = 0.61 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:04.033169: step 64250, loss = 0.66 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:04.351123: step 64260, loss = 0.61 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:04.672569: step 64270, loss = 0.69 (7967.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:04.992922: step 64280, loss = 0.63 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:05.317954: step 64290, loss = 0.60 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:05.642627: step 64300, loss = 0.66 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:06.095964: step 64310, loss = 0.55 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:06.417197: step 64320, loss = 0.49 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:06.736973: step 64330, loss = 0.69 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:07.055248: step 64340, loss = 0.71 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:07.375418: step 64350, loss = 0.64 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:07.694448: step 64360, loss = 0.56 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:08.014548: step 64370, loss = 0.62 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:08.336411: step 64380, loss = 0.72 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:08.658247: step 64390, loss = 0.62 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:08.978599: step 64400, loss = 0.59 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:09.443303: step 64410, loss = 0.72 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:09.764426: step 64420, loss = 0.66 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:10.083407: step 64430, loss = 0.69 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:10.407611: step 64440, loss = 0.66 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:10.729614: step 64450, loss = 0.56 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:11.049457: step 64460, loss = 0.65 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:11.369357: step 64470, loss = 0.54 (7909.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:11.694569: step 64480, loss = 0.56 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:12.018041: step 64490, loss = 0.63 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:12.339292: step 64500, loss = 0.65 (7772.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:12.807778: step 64510, loss = 0.62 (7833.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:13.127719: step 64520, loss = 0.60 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:13.447561: step 64530, loss = 0.54 (7857.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:13.770330: step 64540, loss = 0.64 (7911.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:14.090225: step 64550, loss = 0.65 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:14.410433: step 64560, loss = 0.69 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:14.728638: step 64570, loss = 0.54 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:15.050222: step 64580, loss = 0.55 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:15.371633: step 64590, loss = 0.47 (7627.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:15.693984: step 64600, loss = 0.52 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:16.149298: step 64610, loss = 0.53 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:16.468982: step 64620, loss = 0.58 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:16.789260: step 64630, loss = 0.66 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:17.109670: step 64640, loss = 0.60 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:17.430594: step 64650, loss = 0.56 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:17.751275: step 64660, loss = 0.58 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:18.070180: step 64670, loss = 0.49 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:18.392328: step 64680, loss = 0.65 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:18.711593: step 64690, loss = 0.62 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:19.031008: step 64700, loss = 0.59 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:19.493243: step 64710, loss = 0.53 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:19.814006: step 64720, loss = 0.60 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:20.132674: step 64730, loss = 0.68 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:20.452726: step 64740, loss = 0.53 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:20.774299: step 64750, loss = 0.54 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:21.098226: step 64760, loss = 0.57 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:21.421641: step 64770, loss = 0.58 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:21.741118: step 64780, loss = 0.77 (8160.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:22.060686: step 64790, loss = 0.68 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:22.381099: step 64800, loss = 0.62 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:22.830110: step 64810, loss = 0.61 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:23.148627: step 64820, loss = 0.74 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:23.469853: step 64830, loss = 0.58 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:23.792729: step 64840, loss = 0.70 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:24.114241: step 64850, loss = 0.54 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:24.435762: step 64860, loss = 0.66 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:24.755637: step 64870, loss = 0.51 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:25.080551: step 64880, loss = 0.61 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:25.400807: step 64890, loss = 0.55 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:25.720776: step 64900, loss = 0.58 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:26.180240: step 64910, loss = 0.56 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:26.500716: step 64920, loss = 0.69 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:26.820825: step 64930, loss = 0.65 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:27.140561: step 64940, loss = 0.52 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:27.461815: step 64950, loss = 0.61 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:27.783103: step 64960, loss = 0.60 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:28.102510: step 64970, loss = 0.50 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:28.421903: step 64980, loss = 0.61 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:28.743124: step 64990, loss = 0.62 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:29.062214: step 65000, loss = 0.56 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:29.613462: step 65010, loss = 0.61 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:29.932822: step 65020, loss = 0.63 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:30.254709: step 65030, loss = 0.64 (7767.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:30.577715: step 65040, loss = 0.59 (7651.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:30.898594: step 65050, loss = 0.57 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:31.222750: step 65060, loss = 0.73 (7961.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:31.543355: step 65070, loss = 0.45 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:31.869315: step 65080, loss = 0.53 (7946.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:32.190239: step 65090, loss = 0.66 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:32.512430: step 65100, loss = 0.64 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:32.973283: step 65110, loss = 0.55 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:33.295820: step 65120, loss = 0.52 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:33.618445: step 65130, loss = 0.61 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:33.942312: step 65140, loss = 0.71 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:34.261083: step 65150, loss = 0.48 (7935.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:34.581401: step 65160, loss = 0.51 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:34.904995: step 65170, loss = 0.69 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:35.226124: step 65180, loss = 0.50 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:35.546092: step 65190, loss = 0.59 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:35.865835: step 65200, loss = 0.54 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:36.327707: step 65210, loss = 0.58 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:36.647834: step 65220, loss = 0.55 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:36.967949: step 65230, loss = 0.67 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:37.289654: step 65240, loss = 0.60 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:37.611311: step 65250, loss = 0.67 (7763.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:37.931233: step 65260, loss = 0.51 (8039.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:38.254342: step 65270, loss = 0.58 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:38.574252: step 65280, loss = 0.53 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:38.894991: step 65290, loss = 0.61 (8120.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:39.217480: step 65300, loss = 0.54 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:39.678434: step 65310, loss = 0.73 (7872.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:40.000198: step 65320, loss = 0.44 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:40.325431: step 65330, loss = 0.46 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:40.647186: step 65340, loss = 0.66 (7759.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:40.970858: step 65350, loss = 0.55 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:41.291069: step 65360, loss = 0.56 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:41.612616: step 65370, loss = 0.57 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:41.931649: step 65380, loss = 0.49 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:42.253554: step 65390, loss = 0.66 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:42.576267: step 65400, loss = 0.62 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:43.039170: step 65410, loss = 0.61 (7510.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:43.359027: step 65420, loss = 0.52 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:43.678891: step 65430, loss = 0.80 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:44.001876: step 65440, loss = 0.51 (7789.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:44.322398: step 65450, loss = 0.57 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:44.641886: step 65460, loss = 0.66 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:44.963911: step 65470, loss = 0.64 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:45.284751: step 65480, loss = 0.61 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:45.607264: step 65490, loss = 0.61 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:45.928405: step 65500, loss = 0.62 (7830.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:46.384873: step 65510, loss = 0.57 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:46.707467: step 65520, loss = 0.56 (7829.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:47.027871: step 65530, loss = 0.50 (7815.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:47.347987: step 65540, loss = 0.57 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:47.668438: step 65550, loss = 0.50 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:47.991436: step 65560, loss = 0.57 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:48.314964: step 65570, loss = 0.57 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:48.639282: step 65580, loss = 0.53 (7977.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:48.961517: step 65590, loss = 0.45 (7848.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:49.282777: step 65600, loss = 0.72 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:49.739728: step 65610, loss = 0.54 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:50.060169: step 65620, loss = 0.64 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:50.380590: step 65630, loss = 0.67 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:50.701258: step 65640, loss = 0.81 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:51.022805: step 65650, loss = 0.66 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:51.346206: step 65660, loss = 0.69 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:51.671881: step 65670, loss = 0.65 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:51.998698: step 65680, loss = 0.64 (7973.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:52.317920: step 65690, loss = 0.59 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:52.636362: step 65700, loss = 0.56 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:53.103227: step 65710, loss = 0.41 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:53.423711: step 65720, loss = 0.57 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:53.744040: step 65730, loss = 0.54 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:54.065387: step 65740, loss = 0.54 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:54.388219: step 65750, loss = 0.54 (7742.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:54.708980: step 65760, loss = 0.69 (7701.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:55.029793: step 65770, loss = 0.54 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:55.349758: step 65780, loss = 0.59 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:55.672709: step 65790, loss = 0.46 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:55.992545: step 65800, loss = 0.68 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:56.450323: step 65810, loss = 0.62 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:56.776360: step 65820, loss = 0.59 (7696.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:57.097183: step 65830, loss = 0.60 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:57.418078: step 65840, loss = 0.43 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:57.738061: step 65850, loss = 0.57 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:58.059276: step 65860, loss = 0.64 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:58.382178: step 65870, loss = 0.51 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:58.705451: step 65880, loss = 0.68 (7695.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:16:59.025739: step 65890, loss = 0.65 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:59.347689: step 65900, loss = 0.54 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:16:59.810394: step 65910, loss = 0.59 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:00.131477: step 65920, loss = 0.59 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:00.452193: step 65930, loss = 0.54 (8153.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:00.775230: step 65940, loss = 0.50 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:01.097322: step 65950, loss = 0.54 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:01.418196: step 65960, loss = 0.62 (7996.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:01.742471: step 65970, loss = 0.52 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:02.063356: step 65980, loss = 0.57 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:02.384734: step 65990, loss = 0.68 (7827.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:02.704667: step 66000, loss = 0.57 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:03.272518: step 66010, loss = 0.62 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:03.593863: step 66020, loss = 0.69 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:03.916162: step 66030, loss = 0.76 (8151.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:04.236441: step 66040, loss = 0.61 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:04.555758: step 66050, loss = 0.52 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:04.875842: step 66060, loss = 0.60 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:05.197363: step 66070, loss = 0.60 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:05.519278: step 66080, loss = 0.56 (7784.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:05.839960: step 66090, loss = 0.51 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:06.163634: step 66100, loss = 0.60 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:06.626269: step 66110, loss = 0.66 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:06.948765: step 66120, loss = 0.53 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:07.269574: step 66130, loss = 0.61 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:07.590406: step 66140, loss = 0.72 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:07.912695: step 66150, loss = 0.55 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:08.235916: step 66160, loss = 0.69 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:08.556812: step 66170, loss = 0.58 (7835.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:08.878828: step 66180, loss = 0.55 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:09.203194: step 66190, loss = 0.64 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:09.523821: step 66200, loss = 0.50 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:09.983617: step 66210, loss = 0.62 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:10.304204: step 66220, loss = 0.58 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:10.625107: step 66230, loss = 0.56 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:10.946605: step 66240, loss = 0.55 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:11.268076: step 66250, loss = 0.52 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:11.588632: step 66260, loss = 0.59 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:11.911515: step 66270, loss = 0.55 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:12.231172: step 66280, loss = 0.51 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:12.551030: step 66290, loss = 0.62 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:12.869875: step 66300, loss = 0.63 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:13.329253: step 66310, loss = 0.57 (7597.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:17:13.653960: step 66320, loss = 0.59 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:13.975450: step 66330, loss = 0.72 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:14.295403: step 66340, loss = 0.58 (7517.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:17:14.615955: step 66350, loss = 0.63 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:14.937265: step 66360, loss = 0.52 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:15.257897: step 66370, loss = 0.65 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:15.578934: step 66380, loss = 0.54 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:15.897875: step 66390, loss = 0.61 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:16.217008: step 66400, loss = 0.67 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:16.676132: step 66410, loss = 0.55 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:16.998082: step 66420, loss = 0.58 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:17.319269: step 66430, loss = 0.52 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:17.640334: step 66440, loss = 0.51 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:17.961639: step 66450, loss = 0.61 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:18.280443: step 66460, loss = 0.69 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:18.600841: step 66470, loss = 0.56 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:18.921920: step 66480, loss = 0.68 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:19.243631: step 66490, loss = 0.57 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:19.564859: step 66500, loss = 0.72 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:20.036228: step 66510, loss = 0.58 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:20.356682: step 66520, loss = 0.51 (8014.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:20.678488: step 66530, loss = 0.59 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:20.999034: step 66540, loss = 0.55 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:21.318575: step 66550, loss = 0.69 (8136.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:21.637625: step 66560, loss = 0.68 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:21.956865: step 66570, loss = 0.62 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:22.278021: step 66580, loss = 0.74 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:22.598292: step 66590, loss = 0.55 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:22.916416: step 66600, loss = 0.80 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:23.376650: step 66610, loss = 0.63 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:23.697427: step 66620, loss = 0.45 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:24.018817: step 66630, loss = 0.68 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:24.341588: step 66640, loss = 0.61 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:24.660827: step 66650, loss = 0.49 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:24.980641: step 66660, loss = 0.63 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:25.301697: step 66670, loss = 0.63 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:25.624375: step 66680, loss = 0.55 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:25.949852: step 66690, loss = 0.54 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:26.271308: step 66700, loss = 0.61 (7829.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:26.742105: step 66710, loss = 0.66 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:27.067628: step 66720, loss = 0.57 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:27.387166: step 66730, loss = 0.55 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:27.714797: step 66740, loss = 0.49 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:28.037926: step 66750, loss = 0.53 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:28.358645: step 66760, loss = 0.57 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:28.677879: step 66770, loss = 0.66 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:28.999426: step 66780, loss = 0.52 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:29.319905: step 66790, loss = 0.51 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:29.642491: step 66800, loss = 0.63 (7873.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:30.109360: step 66810, loss = 0.75 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:30.435873: step 66820, loss = 0.69 (7616.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:17:30.758212: step 66830, loss = 0.59 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:31.078372: step 66840, loss = 0.55 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:31.401113: step 66850, loss = 0.58 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:31.719633: step 66860, loss = 0.55 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:32.037337: step 66870, loss = 0.62 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:32.362657: step 66880, loss = 0.75 (8136.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:32.684884: step 66890, loss = 0.61 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:33.006303: step 66900, loss = 0.66 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:33.467835: step 66910, loss = 0.59 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:33.789178: step 66920, loss = 0.61 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:34.109999: step 66930, loss = 0.56 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:34.431050: step 66940, loss = 0.53 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:34.753795: step 66950, loss = 0.61 (7906.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:35.074923: step 66960, loss = 0.56 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:35.399432: step 66970, loss = 0.68 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:35.720878: step 66980, loss = 0.58 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:36.042645: step 66990, loss = 0.76 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:36.362517: step 67000, loss = 0.61 (7960.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:36.959572: step 67010, loss = 0.58 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:37.279288: step 67020, loss = 0.47 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:37.599478: step 67030, loss = 0.49 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:37.920510: step 67040, loss = 0.67 (7798.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:38.239729: step 67050, loss = 0.57 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:38.561084: step 67060, loss = 0.67 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:38.878849: step 67070, loss = 0.60 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:39.201526: step 67080, loss = 0.50 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:39.521671: step 67090, loss = 0.60 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:39.842223: step 67100, loss = 0.50 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:40.307707: step 67110, loss = 0.57 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:40.626381: step 67120, loss = 0.58 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:40.947756: step 67130, loss = 0.74 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:41.269214: step 67140, loss = 0.67 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:41.592791: step 67150, loss = 0.51 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:41.912203: step 67160, loss = 0.66 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:42.233938: step 67170, loss = 0.55 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:42.555345: step 67180, loss = 0.65 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:42.874498: step 67190, loss = 0.54 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:43.195561: step 67200, loss = 0.71 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:43.654933: step 67210, loss = 0.57 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:43.976270: step 67220, loss = 0.57 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:44.296046: step 67230, loss = 0.63 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:44.615662: step 67240, loss = 0.70 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:44.934439: step 67250, loss = 0.45 (8032.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:45.255262: step 67260, loss = 0.63 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:45.573772: step 67270, loss = 0.61 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:45.892216: step 67280, loss = 0.58 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:46.213729: step 67290, loss = 0.61 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:46.533873: step 67300, loss = 0.45 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:46.988894: step 67310, loss = 0.50 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:47.309224: step 67320, loss = 0.66 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:47.631564: step 67330, loss = 0.64 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:47.961422: step 67340, loss = 0.60 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:48.288099: step 67350, loss = 0.57 (7895.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:48.608513: step 67360, loss = 0.73 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:48.929847: step 67370, loss = 0.53 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:49.249823: step 67380, loss = 0.65 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:49.569285: step 67390, loss = 0.60 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:49.890107: step 67400, loss = 0.55 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:50.340464: step 67410, loss = 0.55 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:50.661172: step 67420, loss = 0.53 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:50.980397: step 67430, loss = 0.60 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:51.300226: step 67440, loss = 0.59 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:51.619563: step 67450, loss = 0.58 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:51.940603: step 67460, loss = 0.51 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:52.261235: step 67470, loss = 0.56 (7976.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:52.580817: step 67480, loss = 0.78 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:52.901499: step 67490, loss = 0.63 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:53.222749: step 67500, loss = 0.53 (7763.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:53.678237: step 67510, loss = 0.58 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:53.997439: step 67520, loss = 0.60 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:54.319000: step 67530, loss = 0.48 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:54.639387: step 67540, loss = 0.56 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:54.959670: step 67550, loss = 0.51 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:55.282495: step 67560, loss = 0.47 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:55.604858: step 67570, loss = 0.55 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:55.925957: step 67580, loss = 0.51 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:56.246924: step 67590, loss = 0.64 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:56.567133: step 67600, loss = 0.61 (8116.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:57.015602: step 67610, loss = 0.60 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:57.335889: step 67620, loss = 0.47 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:57.655030: step 67630, loss = 0.57 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:57.977252: step 67640, loss = 0.63 (7804.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:58.297153: step 67650, loss = 0.62 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:58.616565: step 67660, loss = 0.64 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:58.938412: step 67670, loss = 0.62 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:59.257736: step 67680, loss = 0.59 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:59.580712: step 67690, loss = 0.58 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:17:59.899998: step 67700, loss = 0.58 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:00.354901: step 67710, loss = 0.62 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:00.674093: step 67720, loss = 0.57 (8017.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:00.994076: step 67730, loss = 0.65 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:01.315828: step 67740, loss = 0.55 (7855.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:01.635927: step 67750, loss = 0.60 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:01.956157: step 67760, loss = 0.55 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:02.276200: step 67770, loss = 0.64 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:02.597471: step 67780, loss = 0.68 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:02.917504: step 67790, loss = 0.62 (7873.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:03.236187: step 67800, loss = 0.65 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:03.700806: step 67810, loss = 0.63 (7535.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:04.023097: step 67820, loss = 0.61 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:04.349427: step 67830, loss = 0.61 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:04.670699: step 67840, loss = 0.52 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:04.988612: step 67850, loss = 0.66 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:05.309063: step 67860, loss = 0.53 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:05.630827: step 67870, loss = 0.79 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:05.950915: step 67880, loss = 0.68 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:06.273816: step 67890, loss = 0.52 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:06.594509: step 67900, loss = 0.54 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:07.058367: step 67910, loss = 0.60 (7594.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:07.378723: step 67920, loss = 0.50 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:07.698345: step 67930, loss = 0.55 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:08.018212: step 67940, loss = 0.58 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:08.337432: step 67950, loss = 0.59 (8175.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:08.658046: step 67960, loss = 0.59 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:08.978442: step 67970, loss = 0.65 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:09.298458: step 67980, loss = 0.64 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:09.618248: step 67990, loss = 0.78 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:09.937789: step 68000, loss = 0.63 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:10.495354: step 68010, loss = 0.58 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:10.815801: step 68020, loss = 0.62 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:11.135333: step 68030, loss = 0.74 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:11.456819: step 68040, loss = 0.79 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:11.777756: step 68050, loss = 0.53 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:12.100389: step 68060, loss = 0.60 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:12.419682: step 68070, loss = 0.63 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:12.744174: step 68080, loss = 0.42 (7822.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:13.065264: step 68090, loss = 0.49 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:13.387242: step 68100, loss = 0.58 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:13.848407: step 68110, loss = 0.64 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:14.168538: step 68120, loss = 0.60 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:14.486122: step 68130, loss = 0.55 (8120.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:14.806235: step 68140, loss = 0.55 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:15.126112: step 68150, loss = 0.70 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:15.445972: step 68160, loss = 0.67 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:15.770205: step 68170, loss = 0.54 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:16.089228: step 68180, loss = 0.56 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:16.412508: step 68190, loss = 0.48 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:16.732958: step 68200, loss = 0.52 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:17.182587: step 68210, loss = 0.67 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:17.503886: step 68220, loss = 0.58 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:17.824239: step 68230, loss = 0.56 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:18.144336: step 68240, loss = 0.47 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:18.465125: step 68250, loss = 0.54 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:18.787123: step 68260, loss = 0.61 (7635.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:19.107145: step 68270, loss = 0.59 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:19.426184: step 68280, loss = 0.69 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:19.744529: step 68290, loss = 0.64 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:20.064434: step 68300, loss = 0.52 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:20.513345: step 68310, loss = 0.61 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:20.832983: step 68320, loss = 0.65 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:21.153396: step 68330, loss = 0.55 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:21.474071: step 68340, loss = 0.64 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:21.794294: step 68350, loss = 0.52 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:22.115083: step 68360, loss = 0.57 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:22.435340: step 68370, loss = 0.62 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:22.757432: step 68380, loss = 0.53 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:23.078075: step 68390, loss = 0.60 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:23.397527: step 68400, loss = 0.50 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:23.862783: step 68410, loss = 0.59 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:24.183627: step 68420, loss = 0.56 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:24.507006: step 68430, loss = 0.64 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:24.827255: step 68440, loss = 0.66 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:25.149700: step 68450, loss = 0.54 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:25.472281: step 68460, loss = 0.80 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:25.792157: step 68470, loss = 0.61 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:26.112527: step 68480, loss = 0.68 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:26.431716: step 68490, loss = 0.54 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:26.752453: step 68500, loss = 0.56 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:27.215211: step 68510, loss = 0.68 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:27.535903: step 68520, loss = 0.67 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:27.857855: step 68530, loss = 0.55 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:28.176745: step 68540, loss = 0.51 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:28.497686: step 68550, loss = 0.62 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:28.819110: step 68560, loss = 0.56 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:29.139767: step 68570, loss = 0.65 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:29.459824: step 68580, loss = 0.52 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:29.779657: step 68590, loss = 0.52 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:30.100883: step 68600, loss = 0.67 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:30.553485: step 68610, loss = 0.54 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:30.875465: step 68620, loss = 0.63 (7865.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:31.195974: step 68630, loss = 0.67 (8139.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:31.516894: step 68640, loss = 0.58 (7841.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:31.836386: step 68650, loss = 0.48 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:32.157160: step 68660, loss = 0.61 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:32.479307: step 68670, loss = 0.57 (7801.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:32.799083: step 68680, loss = 0.65 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:33.121524: step 68690, loss = 0.57 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:33.442985: step 68700, loss = 0.62 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:33.891884: step 68710, loss = 0.58 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:34.214443: step 68720, loss = 0.58 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:34.536936: step 68730, loss = 0.62 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:34.856632: step 68740, loss = 0.59 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:35.179375: step 68750, loss = 0.52 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:35.498683: step 68760, loss = 0.58 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:35.817374: step 68770, loss = 0.61 (7911.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:36.136360: step 68780, loss = 0.59 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:36.458786: step 68790, loss = 0.60 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:36.781074: step 68800, loss = 0.52 (7891.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:37.230012: step 68810, loss = 0.67 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:37.552588: step 68820, loss = 0.66 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:37.879287: step 68830, loss = 0.63 (7844.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:38.200814: step 68840, loss = 0.79 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:38.526726: step 68850, loss = 0.57 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:38.845616: step 68860, loss = 0.67 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:39.165568: step 68870, loss = 0.48 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:39.485057: step 68880, loss = 0.54 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:39.805596: step 68890, loss = 0.60 (7971.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:40.131571: step 68900, loss = 0.61 (7658.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:40.583627: step 68910, loss = 0.65 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:40.904547: step 68920, loss = 0.62 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:41.224443: step 68930, loss = 0.53 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:41.544811: step 68940, loss = 0.73 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:41.863638: step 68950, loss = 0.64 (8146.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:42.183212: step 68960, loss = 0.72 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:42.503273: step 68970, loss = 0.60 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:42.822569: step 68980, loss = 0.57 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:43.143356: step 68990, loss = 0.55 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:43.465671: step 69000, loss = 0.55 (7767.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:44.010443: step 69010, loss = 0.48 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:44.329806: step 69020, loss = 0.55 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:44.650489: step 69030, loss = 0.67 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:44.971954: step 69040, loss = 0.82 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:45.294611: step 69050, loss = 0.61 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:45.613447: step 69060, loss = 0.57 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:45.933387: step 69070, loss = 0.59 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:46.251678: step 69080, loss = 0.63 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:46.573038: step 69090, loss = 0.53 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:46.894573: step 69100, loss = 0.59 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:47.356461: step 69110, loss = 0.54 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:47.676558: step 69120, loss = 0.58 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:47.998666: step 69130, loss = 0.50 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:48.321393: step 69140, loss = 0.61 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:48.641878: step 69150, loss = 0.53 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:48.962250: step 69160, loss = 0.59 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:49.285095: step 69170, loss = 0.54 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:49.608218: step 69180, loss = 0.57 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:49.930359: step 69190, loss = 0.55 (7813.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:50.251200: step 69200, loss = 0.61 (8123.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:50.707659: step 69210, loss = 0.57 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:51.026314: step 69220, loss = 0.55 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:51.345906: step 69230, loss = 0.62 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:51.667008: step 69240, loss = 0.54 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:51.986651: step 69250, loss = 0.63 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:52.309728: step 69260, loss = 0.60 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:52.629902: step 69270, loss = 0.49 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:52.952051: step 69280, loss = 0.58 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:53.273302: step 69290, loss = 0.68 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:53.592221: step 69300, loss = 0.51 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:54.057223: step 69310, loss = 0.54 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:54.383449: step 69320, loss = 0.54 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:54.702737: step 69330, loss = 0.68 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:55.022908: step 69340, loss = 0.42 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:55.343109: step 69350, loss = 0.61 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:55.662685: step 69360, loss = 0.56 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:55.980950: step 69370, loss = 0.50 (8115.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:56.301734: step 69380, loss = 0.65 (7803.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:56.629134: step 69390, loss = 0.57 (7667.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:56.950578: step 69400, loss = 0.58 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:57.409127: step 69410, loss = 0.62 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:57.728271: step 69420, loss = 0.45 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:58.048196: step 69430, loss = 0.50 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:58.371680: step 69440, loss = 0.57 (7513.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:18:58.695362: step 69450, loss = 0.54 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:59.014421: step 69460, loss = 0.52 (8071.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:59.334777: step 69470, loss = 0.60 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:59.654940: step 69480, loss = 0.63 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:18:59.977303: step 69490, loss = 0.52 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:00.298613: step 69500, loss = 0.52 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:00.750805: step 69510, loss = 0.52 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:01.069432: step 69520, loss = 0.65 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:01.390187: step 69530, loss = 0.61 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:01.711164: step 69540, loss = 0.63 (7861.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:02.030774: step 69550, loss = 0.50 (7988.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:02.354388: step 69560, loss = 0.62 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:02.677915: step 69570, loss = 0.58 (7983.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:02.997722: step 69580, loss = 0.77 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:03.319425: step 69590, loss = 0.60 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:03.641456: step 69600, loss = 0.61 (7594.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:04.093943: step 69610, loss = 0.77 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:04.418117: step 69620, loss = 0.57 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:04.740532: step 69630, loss = 0.46 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:05.062261: step 69640, loss = 0.56 (7678.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:05.383918: step 69650, loss = 0.63 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:05.704400: step 69660, loss = 0.46 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:06.024078: step 69670, loss = 0.70 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:06.345289: step 69680, loss = 0.73 (7659.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:06.665807: step 69690, loss = 0.60 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:06.986223: step 69700, loss = 0.67 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:07.450297: step 69710, loss = 0.57 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:07.770746: step 69720, loss = 0.59 (8158.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:08.092148: step 69730, loss = 0.64 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:08.415776: step 69740, loss = 0.50 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:08.736442: step 69750, loss = 0.66 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:09.060963: step 69760, loss = 0.54 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:09.383185: step 69770, loss = 0.76 (8006.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:09.703861: step 69780, loss = 0.48 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:10.023649: step 69790, loss = 0.62 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:10.344766: step 69800, loss = 0.61 (7803.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:10.804037: step 69810, loss = 0.63 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:11.124574: step 69820, loss = 0.59 (7948.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:11.452443: step 69830, loss = 0.52 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:11.771616: step 69840, loss = 0.63 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:12.091647: step 69850, loss = 0.57 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:12.411507: step 69860, loss = 0.66 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:12.734092: step 69870, loss = 0.54 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:13.053217: step 69880, loss = 0.66 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:13.375714: step 69890, loss = 0.68 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:13.695881: step 69900, loss = 0.65 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:14.156886: step 69910, loss = 0.62 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:14.478155: step 69920, loss = 0.50 (7592.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:14.801373: step 69930, loss = 0.54 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:15.124983: step 69940, loss = 0.55 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:15.445070: step 69950, loss = 0.54 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:15.765989: step 69960, loss = 0.49 (7821.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:16.086632: step 69970, loss = 0.63 (7866.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:16.407342: step 69980, loss = 0.55 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:16.729499: step 69990, loss = 0.66 (7938.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:17.049491: step 70000, loss = 0.59 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:17.603274: step 70010, loss = 0.49 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:17.924138: step 70020, loss = 0.59 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:18.245850: step 70030, loss = 0.73 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:18.566859: step 70040, loss = 0.60 (7846.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:18.887223: step 70050, loss = 0.60 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:19.208805: step 70060, loss = 0.62 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:19.529105: step 70070, loss = 0.62 (7877.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:19.849819: step 70080, loss = 0.62 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:20.172412: step 70090, loss = 0.51 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:20.497454: step 70100, loss = 0.63 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:20.959352: step 70110, loss = 0.64 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:21.280538: step 70120, loss = 0.70 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:21.604005: step 70130, loss = 0.67 (8034.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:21.925241: step 70140, loss = 0.59 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:22.248490: step 70150, loss = 0.55 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:22.568804: step 70160, loss = 0.47 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:22.888567: step 70170, loss = 0.58 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:23.210793: step 70180, loss = 0.57 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:23.530933: step 70190, loss = 0.62 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:23.850969: step 70200, loss = 0.43 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:24.304172: step 70210, loss = 0.67 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:24.624932: step 70220, loss = 0.70 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:24.944451: step 70230, loss = 0.65 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:25.265178: step 70240, loss = 0.59 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:25.585775: step 70250, loss = 0.72 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:25.906065: step 70260, loss = 0.60 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:26.228655: step 70270, loss = 0.56 (7943.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:26.547208: step 70280, loss = 0.68 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:26.868782: step 70290, loss = 0.73 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:27.188125: step 70300, loss = 0.50 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:27.635999: step 70310, loss = 0.66 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:27.960093: step 70320, loss = 0.57 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:28.281298: step 70330, loss = 0.56 (7843.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:28.609623: step 70340, loss = 0.67 (7497.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:28.930801: step 70350, loss = 0.53 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:29.253392: step 70360, loss = 0.47 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:29.575642: step 70370, loss = 0.62 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:29.895818: step 70380, loss = 0.59 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:30.216009: step 70390, loss = 0.71 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:30.539478: step 70400, loss = 0.58 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:31.000299: step 70410, loss = 0.55 (7932.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:31.321061: step 70420, loss = 0.63 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:31.647457: step 70430, loss = 0.70 (7434.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:31.969143: step 70440, loss = 0.55 (7806.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:32.290120: step 70450, loss = 0.46 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:32.611745: step 70460, loss = 0.58 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:32.931235: step 70470, loss = 0.58 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:33.251658: step 70480, loss = 0.56 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:33.572523: step 70490, loss = 0.54 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:33.893797: step 70500, loss = 0.55 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:34.355738: step 70510, loss = 0.59 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:34.675636: step 70520, loss = 0.49 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:34.996931: step 70530, loss = 0.75 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:35.315264: step 70540, loss = 0.60 (8142.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:35.635380: step 70550, loss = 0.52 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:35.956144: step 70560, loss = 0.49 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:36.275342: step 70570, loss = 0.58 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:36.598270: step 70580, loss = 0.58 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:36.919973: step 70590, loss = 0.54 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:37.238645: step 70600, loss = 0.51 (7939.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:37.701125: step 70610, loss = 0.66 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:38.022162: step 70620, loss = 0.50 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:38.341418: step 70630, loss = 0.52 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:38.661464: step 70640, loss = 0.49 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:38.984870: step 70650, loss = 0.63 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:39.305518: step 70660, loss = 0.60 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:39.625950: step 70670, loss = 0.63 (7880.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:39.944526: step 70680, loss = 0.75 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:40.266059: step 70690, loss = 0.53 (7909.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:40.586566: step 70700, loss = 0.47 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:41.047917: step 70710, loss = 0.63 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:41.369752: step 70720, loss = 0.52 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:41.694671: step 70730, loss = 0.45 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:42.017510: step 70740, loss = 0.59 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:42.338036: step 70750, loss = 0.60 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:42.656116: step 70760, loss = 0.59 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:42.974760: step 70770, loss = 0.70 (8156.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:43.293538: step 70780, loss = 0.61 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:43.614570: step 70790, loss = 0.54 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:43.937482: step 70800, loss = 0.62 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:44.393805: step 70810, loss = 0.54 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:44.715063: step 70820, loss = 0.55 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:45.036540: step 70830, loss = 0.57 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:45.357848: step 70840, loss = 0.65 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:45.677939: step 70850, loss = 0.61 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:45.997592: step 70860, loss = 0.53 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:46.317736: step 70870, loss = 0.62 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:46.638437: step 70880, loss = 0.54 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:46.962203: step 70890, loss = 0.57 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:47.281809: step 70900, loss = 0.64 (7810.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:47.729231: step 70910, loss = 0.56 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:48.048866: step 70920, loss = 0.63 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:48.369568: step 70930, loss = 0.52 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:48.689559: step 70940, loss = 0.71 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:49.009530: step 70950, loss = 0.69 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:49.328443: step 70960, loss = 0.69 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:49.650739: step 70970, loss = 0.65 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:49.970069: step 70980, loss = 0.56 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:50.289157: step 70990, loss = 0.56 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:50.608769: step 71000, loss = 0.61 (8146.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:51.161721: step 71010, loss = 0.51 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:51.481472: step 71020, loss = 0.61 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:51.801461: step 71030, loss = 0.50 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:52.120607: step 71040, loss = 0.58 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:52.442299: step 71050, loss = 0.50 (7804.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:52.763673: step 71060, loss = 0.48 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:53.085118: step 71070, loss = 0.55 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:53.406598: step 71080, loss = 0.59 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:53.727969: step 71090, loss = 0.64 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:54.048637: step 71100, loss = 0.63 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:54.512429: step 71110, loss = 0.57 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:54.833331: step 71120, loss = 0.71 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:55.155194: step 71130, loss = 0.64 (8153.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:55.477775: step 71140, loss = 0.67 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:55.797416: step 71150, loss = 0.46 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:56.117912: step 71160, loss = 0.47 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:56.441660: step 71170, loss = 0.47 (8155.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:56.768580: step 71180, loss = 0.61 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:57.089150: step 71190, loss = 0.61 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:57.410031: step 71200, loss = 0.76 (7639.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:19:57.867782: step 71210, loss = 0.56 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:58.185590: step 71220, loss = 0.51 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:58.505856: step 71230, loss = 0.45 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:58.827843: step 71240, loss = 0.58 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:59.148389: step 71250, loss = 0.55 (7994.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:59.469835: step 71260, loss = 0.53 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:19:59.789844: step 71270, loss = 0.63 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:00.111696: step 71280, loss = 0.61 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:00.434318: step 71290, loss = 0.50 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:00.753498: step 71300, loss = 0.63 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:01.214207: step 71310, loss = 0.59 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:01.538934: step 71320, loss = 0.62 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:01.861990: step 71330, loss = 0.66 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:02.181290: step 71340, loss = 0.50 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:02.501627: step 71350, loss = 0.50 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:02.820972: step 71360, loss = 0.75 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:03.142689: step 71370, loss = 0.81 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:03.462760: step 71380, loss = 0.56 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:03.781799: step 71390, loss = 0.55 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:04.102673: step 71400, loss = 0.53 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:04.563983: step 71410, loss = 0.65 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:04.883806: step 71420, loss = 0.51 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:05.206134: step 71430, loss = 0.55 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:05.526527: step 71440, loss = 0.56 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:05.849247: step 71450, loss = 0.50 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:06.169671: step 71460, loss = 0.61 (8095.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:06.492158: step 71470, loss = 0.62 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:06.813121: step 71480, loss = 0.66 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:07.132765: step 71490, loss = 0.72 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:07.452525: step 71500, loss = 0.58 (7866.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:07.914576: step 71510, loss = 0.63 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:08.235897: step 71520, loss = 0.58 (8138.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:08.555576: step 71530, loss = 0.71 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:08.875746: step 71540, loss = 0.67 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:09.195286: step 71550, loss = 0.74 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:09.514786: step 71560, loss = 0.44 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:09.837578: step 71570, loss = 0.57 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:10.156593: step 71580, loss = 0.56 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:10.477312: step 71590, loss = 0.72 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:10.796705: step 71600, loss = 0.68 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:11.245298: step 71610, loss = 0.66 (8083.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:11.564579: step 71620, loss = 0.53 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:11.886614: step 71630, loss = 0.58 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:12.208602: step 71640, loss = 0.49 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:12.528640: step 71650, loss = 0.56 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:12.851779: step 71660, loss = 0.47 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:13.172566: step 71670, loss = 0.49 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:13.495262: step 71680, loss = 0.60 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:13.815378: step 71690, loss = 0.67 (7810.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:14.135836: step 71700, loss = 0.62 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:14.583724: step 71710, loss = 0.56 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:14.905324: step 71720, loss = 0.73 (7975.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:15.227320: step 71730, loss = 0.56 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:15.547994: step 71740, loss = 0.53 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:15.868043: step 71750, loss = 0.74 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:16.188329: step 71760, loss = 0.60 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:16.507668: step 71770, loss = 0.57 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:16.830655: step 71780, loss = 0.61 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:17.154227: step 71790, loss = 0.60 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:17.476495: step 71800, loss = 0.43 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:17.947366: step 71810, loss = 0.51 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:18.269708: step 71820, loss = 0.64 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:18.590424: step 71830, loss = 0.62 (7656.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:18.912749: step 71840, loss = 0.69 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:19.235286: step 71850, loss = 0.53 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:19.556348: step 71860, loss = 0.60 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:19.877239: step 71870, loss = 0.68 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:20.196798: step 71880, loss = 0.71 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:20.517819: step 71890, loss = 0.68 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:20.836962: step 71900, loss = 0.59 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:21.296161: step 71910, loss = 0.48 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:21.615011: step 71920, loss = 0.64 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:21.935441: step 71930, loss = 0.61 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:22.257683: step 71940, loss = 0.60 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:22.577711: step 71950, loss = 0.54 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:22.898383: step 71960, loss = 0.64 (7782.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:23.225414: step 71970, loss = 0.73 (7442.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:23.543869: step 71980, loss = 0.55 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:23.868111: step 71990, loss = 0.57 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:24.188140: step 72000, loss = 0.49 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:24.740066: step 72010, loss = 0.72 (8133.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:25.062164: step 72020, loss = 0.56 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:25.384094: step 72030, loss = 0.58 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:25.704382: step 72040, loss = 0.70 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:26.027413: step 72050, loss = 0.64 (7599.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:26.348545: step 72060, loss = 0.63 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:26.669636: step 72070, loss = 0.53 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:26.993194: step 72080, loss = 0.58 (7547.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:27.315257: step 72090, loss = 0.46 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:27.635800: step 72100, loss = 0.65 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:28.084222: step 72110, loss = 0.63 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:28.404911: step 72120, loss = 0.56 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:28.725292: step 72130, loss = 0.70 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:29.044222: step 72140, loss = 0.58 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:29.365856: step 72150, loss = 0.57 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:29.688491: step 72160, loss = 0.60 (7395.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:30.012351: step 72170, loss = 0.63 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:30.331645: step 72180, loss = 0.63 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:30.653783: step 72190, loss = 0.71 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:30.975060: step 72200, loss = 0.61 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:31.432284: step 72210, loss = 0.54 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:31.755711: step 72220, loss = 0.59 (7834.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:32.076784: step 72230, loss = 0.59 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:32.396532: step 72240, loss = 0.64 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:32.717542: step 72250, loss = 0.62 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:33.037565: step 72260, loss = 0.68 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:33.358586: step 72270, loss = 0.83 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:33.680196: step 72280, loss = 0.61 (7747.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:34.002302: step 72290, loss = 0.54 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:34.324679: step 72300, loss = 0.62 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:34.774314: step 72310, loss = 0.61 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:35.095936: step 72320, loss = 0.58 (8121.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:35.422637: step 72330, loss = 0.51 (7350.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:20:35.743299: step 72340, loss = 0.63 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:36.069048: step 72350, loss = 0.57 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:36.389641: step 72360, loss = 0.69 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:36.707610: step 72370, loss = 0.56 (8010.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:37.027353: step 72380, loss = 0.65 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:37.348003: step 72390, loss = 0.48 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:37.667697: step 72400, loss = 0.53 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:38.133503: step 72410, loss = 0.65 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:38.455411: step 72420, loss = 0.57 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:38.775379: step 72430, loss = 0.62 (7842.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:39.096903: step 72440, loss = 0.71 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:39.416196: step 72450, loss = 0.66 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:39.735738: step 72460, loss = 0.56 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:40.057628: step 72470, loss = 0.65 (8125.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:40.378612: step 72480, loss = 0.56 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:40.698645: step 72490, loss = 0.61 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:41.020274: step 72500, loss = 0.57 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:41.481643: step 72510, loss = 0.59 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:41.801067: step 72520, loss = 0.58 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:42.121643: step 72530, loss = 0.62 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:42.441796: step 72540, loss = 0.63 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:42.760639: step 72550, loss = 0.50 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:43.081557: step 72560, loss = 0.60 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:43.403464: step 72570, loss = 0.69 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:43.723492: step 72580, loss = 0.69 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:44.044058: step 72590, loss = 0.59 (8104.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:44.369257: step 72600, loss = 0.59 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:44.823478: step 72610, loss = 0.67 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:45.142899: step 72620, loss = 0.68 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:45.463663: step 72630, loss = 0.68 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:45.785284: step 72640, loss = 0.60 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:46.107894: step 72650, loss = 0.61 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:46.428206: step 72660, loss = 0.61 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:46.747632: step 72670, loss = 0.68 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:47.068375: step 72680, loss = 0.55 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:47.389254: step 72690, loss = 0.56 (8097.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:47.709871: step 72700, loss = 0.78 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:48.157228: step 72710, loss = 0.61 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:48.478340: step 72720, loss = 0.54 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:48.798948: step 72730, loss = 0.45 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:49.119374: step 72740, loss = 0.50 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:49.438731: step 72750, loss = 0.61 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:49.758601: step 72760, loss = 0.65 (8107.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:50.077758: step 72770, loss = 0.57 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:50.397278: step 72780, loss = 0.57 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:50.716904: step 72790, loss = 0.44 (8142.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:51.035614: step 72800, loss = 0.66 (7966.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:51.498258: step 72810, loss = 0.66 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:51.819196: step 72820, loss = 0.50 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:52.143259: step 72830, loss = 0.64 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:52.464588: step 72840, loss = 0.59 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:52.783671: step 72850, loss = 0.50 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:53.105001: step 72860, loss = 1.05 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:53.432433: step 72870, loss = 0.62 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:53.754337: step 72880, loss = 0.53 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:54.074096: step 72890, loss = 0.57 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:54.395406: step 72900, loss = 0.47 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:54.858465: step 72910, loss = 0.60 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:55.178198: step 72920, loss = 0.49 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:55.501829: step 72930, loss = 0.67 (8126.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:55.824031: step 72940, loss = 0.62 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:56.142442: step 72950, loss = 0.68 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:56.462855: step 72960, loss = 0.68 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:56.784038: step 72970, loss = 0.60 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:57.105635: step 72980, loss = 0.64 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:57.431109: step 72990, loss = 0.67 (7975.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:57.755496: step 73000, loss = 0.74 (7838.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:58.311665: step 73010, loss = 0.61 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:58.630944: step 73020, loss = 0.55 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:58.955670: step 73030, loss = 0.69 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:59.274215: step 73040, loss = 0.50 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:59.595170: step 73050, loss = 0.57 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:20:59.916480: step 73060, loss = 0.53 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:00.240378: step 73070, loss = 0.63 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:00.564768: step 73080, loss = 0.65 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:00.884639: step 73090, loss = 0.52 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:01.207426: step 73100, loss = 0.56 (8017.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:01.667403: step 73110, loss = 0.61 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:01.987694: step 73120, loss = 0.73 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:02.308290: step 73130, loss = 0.61 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:02.627852: step 73140, loss = 0.67 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:02.950718: step 73150, loss = 0.55 (8143.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:03.272764: step 73160, loss = 0.61 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:03.599181: step 73170, loss = 0.72 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:03.929180: step 73180, loss = 0.62 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:04.249951: step 73190, loss = 0.54 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:04.570297: step 73200, loss = 0.79 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:05.022209: step 73210, loss = 0.65 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:05.344820: step 73220, loss = 0.63 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:05.668022: step 73230, loss = 0.58 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:05.987723: step 73240, loss = 0.63 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:06.309849: step 73250, loss = 0.61 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:06.631823: step 73260, loss = 0.57 (8192.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:06.955725: step 73270, loss = 0.47 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:07.274861: step 73280, loss = 0.50 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:07.598587: step 73290, loss = 0.54 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:07.918293: step 73300, loss = 0.66 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:08.367025: step 73310, loss = 0.54 (7813.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:08.687200: step 73320, loss = 0.64 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:09.008295: step 73330, loss = 0.59 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:09.330120: step 73340, loss = 0.67 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:09.651078: step 73350, loss = 0.51 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:09.972751: step 73360, loss = 0.59 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:10.295600: step 73370, loss = 0.67 (7882.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:10.614900: step 73380, loss = 0.63 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:10.937815: step 73390, loss = 0.65 (7956.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:11.258274: step 73400, loss = 0.57 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:11.714419: step 73410, loss = 0.42 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:12.036572: step 73420, loss = 0.56 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:12.356450: step 73430, loss = 0.55 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:12.677985: step 73440, loss = 0.59 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:12.996640: step 73450, loss = 0.61 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:13.316570: step 73460, loss = 0.66 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:13.635432: step 73470, loss = 0.57 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:13.959767: step 73480, loss = 0.59 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:14.280400: step 73490, loss = 0.53 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:14.599215: step 73500, loss = 0.62 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:15.061182: step 73510, loss = 0.54 (7835.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:15.382083: step 73520, loss = 0.59 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:15.703824: step 73530, loss = 0.65 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:16.024606: step 73540, loss = 0.58 (7899.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:16.344298: step 73550, loss = 0.63 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:16.662901: step 73560, loss = 0.63 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:16.983211: step 73570, loss = 0.69 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:17.308564: step 73580, loss = 0.61 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:17.630872: step 73590, loss = 0.63 (7803.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:17.951281: step 73600, loss = 0.55 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:18.406401: step 73610, loss = 0.53 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:18.725443: step 73620, loss = 0.58 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:19.044333: step 73630, loss = 0.56 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:19.364912: step 73640, loss = 0.63 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:19.685022: step 73650, loss = 0.58 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:20.006652: step 73660, loss = 0.55 (7892.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:20.327604: step 73670, loss = 0.53 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:20.649774: step 73680, loss = 0.56 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:20.969718: step 73690, loss = 0.62 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:21.287926: step 73700, loss = 0.59 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:21.756994: step 73710, loss = 0.62 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:22.076477: step 73720, loss = 0.62 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:22.397404: step 73730, loss = 0.54 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:22.717518: step 73740, loss = 0.51 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:23.038023: step 73750, loss = 0.48 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:23.355315: step 73760, loss = 0.67 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:23.674915: step 73770, loss = 0.55 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:23.998216: step 73780, loss = 0.57 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:24.317444: step 73790, loss = 0.61 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:24.637761: step 73800, loss = 0.76 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:25.099654: step 73810, loss = 0.52 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:25.423803: step 73820, loss = 0.55 (7490.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:25.743344: step 73830, loss = 0.61 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:26.063030: step 73840, loss = 0.68 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:26.382828: step 73850, loss = 0.53 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:26.702757: step 73860, loss = 0.60 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:27.022820: step 73870, loss = 0.46 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:27.342692: step 73880, loss = 0.63 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:27.661283: step 73890, loss = 0.59 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:27.984793: step 73900, loss = 0.61 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:28.451636: step 73910, loss = 0.64 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:28.772026: step 73920, loss = 0.52 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:29.094199: step 73930, loss = 0.41 (7732.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:29.415319: step 73940, loss = 0.61 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:29.735192: step 73950, loss = 0.52 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:30.056032: step 73960, loss = 0.47 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:30.378977: step 73970, loss = 0.57 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:30.706042: step 73980, loss = 0.56 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:31.029091: step 73990, loss = 0.51 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:31.351482: step 74000, loss = 0.65 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:31.909892: step 74010, loss = 0.55 (7777.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:32.229162: step 74020, loss = 0.75 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:32.551757: step 74030, loss = 0.56 (7452.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:32.871779: step 74040, loss = 0.54 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:33.194842: step 74050, loss = 0.54 (8130.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:33.514699: step 74060, loss = 0.48 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:33.834618: step 74070, loss = 0.63 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:34.156227: step 74080, loss = 0.61 (7806.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:34.475357: step 74090, loss = 0.56 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:34.795751: step 74100, loss = 0.57 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:35.247250: step 74110, loss = 0.66 (7762.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:35.567929: step 74120, loss = 0.54 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:35.892610: step 74130, loss = 0.60 (7637.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:36.211427: step 74140, loss = 0.56 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:36.531480: step 74150, loss = 0.61 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:36.856569: step 74160, loss = 0.61 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:37.177435: step 74170, loss = 0.49 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:37.498564: step 74180, loss = 0.59 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:37.817848: step 74190, loss = 0.56 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:38.140185: step 74200, loss = 0.55 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:38.592453: step 74210, loss = 0.56 (8161.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:38.913048: step 74220, loss = 0.61 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:39.232092: step 74230, loss = 0.70 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:39.551586: step 74240, loss = 0.64 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:39.874110: step 74250, loss = 0.62 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:40.193880: step 74260, loss = 0.63 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:40.517608: step 74270, loss = 0.52 (7811.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:40.838193: step 74280, loss = 0.65 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:41.160000: step 74290, loss = 0.73 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:41.481730: step 74300, loss = 0.67 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:41.941342: step 74310, loss = 0.66 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:42.261991: step 74320, loss = 0.71 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:42.584312: step 74330, loss = 0.53 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:42.903261: step 74340, loss = 0.54 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:43.222236: step 74350, loss = 0.75 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:43.541567: step 74360, loss = 0.56 (8130.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:43.861399: step 74370, loss = 0.57 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:44.180956: step 74380, loss = 0.57 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:44.501446: step 74390, loss = 0.69 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:44.821345: step 74400, loss = 0.51 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:45.284848: step 74410, loss = 0.59 (8037.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:45.607425: step 74420, loss = 0.61 (7388.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:45.927485: step 74430, loss = 0.73 (8138.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:46.248676: step 74440, loss = 0.61 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:46.571263: step 74450, loss = 0.61 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:46.892323: step 74460, loss = 0.50 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:47.217287: step 74470, loss = 0.62 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:47.536694: step 74480, loss = 0.51 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:47.858479: step 74490, loss = 0.52 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:48.180814: step 74500, loss = 0.61 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:48.638254: step 74510, loss = 0.52 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:48.959293: step 74520, loss = 0.63 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:49.279862: step 74530, loss = 0.67 (7950.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:49.601505: step 74540, loss = 0.65 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:49.921593: step 74550, loss = 0.58 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:50.242993: step 74560, loss = 0.63 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:50.562552: step 74570, loss = 0.50 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:50.883580: step 74580, loss = 0.52 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:51.204316: step 74590, loss = 0.57 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:51.524957: step 74600, loss = 0.56 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:51.989085: step 74610, loss = 0.64 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:52.311098: step 74620, loss = 0.56 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:52.631235: step 74630, loss = 0.70 (8171.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:52.953935: step 74640, loss = 0.55 (7581.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:53.272792: step 74650, loss = 0.57 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:53.594140: step 74660, loss = 0.70 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:53.916206: step 74670, loss = 0.57 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:54.236159: step 74680, loss = 0.46 (7814.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:54.558474: step 74690, loss = 0.62 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:54.877871: step 74700, loss = 0.58 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:55.336086: step 74710, loss = 0.50 (7675.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:21:55.657219: step 74720, loss = 0.54 (8138.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:55.978441: step 74730, loss = 0.58 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:56.298698: step 74740, loss = 0.64 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:56.619597: step 74750, loss = 0.61 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:56.943334: step 74760, loss = 0.71 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:57.264279: step 74770, loss = 0.60 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:57.585367: step 74780, loss = 0.60 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:57.908453: step 74790, loss = 0.65 (7821.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:58.229760: step 74800, loss = 0.52 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:58.689838: step 74810, loss = 0.51 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:59.010657: step 74820, loss = 0.59 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:59.331771: step 74830, loss = 0.66 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:59.653367: step 74840, loss = 0.60 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:21:59.972250: step 74850, loss = 0.70 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:00.292102: step 74860, loss = 0.67 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:00.611945: step 74870, loss = 0.64 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:00.930684: step 74880, loss = 0.44 (7977.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:01.250262: step 74890, loss = 0.67 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:01.570907: step 74900, loss = 0.49 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:02.026278: step 74910, loss = 0.71 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:02.347194: step 74920, loss = 0.58 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:02.669054: step 74930, loss = 0.55 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:02.989329: step 74940, loss = 0.64 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:03.311787: step 74950, loss = 0.66 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:03.633258: step 74960, loss = 0.65 (7905.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:03.956405: step 74970, loss = 0.66 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:04.274828: step 74980, loss = 0.63 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:04.596228: step 74990, loss = 0.57 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:04.917272: step 75000, loss = 0.59 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:05.515020: step 75010, loss = 0.66 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:05.836223: step 75020, loss = 0.51 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:06.156277: step 75030, loss = 0.52 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:06.475530: step 75040, loss = 0.46 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:06.796101: step 75050, loss = 0.64 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:07.117359: step 75060, loss = 0.59 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:07.437157: step 75070, loss = 0.74 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:07.759385: step 75080, loss = 0.55 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:08.078616: step 75090, loss = 0.67 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:08.399419: step 75100, loss = 0.50 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:08.875866: step 75110, loss = 0.47 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:09.196718: step 75120, loss = 0.76 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:09.516411: step 75130, loss = 0.66 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:09.835293: step 75140, loss = 0.46 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:10.155349: step 75150, loss = 0.64 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:10.475255: step 75160, loss = 0.70 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:10.796727: step 75170, loss = 0.56 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:11.116220: step 75180, loss = 0.58 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:11.437799: step 75190, loss = 0.73 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:11.757255: step 75200, loss = 0.70 (7805.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:12.222500: step 75210, loss = 0.60 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:12.549518: step 75220, loss = 0.62 (7683.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:22:12.868202: step 75230, loss = 0.53 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:13.191077: step 75240, loss = 0.56 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:13.510518: step 75250, loss = 0.67 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:13.833428: step 75260, loss = 0.72 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:14.152853: step 75270, loss = 0.64 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:14.471141: step 75280, loss = 0.57 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:14.794032: step 75290, loss = 0.64 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:15.114588: step 75300, loss = 0.53 (7962.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:15.574811: step 75310, loss = 0.47 (7689.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:22:15.896175: step 75320, loss = 0.67 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:16.214556: step 75330, loss = 0.54 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:16.534680: step 75340, loss = 0.57 (8149.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:16.854440: step 75350, loss = 0.51 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:17.174576: step 75360, loss = 0.56 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:17.494117: step 75370, loss = 0.76 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:17.813588: step 75380, loss = 0.61 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:18.135807: step 75390, loss = 0.56 (7802.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:18.455377: step 75400, loss = 0.65 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:18.918400: step 75410, loss = 0.55 (7922.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:19.241671: step 75420, loss = 0.60 (7847.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:19.564560: step 75430, loss = 0.62 (8037.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:19.886236: step 75440, loss = 0.52 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:20.207026: step 75450, loss = 0.69 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:20.528288: step 75460, loss = 0.65 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:20.847897: step 75470, loss = 0.69 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:21.170386: step 75480, loss = 0.46 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:21.492088: step 75490, loss = 0.58 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:21.813385: step 75500, loss = 0.45 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:22.277580: step 75510, loss = 0.64 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:22.597176: step 75520, loss = 0.72 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:22.917969: step 75530, loss = 0.65 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:23.240863: step 75540, loss = 0.62 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:23.562349: step 75550, loss = 0.69 (7801.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:23.884520: step 75560, loss = 0.57 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:24.205824: step 75570, loss = 0.56 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:24.527140: step 75580, loss = 0.61 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:24.851205: step 75590, loss = 0.50 (7590.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:22:25.169202: step 75600, loss = 0.65 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:25.620482: step 75610, loss = 0.52 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:25.943179: step 75620, loss = 0.51 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:26.263096: step 75630, loss = 0.44 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:26.582420: step 75640, loss = 0.57 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:26.901700: step 75650, loss = 0.48 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:27.223004: step 75660, loss = 0.78 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:27.545216: step 75670, loss = 0.70 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:27.867367: step 75680, loss = 0.61 (7731.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:22:28.187237: step 75690, loss = 0.66 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:28.506640: step 75700, loss = 0.54 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:28.968899: step 75710, loss = 0.60 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:29.287296: step 75720, loss = 0.56 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:29.610868: step 75730, loss = 0.57 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:29.929829: step 75740, loss = 0.53 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:30.254397: step 75750, loss = 0.58 (7796.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:30.574669: step 75760, loss = 0.63 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:30.896678: step 75770, loss = 0.48 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:31.216191: step 75780, loss = 0.53 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:31.536147: step 75790, loss = 0.68 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:31.855740: step 75800, loss = 0.66 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:32.321518: step 75810, loss = 0.57 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:32.645095: step 75820, loss = 0.62 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:32.964491: step 75830, loss = 0.58 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:33.286115: step 75840, loss = 0.60 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:33.609507: step 75850, loss = 0.72 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:33.928148: step 75860, loss = 0.54 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:34.247793: step 75870, loss = 0.62 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:34.570038: step 75880, loss = 0.64 (7888.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:34.892594: step 75890, loss = 0.54 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:35.213763: step 75900, loss = 0.62 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:35.676115: step 75910, loss = 0.62 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:36.003701: step 75920, loss = 0.49 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:36.329057: step 75930, loss = 0.48 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:36.649700: step 75940, loss = 0.62 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:36.970692: step 75950, loss = 0.62 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:37.291611: step 75960, loss = 0.64 (7872.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:37.613580: step 75970, loss = 0.69 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:37.934035: step 75980, loss = 0.59 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:38.254328: step 75990, loss = 0.60 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:38.575184: step 76000, loss = 0.68 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:39.120481: step 76010, loss = 0.60 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:39.439921: step 76020, loss = 0.49 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:39.760101: step 76030, loss = 0.60 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:40.079979: step 76040, loss = 0.52 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:40.400143: step 76050, loss = 0.44 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:40.720117: step 76060, loss = 0.57 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:41.042529: step 76070, loss = 0.63 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:41.363626: step 76080, loss = 0.65 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:41.684068: step 76090, loss = 0.62 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:42.002845: step 76100, loss = 0.60 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:42.465635: step 76110, loss = 0.65 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:42.783530: step 76120, loss = 0.68 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:43.106730: step 76130, loss = 0.52 (7786.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:43.427832: step 76140, loss = 0.65 (8034.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:43.751399: step 76150, loss = 0.60 (7567.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:22:44.071286: step 76160, loss = 0.64 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:44.394804: step 76170, loss = 0.55 (8050.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:44.716647: step 76180, loss = 0.50 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:45.034868: step 76190, loss = 0.52 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:45.354601: step 76200, loss = 0.61 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:45.807398: step 76210, loss = 0.53 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:46.126918: step 76220, loss = 0.50 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:46.447410: step 76230, loss = 0.69 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:46.765968: step 76240, loss = 0.61 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:47.090210: step 76250, loss = 0.51 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:47.411015: step 76260, loss = 0.74 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:47.730514: step 76270, loss = 0.52 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:48.049851: step 76280, loss = 0.62 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:48.370982: step 76290, loss = 0.52 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:48.692998: step 76300, loss = 0.52 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:49.154714: step 76310, loss = 0.62 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:49.476351: step 76320, loss = 0.52 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:49.796509: step 76330, loss = 0.68 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:50.119906: step 76340, loss = 0.66 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:50.440933: step 76350, loss = 0.53 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:50.766646: step 76360, loss = 0.63 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:51.087468: step 76370, loss = 0.57 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:51.409059: step 76380, loss = 0.62 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:51.729346: step 76390, loss = 0.56 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:52.049345: step 76400, loss = 0.64 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:52.513370: step 76410, loss = 0.49 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:52.835730: step 76420, loss = 0.63 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:53.156461: step 76430, loss = 0.54 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:53.477301: step 76440, loss = 0.69 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:53.797100: step 76450, loss = 0.63 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:54.116133: step 76460, loss = 0.52 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:54.434861: step 76470, loss = 0.60 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:54.755440: step 76480, loss = 0.57 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:55.075385: step 76490, loss = 0.51 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:55.396643: step 76500, loss = 0.67 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:55.854741: step 76510, loss = 0.49 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:56.174830: step 76520, loss = 0.52 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:56.498571: step 76530, loss = 0.60 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:56.817519: step 76540, loss = 0.63 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:57.140986: step 76550, loss = 0.70 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:57.459327: step 76560, loss = 0.56 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:57.779624: step 76570, loss = 0.62 (7903.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:58.099546: step 76580, loss = 0.55 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:58.418462: step 76590, loss = 0.56 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:58.741543: step 76600, loss = 0.51 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:59.203326: step 76610, loss = 0.55 (7259.3 examples/sec; 0.018 sec/batch)
2017-09-16 16:22:59.526154: step 76620, loss = 0.52 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:22:59.846731: step 76630, loss = 0.61 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:00.167125: step 76640, loss = 0.49 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:00.486359: step 76650, loss = 0.50 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:00.807086: step 76660, loss = 0.57 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:01.129493: step 76670, loss = 0.58 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:01.450444: step 76680, loss = 0.63 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:01.772211: step 76690, loss = 0.47 (7627.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:02.095266: step 76700, loss = 0.59 (7829.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:02.555332: step 76710, loss = 0.66 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:02.876886: step 76720, loss = 0.49 (7988.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:03.195925: step 76730, loss = 0.54 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:03.517638: step 76740, loss = 0.58 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:03.836236: step 76750, loss = 0.56 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:04.156925: step 76760, loss = 0.47 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:04.476916: step 76770, loss = 0.61 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:04.797554: step 76780, loss = 0.59 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:05.116663: step 76790, loss = 0.69 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:05.435370: step 76800, loss = 0.57 (8144.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:05.889134: step 76810, loss = 0.50 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:06.215410: step 76820, loss = 0.48 (7119.2 examples/sec; 0.018 sec/batch)
2017-09-16 16:23:06.536889: step 76830, loss = 0.58 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:06.859846: step 76840, loss = 0.58 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:07.179393: step 76850, loss = 0.62 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:07.499014: step 76860, loss = 0.61 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:07.819186: step 76870, loss = 0.44 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:08.140615: step 76880, loss = 0.57 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:08.460818: step 76890, loss = 0.48 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:08.783417: step 76900, loss = 0.60 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:09.234854: step 76910, loss = 0.61 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:09.555205: step 76920, loss = 0.63 (7467.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:09.877836: step 76930, loss = 0.73 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:10.197134: step 76940, loss = 0.54 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:10.517475: step 76950, loss = 0.53 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:10.838388: step 76960, loss = 0.59 (7945.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:11.158041: step 76970, loss = 0.60 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:11.477453: step 76980, loss = 0.64 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:11.798426: step 76990, loss = 0.67 (8146.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:12.118343: step 77000, loss = 0.53 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:12.670306: step 77010, loss = 0.63 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:12.988624: step 77020, loss = 0.56 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:13.310779: step 77030, loss = 0.59 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:13.631217: step 77040, loss = 0.59 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:13.950008: step 77050, loss = 0.63 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:14.270964: step 77060, loss = 0.56 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:14.592070: step 77070, loss = 0.75 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:14.916180: step 77080, loss = 0.74 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:15.236112: step 77090, loss = 0.50 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:15.557433: step 77100, loss = 0.60 (7887.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:16.019008: step 77110, loss = 0.60 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:16.338111: step 77120, loss = 0.58 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:16.657462: step 77130, loss = 0.45 (8106.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:16.977420: step 77140, loss = 0.57 (8146.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:17.296422: step 77150, loss = 0.57 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:17.616070: step 77160, loss = 0.59 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:17.934682: step 77170, loss = 0.43 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:18.259543: step 77180, loss = 0.67 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:18.581088: step 77190, loss = 0.55 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:18.902601: step 77200, loss = 0.62 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:19.361246: step 77210, loss = 0.57 (8087.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:19.682439: step 77220, loss = 0.55 (7815.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:20.003552: step 77230, loss = 0.58 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:20.325034: step 77240, loss = 0.52 (7830.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:20.644038: step 77250, loss = 0.63 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:20.966084: step 77260, loss = 0.48 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:21.288738: step 77270, loss = 0.60 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:21.608797: step 77280, loss = 0.60 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:21.930308: step 77290, loss = 0.66 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:22.249701: step 77300, loss = 0.58 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:22.704699: step 77310, loss = 0.54 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:23.026225: step 77320, loss = 0.58 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:23.344755: step 77330, loss = 0.60 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:23.664852: step 77340, loss = 0.62 (8055.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:23.983938: step 77350, loss = 0.64 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:24.303998: step 77360, loss = 0.63 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:24.624813: step 77370, loss = 0.67 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:24.944231: step 77380, loss = 0.61 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:25.265229: step 77390, loss = 0.60 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:25.586791: step 77400, loss = 0.60 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:26.045893: step 77410, loss = 0.58 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:26.365843: step 77420, loss = 0.62 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:26.686299: step 77430, loss = 0.64 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:27.005789: step 77440, loss = 0.65 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:27.328641: step 77450, loss = 0.47 (7414.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:27.647962: step 77460, loss = 0.62 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:27.966900: step 77470, loss = 0.54 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:28.289064: step 77480, loss = 0.57 (7855.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:28.609773: step 77490, loss = 0.69 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:28.930716: step 77500, loss = 0.62 (7670.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:29.391805: step 77510, loss = 0.54 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:29.712232: step 77520, loss = 0.68 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:30.033007: step 77530, loss = 0.57 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:30.353567: step 77540, loss = 0.60 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:30.675529: step 77550, loss = 0.57 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:30.995523: step 77560, loss = 0.58 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:31.314338: step 77570, loss = 0.62 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:31.636070: step 77580, loss = 0.49 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:31.959597: step 77590, loss = 0.54 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:32.281621: step 77600, loss = 0.49 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:32.744392: step 77610, loss = 0.64 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:33.066011: step 77620, loss = 0.65 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:33.386077: step 77630, loss = 0.54 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:33.705875: step 77640, loss = 0.64 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:34.025537: step 77650, loss = 0.64 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:34.346530: step 77660, loss = 0.51 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:34.664561: step 77670, loss = 0.60 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:34.985567: step 77680, loss = 0.63 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:35.307043: step 77690, loss = 0.65 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:35.628105: step 77700, loss = 0.63 (7895.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:36.082347: step 77710, loss = 0.70 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:36.403338: step 77720, loss = 0.67 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:36.725159: step 77730, loss = 0.55 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:37.046773: step 77740, loss = 0.59 (7856.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:37.368611: step 77750, loss = 0.53 (7417.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:37.687970: step 77760, loss = 0.66 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:38.006914: step 77770, loss = 0.57 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:38.327433: step 77780, loss = 0.57 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:38.647850: step 77790, loss = 0.48 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:38.967694: step 77800, loss = 0.59 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:39.425512: step 77810, loss = 0.57 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:39.747388: step 77820, loss = 0.60 (7567.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:40.069074: step 77830, loss = 0.64 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:40.389441: step 77840, loss = 0.56 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:40.711291: step 77850, loss = 0.59 (7983.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:41.031613: step 77860, loss = 0.61 (7814.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:41.352224: step 77870, loss = 0.54 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:41.675189: step 77880, loss = 0.50 (7531.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:42.008563: step 77890, loss = 0.60 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:42.328539: step 77900, loss = 0.59 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:42.788160: step 77910, loss = 0.54 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:43.107884: step 77920, loss = 0.53 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:43.428084: step 77930, loss = 0.61 (8164.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:43.748695: step 77940, loss = 0.59 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:44.068903: step 77950, loss = 0.58 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:44.387625: step 77960, loss = 0.58 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:44.707909: step 77970, loss = 0.49 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:45.028078: step 77980, loss = 0.61 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:45.348549: step 77990, loss = 0.52 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:45.669312: step 78000, loss = 0.61 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:46.229029: step 78010, loss = 0.55 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:46.549622: step 78020, loss = 0.54 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:46.870945: step 78030, loss = 0.62 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:47.192195: step 78040, loss = 0.50 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:47.514580: step 78050, loss = 0.55 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:47.836478: step 78060, loss = 0.47 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:48.157235: step 78070, loss = 0.74 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:48.482416: step 78080, loss = 0.58 (7583.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:23:48.804219: step 78090, loss = 0.60 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:49.124984: step 78100, loss = 0.45 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:49.586532: step 78110, loss = 0.65 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:49.906439: step 78120, loss = 0.56 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:50.227302: step 78130, loss = 0.56 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:50.550119: step 78140, loss = 0.50 (8031.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:50.869420: step 78150, loss = 0.61 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:51.189594: step 78160, loss = 0.69 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:51.511749: step 78170, loss = 0.54 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:51.832316: step 78180, loss = 0.81 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:52.152045: step 78190, loss = 0.54 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:52.470855: step 78200, loss = 0.58 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:52.920358: step 78210, loss = 0.59 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:53.239269: step 78220, loss = 0.65 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:53.559022: step 78230, loss = 0.59 (8132.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:53.879945: step 78240, loss = 0.65 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:54.201693: step 78250, loss = 0.52 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:54.523186: step 78260, loss = 0.58 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:54.843302: step 78270, loss = 0.48 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:55.163511: step 78280, loss = 0.61 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:55.485147: step 78290, loss = 0.64 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:55.803700: step 78300, loss = 0.49 (8150.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:56.247849: step 78310, loss = 0.62 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:56.569872: step 78320, loss = 0.39 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:56.888170: step 78330, loss = 0.62 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:57.208099: step 78340, loss = 0.68 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:57.534958: step 78350, loss = 0.54 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:57.859430: step 78360, loss = 0.60 (7905.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:58.180123: step 78370, loss = 0.65 (7897.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:58.499015: step 78380, loss = 0.55 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:58.820628: step 78390, loss = 0.68 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:59.140374: step 78400, loss = 0.50 (7934.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:59.606024: step 78410, loss = 0.64 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:23:59.929042: step 78420, loss = 0.64 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:00.249580: step 78430, loss = 0.55 (8003.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:00.567972: step 78440, loss = 0.47 (7855.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:00.888310: step 78450, loss = 0.66 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:01.207876: step 78460, loss = 0.57 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:01.529403: step 78470, loss = 0.53 (7821.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:01.850686: step 78480, loss = 0.57 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:02.174869: step 78490, loss = 0.63 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:02.494892: step 78500, loss = 0.48 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:02.956251: step 78510, loss = 0.60 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:03.276071: step 78520, loss = 0.68 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:03.596430: step 78530, loss = 0.60 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:03.916190: step 78540, loss = 0.49 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:04.236476: step 78550, loss = 0.45 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:04.555701: step 78560, loss = 0.78 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:04.874113: step 78570, loss = 0.49 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:05.194514: step 78580, loss = 0.65 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:05.512253: step 78590, loss = 0.59 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:05.830704: step 78600, loss = 0.57 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:06.287190: step 78610, loss = 0.55 (7926.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:06.607016: step 78620, loss = 0.62 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:06.928574: step 78630, loss = 0.61 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:07.247816: step 78640, loss = 0.60 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:07.566812: step 78650, loss = 0.71 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:07.885780: step 78660, loss = 0.78 (8152.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:08.206158: step 78670, loss = 0.66 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:08.529540: step 78680, loss = 0.63 (7406.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:24:08.855981: step 78690, loss = 0.59 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:09.177493: step 78700, loss = 0.65 (7814.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:09.642252: step 78710, loss = 0.47 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:09.963341: step 78720, loss = 0.49 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:10.284288: step 78730, loss = 0.57 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:10.603596: step 78740, loss = 0.57 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:10.925762: step 78750, loss = 0.53 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:11.244248: step 78760, loss = 0.59 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:11.563403: step 78770, loss = 0.62 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:11.887484: step 78780, loss = 0.61 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:12.207526: step 78790, loss = 0.51 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:12.528564: step 78800, loss = 0.70 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:12.982005: step 78810, loss = 0.64 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:13.302353: step 78820, loss = 0.44 (8050.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:13.626686: step 78830, loss = 0.53 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:13.949661: step 78840, loss = 0.46 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:14.271539: step 78850, loss = 0.62 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:14.594474: step 78860, loss = 0.62 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:14.916122: step 78870, loss = 0.56 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:15.236218: step 78880, loss = 0.57 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:15.557179: step 78890, loss = 0.57 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:15.879777: step 78900, loss = 0.57 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:16.338197: step 78910, loss = 0.57 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:16.660163: step 78920, loss = 0.56 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:16.981180: step 78930, loss = 0.58 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:17.300954: step 78940, loss = 0.60 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:17.620572: step 78950, loss = 0.54 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:17.941149: step 78960, loss = 0.61 (7973.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:18.262549: step 78970, loss = 0.65 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:18.582042: step 78980, loss = 0.65 (8163.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:18.905628: step 78990, loss = 0.55 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:19.226143: step 79000, loss = 0.65 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:19.782865: step 79010, loss = 0.72 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:20.101035: step 79020, loss = 0.57 (8097.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:20.420266: step 79030, loss = 0.61 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:20.739702: step 79040, loss = 0.53 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:21.061859: step 79050, loss = 0.65 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:21.381493: step 79060, loss = 0.57 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:21.706197: step 79070, loss = 0.69 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:22.025042: step 79080, loss = 0.62 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:22.351255: step 79090, loss = 0.57 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:22.671606: step 79100, loss = 0.53 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:23.123056: step 79110, loss = 0.64 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:23.442648: step 79120, loss = 0.81 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:23.763768: step 79130, loss = 0.51 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:24.085018: step 79140, loss = 0.60 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:24.409965: step 79150, loss = 0.47 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:24.729250: step 79160, loss = 0.52 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:25.050052: step 79170, loss = 0.48 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:25.372662: step 79180, loss = 0.60 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:25.693771: step 79190, loss = 0.65 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:26.015868: step 79200, loss = 0.58 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:26.477002: step 79210, loss = 0.57 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:26.799471: step 79220, loss = 0.47 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:27.120086: step 79230, loss = 0.58 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:27.440290: step 79240, loss = 0.65 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:27.763436: step 79250, loss = 0.54 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:28.085707: step 79260, loss = 0.69 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:28.404956: step 79270, loss = 0.62 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:28.726401: step 79280, loss = 0.64 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:29.046868: step 79290, loss = 0.58 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:29.367941: step 79300, loss = 0.51 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:29.829192: step 79310, loss = 0.53 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:30.149009: step 79320, loss = 0.60 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:30.468256: step 79330, loss = 0.59 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:30.791642: step 79340, loss = 0.62 (7808.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:31.111664: step 79350, loss = 0.52 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:31.429973: step 79360, loss = 0.65 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:31.751927: step 79370, loss = 0.66 (7789.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:32.075597: step 79380, loss = 0.57 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:32.396396: step 79390, loss = 0.58 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:32.717580: step 79400, loss = 0.63 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:33.178552: step 79410, loss = 0.78 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:33.499165: step 79420, loss = 0.57 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:33.819567: step 79430, loss = 0.60 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:34.141389: step 79440, loss = 0.63 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:34.461894: step 79450, loss = 0.44 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:34.781290: step 79460, loss = 0.63 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:35.102844: step 79470, loss = 0.51 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:35.421091: step 79480, loss = 0.54 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:35.740592: step 79490, loss = 0.58 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:36.060581: step 79500, loss = 0.58 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:36.509197: step 79510, loss = 0.49 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:36.829579: step 79520, loss = 0.75 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:37.149703: step 79530, loss = 0.49 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:37.469178: step 79540, loss = 0.51 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:37.791022: step 79550, loss = 0.58 (7819.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:38.113323: step 79560, loss = 0.55 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:38.434802: step 79570, loss = 0.48 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:38.756896: step 79580, loss = 0.63 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:39.078498: step 79590, loss = 0.62 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:39.396662: step 79600, loss = 0.55 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:39.850219: step 79610, loss = 0.64 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:40.171375: step 79620, loss = 0.59 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:40.495169: step 79630, loss = 0.62 (7833.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:40.815713: step 79640, loss = 0.47 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:41.140229: step 79650, loss = 0.53 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:41.462962: step 79660, loss = 0.58 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:41.784052: step 79670, loss = 0.67 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:42.105568: step 79680, loss = 0.50 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:42.427723: step 79690, loss = 0.57 (8122.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:42.752099: step 79700, loss = 0.62 (8017.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:43.212665: step 79710, loss = 0.63 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:43.532920: step 79720, loss = 0.61 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:43.854753: step 79730, loss = 0.66 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:44.174865: step 79740, loss = 0.62 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:44.494855: step 79750, loss = 0.72 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:44.816498: step 79760, loss = 0.60 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:45.140994: step 79770, loss = 0.64 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:45.464143: step 79780, loss = 0.75 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:45.785221: step 79790, loss = 0.52 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:46.108649: step 79800, loss = 0.61 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:46.575581: step 79810, loss = 0.56 (8170.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:46.896489: step 79820, loss = 0.50 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:47.217243: step 79830, loss = 0.56 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:47.539171: step 79840, loss = 0.50 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:47.860056: step 79850, loss = 0.57 (8095.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:48.177972: step 79860, loss = 0.61 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:48.496909: step 79870, loss = 0.52 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:48.818412: step 79880, loss = 0.67 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:49.139212: step 79890, loss = 0.60 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:49.459200: step 79900, loss = 0.52 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:49.919723: step 79910, loss = 0.57 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:50.239168: step 79920, loss = 0.62 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:50.561493: step 79930, loss = 0.52 (7813.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:50.882599: step 79940, loss = 0.52 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:51.201631: step 79950, loss = 0.55 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:51.520983: step 79960, loss = 0.61 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:51.842672: step 79970, loss = 0.51 (7841.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:52.162674: step 79980, loss = 0.60 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:52.483274: step 79990, loss = 0.50 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:52.804157: step 80000, loss = 0.55 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:53.361891: step 80010, loss = 0.49 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:53.679365: step 80020, loss = 0.54 (8135.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:53.998935: step 80030, loss = 0.42 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:54.317292: step 80040, loss = 0.66 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:54.638379: step 80050, loss = 0.54 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:54.958933: step 80060, loss = 0.62 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:55.282180: step 80070, loss = 0.57 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:55.600950: step 80080, loss = 0.50 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:55.920839: step 80090, loss = 0.51 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:56.240810: step 80100, loss = 0.57 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:56.706597: step 80110, loss = 0.75 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:57.026865: step 80120, loss = 0.53 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:57.346079: step 80130, loss = 0.64 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:57.666957: step 80140, loss = 0.64 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:57.989239: step 80150, loss = 0.66 (7876.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:58.311009: step 80160, loss = 0.45 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:58.632602: step 80170, loss = 0.50 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:58.956321: step 80180, loss = 0.65 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:59.275039: step 80190, loss = 0.65 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:24:59.595665: step 80200, loss = 0.56 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:00.055830: step 80210, loss = 0.50 (7922.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:00.380080: step 80220, loss = 0.65 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:00.706025: step 80230, loss = 0.61 (7437.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:01.026357: step 80240, loss = 0.45 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:01.349192: step 80250, loss = 0.61 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:01.670543: step 80260, loss = 0.55 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:01.989852: step 80270, loss = 0.57 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:02.309762: step 80280, loss = 0.49 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:02.631702: step 80290, loss = 0.58 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:02.951638: step 80300, loss = 0.62 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:03.413469: step 80310, loss = 0.58 (8039.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:03.733369: step 80320, loss = 0.54 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:04.053396: step 80330, loss = 0.63 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:04.372736: step 80340, loss = 0.61 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:04.692354: step 80350, loss = 0.60 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:05.013294: step 80360, loss = 0.50 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:05.332653: step 80370, loss = 0.66 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:05.653214: step 80380, loss = 0.61 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:05.973602: step 80390, loss = 0.63 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:06.293447: step 80400, loss = 0.50 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:06.758157: step 80410, loss = 0.59 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:07.078695: step 80420, loss = 0.45 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:07.399653: step 80430, loss = 0.56 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:07.720591: step 80440, loss = 0.49 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:08.041385: step 80450, loss = 0.56 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:08.361358: step 80460, loss = 0.65 (8128.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:08.681879: step 80470, loss = 0.67 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:09.000383: step 80480, loss = 0.58 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:09.322168: step 80490, loss = 0.59 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:09.644334: step 80500, loss = 0.57 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:10.104568: step 80510, loss = 0.62 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:10.423783: step 80520, loss = 0.57 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:10.746156: step 80530, loss = 0.68 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:11.068413: step 80540, loss = 0.61 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:11.389427: step 80550, loss = 0.56 (8032.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:11.710747: step 80560, loss = 0.62 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:12.034210: step 80570, loss = 0.64 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:12.353191: step 80580, loss = 0.69 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:12.675480: step 80590, loss = 0.58 (7795.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:12.995259: step 80600, loss = 0.56 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:13.459418: step 80610, loss = 0.69 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:13.783565: step 80620, loss = 0.73 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:14.104751: step 80630, loss = 0.58 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:14.423778: step 80640, loss = 0.47 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:14.745946: step 80650, loss = 0.65 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:15.065697: step 80660, loss = 0.51 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:15.386209: step 80670, loss = 0.59 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:15.708022: step 80680, loss = 0.49 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:16.026812: step 80690, loss = 0.56 (7956.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:16.349082: step 80700, loss = 0.55 (7820.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:16.807124: step 80710, loss = 0.55 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:17.128366: step 80720, loss = 0.59 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:17.448220: step 80730, loss = 0.49 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:17.768383: step 80740, loss = 0.50 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:18.089095: step 80750, loss = 0.58 (7886.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:18.408127: step 80760, loss = 0.63 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:18.727279: step 80770, loss = 0.63 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:19.048075: step 80780, loss = 0.73 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:19.368385: step 80790, loss = 0.58 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:19.687881: step 80800, loss = 0.54 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:20.142151: step 80810, loss = 0.70 (7935.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:20.461927: step 80820, loss = 0.59 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:20.781944: step 80830, loss = 0.65 (8182.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:21.101963: step 80840, loss = 0.62 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:21.422647: step 80850, loss = 0.57 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:21.744827: step 80860, loss = 0.72 (7962.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:22.068018: step 80870, loss = 0.64 (7626.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:22.387912: step 80880, loss = 0.52 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:22.711532: step 80890, loss = 0.54 (7544.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:23.032371: step 80900, loss = 0.60 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:23.479645: step 80910, loss = 0.65 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:23.804308: step 80920, loss = 0.62 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:24.124456: step 80930, loss = 0.54 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:24.444270: step 80940, loss = 0.49 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:24.766105: step 80950, loss = 0.66 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:25.086102: step 80960, loss = 0.70 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:25.408071: step 80970, loss = 0.60 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:25.729768: step 80980, loss = 0.51 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:26.049884: step 80990, loss = 0.51 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:26.369302: step 81000, loss = 0.59 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:26.921467: step 81010, loss = 0.53 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:27.241682: step 81020, loss = 0.55 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:27.561515: step 81030, loss = 0.51 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:27.884110: step 81040, loss = 0.62 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:28.204746: step 81050, loss = 0.49 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:28.524905: step 81060, loss = 0.62 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:28.845106: step 81070, loss = 0.62 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:29.165666: step 81080, loss = 0.56 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:29.486297: step 81090, loss = 0.61 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:29.807723: step 81100, loss = 0.63 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:30.269885: step 81110, loss = 0.53 (8055.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:30.587921: step 81120, loss = 0.53 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:30.908333: step 81130, loss = 0.63 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:31.232359: step 81140, loss = 0.72 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:31.557314: step 81150, loss = 0.65 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:31.877755: step 81160, loss = 0.54 (7531.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:32.198962: step 81170, loss = 0.52 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:32.519204: step 81180, loss = 0.61 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:32.840422: step 81190, loss = 0.66 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:33.164222: step 81200, loss = 0.57 (7620.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:33.617384: step 81210, loss = 0.57 (8060.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:33.940145: step 81220, loss = 0.58 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:34.258838: step 81230, loss = 0.50 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:34.578933: step 81240, loss = 0.57 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:34.898993: step 81250, loss = 0.61 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:35.218497: step 81260, loss = 0.46 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:35.538304: step 81270, loss = 0.59 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:35.859733: step 81280, loss = 0.62 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:36.181644: step 81290, loss = 0.59 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:36.502575: step 81300, loss = 0.50 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:36.964575: step 81310, loss = 0.53 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:37.285392: step 81320, loss = 0.50 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:37.605635: step 81330, loss = 0.54 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:37.928028: step 81340, loss = 0.54 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:38.247505: step 81350, loss = 0.53 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:38.569971: step 81360, loss = 0.63 (7623.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:38.889490: step 81370, loss = 0.56 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:39.209735: step 81380, loss = 0.54 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:39.529113: step 81390, loss = 0.48 (7954.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:39.850163: step 81400, loss = 0.70 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:40.299291: step 81410, loss = 0.54 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:40.621265: step 81420, loss = 0.46 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:40.939996: step 81430, loss = 0.61 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:41.260537: step 81440, loss = 0.62 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:41.582027: step 81450, loss = 0.53 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:41.908868: step 81460, loss = 0.73 (7574.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:42.232678: step 81470, loss = 0.63 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:42.551796: step 81480, loss = 0.65 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:42.873796: step 81490, loss = 0.70 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:43.195106: step 81500, loss = 0.64 (7789.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:43.655796: step 81510, loss = 0.61 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:43.981440: step 81520, loss = 0.48 (7976.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:44.305085: step 81530, loss = 0.70 (7821.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:44.625356: step 81540, loss = 0.62 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:44.946200: step 81550, loss = 0.50 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:45.265751: step 81560, loss = 0.48 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:45.583800: step 81570, loss = 0.55 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:45.908286: step 81580, loss = 0.61 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:46.230162: step 81590, loss = 0.54 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:46.549224: step 81600, loss = 0.59 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:47.008085: step 81610, loss = 0.56 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:47.330262: step 81620, loss = 0.57 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:47.653461: step 81630, loss = 0.56 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:47.973284: step 81640, loss = 0.55 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:48.294088: step 81650, loss = 0.40 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:48.612959: step 81660, loss = 0.64 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:48.933605: step 81670, loss = 0.68 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:49.256808: step 81680, loss = 0.64 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:49.575370: step 81690, loss = 0.58 (8173.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:49.895165: step 81700, loss = 0.54 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:50.354495: step 81710, loss = 0.53 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:50.675619: step 81720, loss = 0.46 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:50.997720: step 81730, loss = 0.55 (7825.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:51.318242: step 81740, loss = 0.54 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:51.638290: step 81750, loss = 0.56 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:51.961815: step 81760, loss = 0.53 (7888.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:52.280881: step 81770, loss = 0.52 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:52.600013: step 81780, loss = 0.57 (7840.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:52.920310: step 81790, loss = 0.65 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:53.240135: step 81800, loss = 0.56 (8178.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:53.699054: step 81810, loss = 0.62 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:54.018982: step 81820, loss = 0.58 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:54.340179: step 81830, loss = 0.54 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:54.659114: step 81840, loss = 0.58 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:54.978102: step 81850, loss = 0.64 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:55.298471: step 81860, loss = 0.66 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:55.618102: step 81870, loss = 0.65 (8054.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:55.939750: step 81880, loss = 0.73 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:56.258468: step 81890, loss = 0.53 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:56.579469: step 81900, loss = 0.63 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:57.044659: step 81910, loss = 0.65 (7539.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:25:57.363391: step 81920, loss = 0.58 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:57.682307: step 81930, loss = 0.51 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:58.001173: step 81940, loss = 0.57 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:58.320683: step 81950, loss = 0.62 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:58.641104: step 81960, loss = 0.51 (8165.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:58.961707: step 81970, loss = 0.44 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:59.281806: step 81980, loss = 0.55 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:59.601799: step 81990, loss = 0.49 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:25:59.922376: step 82000, loss = 0.62 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:00.525563: step 82010, loss = 0.59 (7609.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:00.848463: step 82020, loss = 0.62 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:01.169934: step 82030, loss = 0.57 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:01.490764: step 82040, loss = 0.57 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:01.811793: step 82050, loss = 0.66 (8101.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:02.130986: step 82060, loss = 0.54 (8157.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:02.450931: step 82070, loss = 0.60 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:02.772095: step 82080, loss = 0.62 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:03.091446: step 82090, loss = 0.53 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:03.413030: step 82100, loss = 0.67 (7798.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:03.871018: step 82110, loss = 0.53 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:04.191625: step 82120, loss = 0.46 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:04.512559: step 82130, loss = 0.62 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:04.832657: step 82140, loss = 0.53 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:05.151754: step 82150, loss = 0.61 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:05.471828: step 82160, loss = 0.55 (8128.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:05.793302: step 82170, loss = 0.65 (7786.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:06.112359: step 82180, loss = 0.62 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:06.432458: step 82190, loss = 0.60 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:06.753131: step 82200, loss = 0.67 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:07.209768: step 82210, loss = 0.57 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:07.528381: step 82220, loss = 0.57 (8140.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:07.847326: step 82230, loss = 0.65 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:08.165380: step 82240, loss = 0.62 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:08.484334: step 82250, loss = 0.48 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:08.805521: step 82260, loss = 0.58 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:09.126302: step 82270, loss = 0.48 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:09.451258: step 82280, loss = 0.62 (7510.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:09.772276: step 82290, loss = 0.57 (7643.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:10.096656: step 82300, loss = 0.50 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:10.548453: step 82310, loss = 0.63 (8094.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:10.869096: step 82320, loss = 0.59 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:11.190489: step 82330, loss = 0.59 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:11.511664: step 82340, loss = 0.61 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:11.837321: step 82350, loss = 0.51 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:12.157761: step 82360, loss = 0.55 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:12.478083: step 82370, loss = 0.65 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:12.796924: step 82380, loss = 0.57 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:13.117602: step 82390, loss = 0.61 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:13.440840: step 82400, loss = 0.61 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:13.899142: step 82410, loss = 0.76 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:14.218911: step 82420, loss = 0.44 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:14.538114: step 82430, loss = 0.60 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:14.859208: step 82440, loss = 0.81 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:15.181158: step 82450, loss = 0.61 (8078.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:15.504698: step 82460, loss = 0.73 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:15.825126: step 82470, loss = 0.79 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:16.146127: step 82480, loss = 0.59 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:16.470837: step 82490, loss = 0.45 (7623.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:16.793095: step 82500, loss = 0.60 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:17.257878: step 82510, loss = 0.63 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:17.577169: step 82520, loss = 0.69 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:17.896638: step 82530, loss = 0.59 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:18.218471: step 82540, loss = 0.55 (7729.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:18.538348: step 82550, loss = 0.53 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:18.860207: step 82560, loss = 0.46 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:19.183671: step 82570, loss = 0.65 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:19.508899: step 82580, loss = 0.78 (7471.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:19.830555: step 82590, loss = 0.58 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:20.154998: step 82600, loss = 0.53 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:20.615424: step 82610, loss = 0.62 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:20.934492: step 82620, loss = 0.52 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:21.256037: step 82630, loss = 0.68 (7798.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:21.578822: step 82640, loss = 0.56 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:21.903901: step 82650, loss = 0.68 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:22.230278: step 82660, loss = 0.58 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:22.550256: step 82670, loss = 0.63 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:22.871945: step 82680, loss = 0.72 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:23.192353: step 82690, loss = 0.58 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:23.511931: step 82700, loss = 0.60 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:23.971455: step 82710, loss = 0.64 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:24.290426: step 82720, loss = 0.62 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:24.611393: step 82730, loss = 0.59 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:24.934785: step 82740, loss = 0.56 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:25.253188: step 82750, loss = 0.68 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:25.576113: step 82760, loss = 0.61 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:25.900173: step 82770, loss = 0.57 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:26.220750: step 82780, loss = 0.54 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:26.541390: step 82790, loss = 0.59 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:26.859845: step 82800, loss = 0.64 (8005.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:27.317526: step 82810, loss = 0.60 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:27.638114: step 82820, loss = 0.52 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:27.958642: step 82830, loss = 0.55 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:28.280850: step 82840, loss = 0.65 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:28.604165: step 82850, loss = 0.58 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:28.927030: step 82860, loss = 0.62 (7853.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:29.247010: step 82870, loss = 0.65 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:29.566989: step 82880, loss = 0.53 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:29.887545: step 82890, loss = 0.65 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:30.208248: step 82900, loss = 0.55 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:30.669700: step 82910, loss = 0.58 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:30.990157: step 82920, loss = 0.63 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:31.312517: step 82930, loss = 0.52 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:31.632805: step 82940, loss = 0.55 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:31.956066: step 82950, loss = 0.63 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:32.276662: step 82960, loss = 0.61 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:32.597003: step 82970, loss = 0.46 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:32.916967: step 82980, loss = 0.54 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:33.236361: step 82990, loss = 0.49 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:33.558778: step 83000, loss = 0.50 (7835.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:34.120011: step 83010, loss = 0.74 (8114.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:34.441189: step 83020, loss = 0.52 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:34.765914: step 83030, loss = 0.60 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:35.087530: step 83040, loss = 0.48 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:35.409610: step 83050, loss = 0.55 (7886.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:35.728724: step 83060, loss = 0.52 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:36.047795: step 83070, loss = 0.54 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:36.368682: step 83080, loss = 0.72 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:36.689321: step 83090, loss = 0.56 (7966.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:37.010078: step 83100, loss = 0.63 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:37.462290: step 83110, loss = 0.67 (8069.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:37.782200: step 83120, loss = 0.61 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:38.100966: step 83130, loss = 0.66 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:38.420231: step 83140, loss = 0.55 (8137.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:38.739374: step 83150, loss = 0.58 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:39.058392: step 83160, loss = 0.55 (7889.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:39.377291: step 83170, loss = 0.50 (7913.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:39.697229: step 83180, loss = 0.59 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:40.017909: step 83190, loss = 0.55 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:40.340014: step 83200, loss = 0.46 (7515.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:40.805246: step 83210, loss = 0.51 (7479.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:41.124666: step 83220, loss = 0.62 (7941.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:41.443729: step 83230, loss = 0.59 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:41.764324: step 83240, loss = 0.76 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:42.082480: step 83250, loss = 0.54 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:42.407817: step 83260, loss = 0.46 (8175.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:42.732078: step 83270, loss = 0.66 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:43.053518: step 83280, loss = 0.50 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:43.375998: step 83290, loss = 0.62 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:43.694858: step 83300, loss = 0.58 (7921.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:44.139138: step 83310, loss = 0.61 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:44.459692: step 83320, loss = 0.71 (7681.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:44.778984: step 83330, loss = 0.63 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:45.099224: step 83340, loss = 0.52 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:45.417452: step 83350, loss = 0.58 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:45.740346: step 83360, loss = 0.65 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:46.060775: step 83370, loss = 0.47 (7833.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:46.380944: step 83380, loss = 0.56 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:46.699839: step 83390, loss = 0.64 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:47.019552: step 83400, loss = 0.63 (8021.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:47.483820: step 83410, loss = 0.51 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:47.810381: step 83420, loss = 0.51 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:48.135077: step 83430, loss = 0.64 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:48.456479: step 83440, loss = 0.56 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:48.778308: step 83450, loss = 0.49 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:49.100567: step 83460, loss = 0.61 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:49.422059: step 83470, loss = 0.69 (7987.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:49.741623: step 83480, loss = 0.68 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:50.060874: step 83490, loss = 0.59 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:50.385001: step 83500, loss = 0.57 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:50.841774: step 83510, loss = 0.52 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:51.162398: step 83520, loss = 0.62 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:51.482035: step 83530, loss = 0.55 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:51.801235: step 83540, loss = 0.51 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:52.121846: step 83550, loss = 0.69 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:52.441583: step 83560, loss = 0.66 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:52.761481: step 83570, loss = 0.65 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:53.082402: step 83580, loss = 0.61 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:53.403061: step 83590, loss = 0.49 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:53.722600: step 83600, loss = 0.54 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:54.183192: step 83610, loss = 0.51 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:54.502838: step 83620, loss = 0.64 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:54.823497: step 83630, loss = 0.58 (7912.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:55.144813: step 83640, loss = 0.69 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:55.466104: step 83650, loss = 0.51 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:55.784677: step 83660, loss = 0.55 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:56.106122: step 83670, loss = 0.76 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:56.426916: step 83680, loss = 0.55 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:56.746237: step 83690, loss = 0.53 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:57.069628: step 83700, loss = 0.56 (7884.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:57.533309: step 83710, loss = 0.55 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:57.854195: step 83720, loss = 0.60 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:58.179138: step 83730, loss = 0.60 (7448.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:26:58.500204: step 83740, loss = 0.64 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:58.825208: step 83750, loss = 0.65 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:59.145952: step 83760, loss = 0.60 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:59.466916: step 83770, loss = 0.63 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:26:59.788430: step 83780, loss = 0.57 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:00.113987: step 83790, loss = 0.62 (7774.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:00.434489: step 83800, loss = 0.52 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:00.888809: step 83810, loss = 0.54 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:01.210622: step 83820, loss = 0.60 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:01.531671: step 83830, loss = 0.54 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:01.851794: step 83840, loss = 0.72 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:02.172937: step 83850, loss = 0.57 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:02.494524: step 83860, loss = 0.59 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:02.816682: step 83870, loss = 0.50 (7706.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:27:03.138910: step 83880, loss = 0.63 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:03.460069: step 83890, loss = 0.62 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:03.782671: step 83900, loss = 0.59 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:04.248374: step 83910, loss = 0.63 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:04.570467: step 83920, loss = 0.61 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:04.890444: step 83930, loss = 0.63 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:05.210697: step 83940, loss = 0.54 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:05.531560: step 83950, loss = 0.76 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:05.853058: step 83960, loss = 0.51 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:06.175518: step 83970, loss = 0.79 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:06.497140: step 83980, loss = 0.70 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:06.819911: step 83990, loss = 0.48 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:07.141095: step 84000, loss = 0.57 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:07.698739: step 84010, loss = 0.52 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:08.019379: step 84020, loss = 0.58 (8122.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:08.339509: step 84030, loss = 0.56 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:08.661624: step 84040, loss = 0.57 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:08.983761: step 84050, loss = 0.55 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:09.306244: step 84060, loss = 0.59 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:09.628901: step 84070, loss = 0.61 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:09.949955: step 84080, loss = 0.69 (7791.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:10.273832: step 84090, loss = 0.64 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:10.595127: step 84100, loss = 0.54 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:11.048231: step 84110, loss = 0.49 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:11.369335: step 84120, loss = 0.68 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:11.689740: step 84130, loss = 0.55 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:12.014762: step 84140, loss = 0.68 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:12.342486: step 84150, loss = 0.51 (7606.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:27:12.664374: step 84160, loss = 0.58 (8038.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:12.985003: step 84170, loss = 0.57 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:13.306968: step 84180, loss = 0.72 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:13.627150: step 84190, loss = 0.60 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:13.946279: step 84200, loss = 0.62 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:14.408058: step 84210, loss = 0.65 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:14.732181: step 84220, loss = 0.59 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:15.054512: step 84230, loss = 0.63 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:15.374495: step 84240, loss = 0.52 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:15.697486: step 84250, loss = 0.59 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:16.018670: step 84260, loss = 0.49 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:16.342176: step 84270, loss = 0.56 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:16.666202: step 84280, loss = 0.57 (7754.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:27:16.987310: step 84290, loss = 0.60 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:17.310601: step 84300, loss = 0.59 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:17.763277: step 84310, loss = 0.53 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:18.082743: step 84320, loss = 0.59 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:18.403028: step 84330, loss = 0.50 (7941.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:18.723476: step 84340, loss = 0.76 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:19.043519: step 84350, loss = 0.64 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:19.364679: step 84360, loss = 0.59 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:19.684455: step 84370, loss = 0.58 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:20.005722: step 84380, loss = 0.57 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:20.325959: step 84390, loss = 0.60 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:20.645821: step 84400, loss = 0.51 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:21.108228: step 84410, loss = 0.64 (8198.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:21.428789: step 84420, loss = 0.64 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:21.750502: step 84430, loss = 0.46 (7806.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:22.072058: step 84440, loss = 0.51 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:22.389954: step 84450, loss = 0.69 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:22.711620: step 84460, loss = 0.73 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:23.031301: step 84470, loss = 0.69 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:23.350683: step 84480, loss = 0.70 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:23.671929: step 84490, loss = 0.71 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:23.993792: step 84500, loss = 0.62 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:24.454014: step 84510, loss = 0.68 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:24.773291: step 84520, loss = 0.49 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:25.094156: step 84530, loss = 0.63 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:25.416192: step 84540, loss = 0.56 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:25.737022: step 84550, loss = 0.50 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:26.056187: step 84560, loss = 0.63 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:26.378089: step 84570, loss = 0.60 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:26.700308: step 84580, loss = 0.53 (7805.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:27.019626: step 84590, loss = 0.50 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:27.339025: step 84600, loss = 0.45 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:27.795703: step 84610, loss = 0.60 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:28.114828: step 84620, loss = 0.61 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:28.435959: step 84630, loss = 0.62 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:28.759515: step 84640, loss = 0.57 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:29.080175: step 84650, loss = 0.49 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:29.400644: step 84660, loss = 0.81 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:29.720813: step 84670, loss = 0.57 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:30.041669: step 84680, loss = 0.55 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:30.363279: step 84690, loss = 0.69 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:30.682444: step 84700, loss = 0.55 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:31.144819: step 84710, loss = 0.50 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:31.468795: step 84720, loss = 0.51 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:31.792647: step 84730, loss = 0.59 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:32.113591: step 84740, loss = 0.58 (7804.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:32.435454: step 84750, loss = 0.59 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:32.755272: step 84760, loss = 0.56 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:33.076918: step 84770, loss = 0.53 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:33.395491: step 84780, loss = 0.65 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:33.716967: step 84790, loss = 0.59 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:34.038414: step 84800, loss = 0.59 (7800.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:34.502599: step 84810, loss = 0.55 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:34.826442: step 84820, loss = 0.50 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:35.148672: step 84830, loss = 0.60 (7833.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:35.470095: step 84840, loss = 0.70 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:35.789346: step 84850, loss = 0.69 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:36.110625: step 84860, loss = 0.73 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:36.432183: step 84870, loss = 0.63 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:36.753506: step 84880, loss = 0.67 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:37.078479: step 84890, loss = 0.61 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:37.401098: step 84900, loss = 0.57 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:37.862395: step 84910, loss = 0.72 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:38.181396: step 84920, loss = 0.71 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:38.501274: step 84930, loss = 0.63 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:38.820554: step 84940, loss = 0.61 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:39.143671: step 84950, loss = 0.58 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:39.464610: step 84960, loss = 0.59 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:39.788752: step 84970, loss = 0.64 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:40.108439: step 84980, loss = 0.60 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:40.432284: step 84990, loss = 0.58 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:40.754737: step 85000, loss = 0.59 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:41.303122: step 85010, loss = 0.65 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:41.622361: step 85020, loss = 0.60 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:41.943736: step 85030, loss = 0.50 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:42.264337: step 85040, loss = 0.53 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:42.585888: step 85050, loss = 0.57 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:42.906632: step 85060, loss = 0.67 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:43.225289: step 85070, loss = 0.58 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:43.543826: step 85080, loss = 0.50 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:43.864487: step 85090, loss = 0.48 (8108.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:44.184832: step 85100, loss = 0.63 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:44.644856: step 85110, loss = 0.52 (8111.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:44.967780: step 85120, loss = 0.81 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:45.287160: step 85130, loss = 0.58 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:45.606487: step 85140, loss = 0.55 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:45.927394: step 85150, loss = 0.55 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:46.249719: step 85160, loss = 0.54 (7970.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:46.571547: step 85170, loss = 0.64 (7771.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:46.891908: step 85180, loss = 0.52 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:47.212144: step 85190, loss = 0.43 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:47.530069: step 85200, loss = 0.59 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:47.981838: step 85210, loss = 0.63 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:48.303692: step 85220, loss = 0.69 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:48.623467: step 85230, loss = 0.49 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:48.943848: step 85240, loss = 0.64 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:49.262936: step 85250, loss = 0.56 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:49.583181: step 85260, loss = 0.64 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:49.906942: step 85270, loss = 0.65 (7622.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:27:50.231904: step 85280, loss = 0.71 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:50.552279: step 85290, loss = 0.68 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:50.871116: step 85300, loss = 0.71 (8080.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:51.328509: step 85310, loss = 0.67 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:51.651887: step 85320, loss = 0.65 (7779.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:51.971588: step 85330, loss = 0.54 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:52.292243: step 85340, loss = 0.56 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:52.611659: step 85350, loss = 0.77 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:52.932831: step 85360, loss = 0.67 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:53.253696: step 85370, loss = 0.59 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:53.575672: step 85380, loss = 0.54 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:53.895875: step 85390, loss = 0.74 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:54.214893: step 85400, loss = 0.63 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:54.670807: step 85410, loss = 0.69 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:54.988964: step 85420, loss = 0.55 (8148.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:55.311216: step 85430, loss = 0.55 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:55.631080: step 85440, loss = 0.61 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:55.950206: step 85450, loss = 0.56 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:56.273490: step 85460, loss = 0.52 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:56.594828: step 85470, loss = 0.55 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:56.914330: step 85480, loss = 0.59 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:57.236709: step 85490, loss = 0.67 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:57.562748: step 85500, loss = 0.54 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:58.025256: step 85510, loss = 0.50 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:58.347168: step 85520, loss = 0.58 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:58.667048: step 85530, loss = 0.66 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:58.987999: step 85540, loss = 0.58 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:59.308702: step 85550, loss = 0.69 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:59.627771: step 85560, loss = 0.57 (8027.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:27:59.950395: step 85570, loss = 0.76 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:00.271459: step 85580, loss = 0.51 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:00.593107: step 85590, loss = 0.60 (8168.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:00.913282: step 85600, loss = 0.59 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:01.375978: step 85610, loss = 0.59 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:01.697448: step 85620, loss = 0.74 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:02.019620: step 85630, loss = 0.68 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:02.342438: step 85640, loss = 0.51 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:02.662753: step 85650, loss = 0.51 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:02.984480: step 85660, loss = 0.50 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:03.302810: step 85670, loss = 0.62 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:03.623578: step 85680, loss = 0.56 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:03.946023: step 85690, loss = 0.63 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:04.267721: step 85700, loss = 0.66 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:04.730090: step 85710, loss = 0.78 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:05.050400: step 85720, loss = 0.69 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:05.368822: step 85730, loss = 0.54 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:05.688139: step 85740, loss = 0.51 (7988.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:06.009477: step 85750, loss = 0.56 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:06.328879: step 85760, loss = 0.55 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:06.649607: step 85770, loss = 0.54 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:06.970438: step 85780, loss = 0.59 (8003.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:07.294508: step 85790, loss = 0.55 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:07.616317: step 85800, loss = 0.58 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:08.088013: step 85810, loss = 0.57 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:08.408732: step 85820, loss = 0.51 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:08.729928: step 85830, loss = 0.71 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:09.051633: step 85840, loss = 0.64 (8106.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:09.373147: step 85850, loss = 0.63 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:09.694454: step 85860, loss = 0.49 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:10.017639: step 85870, loss = 0.52 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:10.337945: step 85880, loss = 0.54 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:10.656179: step 85890, loss = 0.65 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:10.975352: step 85900, loss = 0.41 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:11.437197: step 85910, loss = 0.50 (7826.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:11.758373: step 85920, loss = 0.65 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:12.078529: step 85930, loss = 0.47 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:12.399361: step 85940, loss = 0.49 (7900.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:12.719996: step 85950, loss = 0.58 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:13.039551: step 85960, loss = 0.63 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:13.358596: step 85970, loss = 0.53 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:13.678930: step 85980, loss = 0.53 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:14.000046: step 85990, loss = 0.64 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:14.324705: step 86000, loss = 0.52 (8005.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:14.871353: step 86010, loss = 0.46 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:15.193879: step 86020, loss = 0.48 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:15.514784: step 86030, loss = 0.55 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:15.836153: step 86040, loss = 0.50 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:16.159802: step 86050, loss = 0.45 (7806.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:16.480193: step 86060, loss = 0.44 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:16.802880: step 86070, loss = 0.69 (7786.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:17.123748: step 86080, loss = 0.44 (7839.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:17.444819: step 86090, loss = 0.74 (7889.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:17.765462: step 86100, loss = 0.70 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:18.222502: step 86110, loss = 0.58 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:18.546589: step 86120, loss = 0.60 (7407.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:28:18.866579: step 86130, loss = 0.63 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:19.187830: step 86140, loss = 0.55 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:19.509034: step 86150, loss = 0.57 (7889.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:19.830235: step 86160, loss = 0.64 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:20.149146: step 86170, loss = 0.66 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:20.470182: step 86180, loss = 0.70 (7837.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:20.792058: step 86190, loss = 0.67 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:21.112833: step 86200, loss = 0.48 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:21.568451: step 86210, loss = 0.59 (7762.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:21.889266: step 86220, loss = 0.64 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:22.210197: step 86230, loss = 0.55 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:22.535472: step 86240, loss = 0.40 (8026.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:22.859389: step 86250, loss = 0.60 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:23.179466: step 86260, loss = 0.61 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:23.499010: step 86270, loss = 0.63 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:23.822058: step 86280, loss = 0.46 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:24.144188: step 86290, loss = 0.56 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:24.464340: step 86300, loss = 0.51 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:24.922605: step 86310, loss = 0.60 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:25.245247: step 86320, loss = 0.49 (7622.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:28:25.566245: step 86330, loss = 0.52 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:25.885797: step 86340, loss = 0.61 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:26.205192: step 86350, loss = 0.52 (8152.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:26.526981: step 86360, loss = 0.56 (7715.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:28:26.848816: step 86370, loss = 0.61 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:27.168278: step 86380, loss = 0.46 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:27.487776: step 86390, loss = 0.61 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:27.808841: step 86400, loss = 0.56 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:28.269583: step 86410, loss = 0.54 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:28.590524: step 86420, loss = 0.68 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:28.911772: step 86430, loss = 0.52 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:29.231605: step 86440, loss = 0.55 (8125.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:29.551684: step 86450, loss = 0.58 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:29.873657: step 86460, loss = 0.66 (7667.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:28:30.195265: step 86470, loss = 0.53 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:30.515989: step 86480, loss = 0.78 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:30.836825: step 86490, loss = 0.58 (7949.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:31.158413: step 86500, loss = 0.45 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:31.618148: step 86510, loss = 0.51 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:31.943919: step 86520, loss = 0.63 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:32.267434: step 86530, loss = 0.69 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:32.589284: step 86540, loss = 0.78 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:32.917248: step 86550, loss = 0.52 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:33.238131: step 86560, loss = 0.51 (8205.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:33.558075: step 86570, loss = 0.49 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:33.877852: step 86580, loss = 0.49 (8098.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:34.198701: step 86590, loss = 0.61 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:34.520043: step 86600, loss = 0.65 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:34.966844: step 86610, loss = 0.75 (8165.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:35.289789: step 86620, loss = 0.65 (7773.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:35.608886: step 86630, loss = 0.62 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:35.928327: step 86640, loss = 0.62 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:36.248528: step 86650, loss = 0.43 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:36.575550: step 86660, loss = 0.54 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:36.895996: step 86670, loss = 0.52 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:37.217305: step 86680, loss = 0.55 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:37.538157: step 86690, loss = 0.53 (7801.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:37.859575: step 86700, loss = 0.53 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:38.321912: step 86710, loss = 0.49 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:38.643189: step 86720, loss = 0.54 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:38.963741: step 86730, loss = 0.61 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:39.283353: step 86740, loss = 0.59 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:39.605813: step 86750, loss = 0.49 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:39.928866: step 86760, loss = 0.54 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:40.248377: step 86770, loss = 0.50 (8082.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:40.570331: step 86780, loss = 0.46 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:40.890130: step 86790, loss = 0.64 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:41.210432: step 86800, loss = 0.48 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:41.659911: step 86810, loss = 0.71 (8075.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:41.979086: step 86820, loss = 0.63 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:42.299855: step 86830, loss = 0.68 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:42.621316: step 86840, loss = 0.53 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:42.941615: step 86850, loss = 0.62 (8036.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:43.263680: step 86860, loss = 0.63 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:43.583804: step 86870, loss = 0.61 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:43.906466: step 86880, loss = 0.61 (7790.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:44.227400: step 86890, loss = 0.55 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:44.547426: step 86900, loss = 0.70 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:45.009230: step 86910, loss = 0.51 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:45.339020: step 86920, loss = 0.50 (7806.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:45.659885: step 86930, loss = 0.62 (7767.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:45.982951: step 86940, loss = 0.50 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:46.304829: step 86950, loss = 0.62 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:46.625134: step 86960, loss = 0.61 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:46.945384: step 86970, loss = 0.61 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:47.266946: step 86980, loss = 0.62 (7829.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:47.589708: step 86990, loss = 0.73 (7916.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:47.911174: step 87000, loss = 0.51 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:48.460683: step 87010, loss = 0.46 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:48.781677: step 87020, loss = 0.43 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:49.103433: step 87030, loss = 0.61 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:49.424090: step 87040, loss = 0.42 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:49.747408: step 87050, loss = 0.56 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:50.067332: step 87060, loss = 0.55 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:50.387430: step 87070, loss = 0.55 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:50.707657: step 87080, loss = 0.57 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:51.028381: step 87090, loss = 0.58 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:51.350002: step 87100, loss = 0.56 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:51.807306: step 87110, loss = 0.58 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:52.127818: step 87120, loss = 0.52 (8040.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:52.448485: step 87130, loss = 0.54 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:52.767885: step 87140, loss = 0.55 (7814.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:53.087353: step 87150, loss = 0.66 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:53.408408: step 87160, loss = 0.53 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:53.728855: step 87170, loss = 0.52 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:54.049236: step 87180, loss = 0.55 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:54.369563: step 87190, loss = 0.61 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:54.688532: step 87200, loss = 0.62 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:55.145421: step 87210, loss = 0.57 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:55.464666: step 87220, loss = 0.56 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:55.785646: step 87230, loss = 0.61 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:56.105482: step 87240, loss = 0.57 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:56.425010: step 87250, loss = 0.50 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:56.743646: step 87260, loss = 0.52 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:57.063731: step 87270, loss = 0.46 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:57.384024: step 87280, loss = 0.50 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:57.707506: step 87290, loss = 0.72 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:58.026830: step 87300, loss = 0.65 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:58.476984: step 87310, loss = 0.56 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:58.795599: step 87320, loss = 0.55 (7963.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:59.113839: step 87330, loss = 0.58 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:59.435242: step 87340, loss = 0.61 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:28:59.754357: step 87350, loss = 0.41 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:00.074439: step 87360, loss = 0.71 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:00.394292: step 87370, loss = 0.65 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:00.716887: step 87380, loss = 0.52 (7825.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:01.036829: step 87390, loss = 0.59 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:01.356796: step 87400, loss = 0.64 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:01.814502: step 87410, loss = 0.75 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:02.135235: step 87420, loss = 0.48 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:02.454363: step 87430, loss = 0.57 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:02.773909: step 87440, loss = 0.65 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:03.095172: step 87450, loss = 0.80 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:03.417168: step 87460, loss = 0.49 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:03.740455: step 87470, loss = 0.49 (7570.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:29:04.065983: step 87480, loss = 0.56 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:04.384824: step 87490, loss = 0.58 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:04.704876: step 87500, loss = 0.58 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:05.167101: step 87510, loss = 0.43 (7775.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:05.487622: step 87520, loss = 0.62 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:05.808723: step 87530, loss = 0.56 (7703.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:29:06.129231: step 87540, loss = 0.63 (7807.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:06.448775: step 87550, loss = 0.53 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:06.771632: step 87560, loss = 0.61 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:07.091585: step 87570, loss = 0.58 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:07.413640: step 87580, loss = 0.58 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:07.732528: step 87590, loss = 0.53 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:08.052555: step 87600, loss = 0.60 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:08.511881: step 87610, loss = 0.52 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:08.832945: step 87620, loss = 0.64 (7821.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:09.155620: step 87630, loss = 0.64 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:09.476317: step 87640, loss = 0.54 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:09.795495: step 87650, loss = 0.66 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:10.117707: step 87660, loss = 0.64 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:10.437729: step 87670, loss = 0.54 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:10.760968: step 87680, loss = 0.51 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:11.081724: step 87690, loss = 0.57 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:11.402028: step 87700, loss = 0.53 (7887.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:11.853715: step 87710, loss = 0.63 (7798.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:12.174700: step 87720, loss = 0.64 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:12.494240: step 87730, loss = 0.63 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:12.815751: step 87740, loss = 0.62 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:13.136735: step 87750, loss = 0.57 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:13.459627: step 87760, loss = 0.62 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:13.781845: step 87770, loss = 0.60 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:14.102303: step 87780, loss = 0.56 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:14.424224: step 87790, loss = 0.48 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:14.745691: step 87800, loss = 0.45 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:15.202920: step 87810, loss = 0.57 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:15.523054: step 87820, loss = 0.50 (7892.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:15.847530: step 87830, loss = 0.52 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:16.167976: step 87840, loss = 0.58 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:16.489235: step 87850, loss = 0.52 (7996.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:16.809268: step 87860, loss = 0.66 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:17.129906: step 87870, loss = 0.56 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:17.449480: step 87880, loss = 0.68 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:17.770832: step 87890, loss = 0.54 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:18.089877: step 87900, loss = 0.56 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:18.551900: step 87910, loss = 0.57 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:18.874171: step 87920, loss = 0.63 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:19.193288: step 87930, loss = 0.52 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:19.514124: step 87940, loss = 0.51 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:19.834454: step 87950, loss = 0.66 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:20.155144: step 87960, loss = 0.58 (7975.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:20.477479: step 87970, loss = 0.74 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:20.798124: step 87980, loss = 0.55 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:21.119065: step 87990, loss = 0.60 (7891.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:21.438641: step 88000, loss = 0.58 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:21.998225: step 88010, loss = 0.54 (7980.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:22.317225: step 88020, loss = 0.62 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:22.637918: step 88030, loss = 0.56 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:22.957463: step 88040, loss = 0.50 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:23.278070: step 88050, loss = 0.64 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:23.599198: step 88060, loss = 0.54 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:23.921126: step 88070, loss = 0.54 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:24.241995: step 88080, loss = 0.60 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:24.561895: step 88090, loss = 0.69 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:24.881391: step 88100, loss = 0.70 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:25.334644: step 88110, loss = 0.57 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:25.656960: step 88120, loss = 0.61 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:25.978392: step 88130, loss = 0.60 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:26.298461: step 88140, loss = 0.56 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:26.618151: step 88150, loss = 0.49 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:26.939204: step 88160, loss = 0.52 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:27.260855: step 88170, loss = 0.60 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:27.581504: step 88180, loss = 0.55 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:27.900079: step 88190, loss = 0.48 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:28.219638: step 88200, loss = 0.52 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:28.680473: step 88210, loss = 0.60 (8098.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:29.002706: step 88220, loss = 0.53 (7841.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:29.323822: step 88230, loss = 0.66 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:29.644988: step 88240, loss = 0.51 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:29.965246: step 88250, loss = 0.64 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:30.285438: step 88260, loss = 0.75 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:30.605767: step 88270, loss = 0.55 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:30.929154: step 88280, loss = 0.49 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:31.251084: step 88290, loss = 0.64 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:31.570130: step 88300, loss = 0.58 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:32.031740: step 88310, loss = 0.57 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:32.351332: step 88320, loss = 0.54 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:32.672120: step 88330, loss = 0.60 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:32.995677: step 88340, loss = 0.61 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:33.319362: step 88350, loss = 0.58 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:33.639271: step 88360, loss = 0.66 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:33.959649: step 88370, loss = 0.62 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:34.279831: step 88380, loss = 0.52 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:34.603418: step 88390, loss = 0.59 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:34.925426: step 88400, loss = 0.66 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:35.386267: step 88410, loss = 0.60 (7764.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:35.706294: step 88420, loss = 0.55 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:36.026852: step 88430, loss = 0.65 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:36.346425: step 88440, loss = 0.51 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:36.668271: step 88450, loss = 0.70 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:36.987427: step 88460, loss = 0.54 (7953.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:37.308508: step 88470, loss = 0.55 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:37.627675: step 88480, loss = 0.60 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:37.948594: step 88490, loss = 0.55 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:38.269312: step 88500, loss = 0.61 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:38.731568: step 88510, loss = 0.51 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:39.050341: step 88520, loss = 0.69 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:39.369706: step 88530, loss = 0.64 (7894.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:39.689673: step 88540, loss = 0.60 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:40.011431: step 88550, loss = 0.64 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:40.331489: step 88560, loss = 0.60 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:40.653005: step 88570, loss = 0.68 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:40.971874: step 88580, loss = 0.58 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:41.294975: step 88590, loss = 0.74 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:41.616596: step 88600, loss = 0.60 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:42.074261: step 88610, loss = 0.61 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:42.396463: step 88620, loss = 0.59 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:42.716907: step 88630, loss = 0.55 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:43.037849: step 88640, loss = 0.59 (7860.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:43.359161: step 88650, loss = 0.57 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:43.680990: step 88660, loss = 0.63 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:44.003925: step 88670, loss = 0.56 (8006.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:44.326514: step 88680, loss = 0.60 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:44.648203: step 88690, loss = 0.69 (7887.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:44.968135: step 88700, loss = 0.59 (7815.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:45.423448: step 88710, loss = 0.60 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:45.747800: step 88720, loss = 0.63 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:46.067223: step 88730, loss = 0.46 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:46.388127: step 88740, loss = 0.64 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:46.709976: step 88750, loss = 0.49 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:47.032529: step 88760, loss = 0.66 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:47.351320: step 88770, loss = 0.44 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:47.670966: step 88780, loss = 0.63 (7838.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:47.989695: step 88790, loss = 0.66 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:48.310066: step 88800, loss = 0.56 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:48.767978: step 88810, loss = 0.56 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:49.089406: step 88820, loss = 0.58 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:49.410098: step 88830, loss = 0.64 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:49.730968: step 88840, loss = 0.58 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:50.052464: step 88850, loss = 0.58 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:50.372765: step 88860, loss = 0.60 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:50.693331: step 88870, loss = 0.58 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:51.017411: step 88880, loss = 0.55 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:51.338977: step 88890, loss = 0.60 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:51.662677: step 88900, loss = 0.58 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:52.120640: step 88910, loss = 0.52 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:52.440897: step 88920, loss = 0.40 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:52.763875: step 88930, loss = 0.56 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:53.085212: step 88940, loss = 0.56 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:53.406815: step 88950, loss = 0.55 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:53.728444: step 88960, loss = 0.56 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:54.051676: step 88970, loss = 0.60 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:54.371671: step 88980, loss = 0.59 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:54.692205: step 88990, loss = 0.66 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:55.014254: step 89000, loss = 0.50 (7745.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:29:55.624250: step 89010, loss = 0.52 (7337.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:29:55.947442: step 89020, loss = 0.46 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:56.271068: step 89030, loss = 0.61 (8035.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:56.592695: step 89040, loss = 0.55 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:56.914477: step 89050, loss = 0.56 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:57.235307: step 89060, loss = 0.53 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:57.555268: step 89070, loss = 0.61 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:57.877528: step 89080, loss = 0.57 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:58.198317: step 89090, loss = 0.57 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:58.523544: step 89100, loss = 0.64 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:58.972682: step 89110, loss = 0.58 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:59.296723: step 89120, loss = 0.67 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:59.617178: step 89130, loss = 0.49 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:29:59.938725: step 89140, loss = 0.64 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:00.260400: step 89150, loss = 0.57 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:00.584329: step 89160, loss = 0.62 (7768.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:00.905803: step 89170, loss = 0.61 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:01.226006: step 89180, loss = 0.46 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:01.546302: step 89190, loss = 0.63 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:01.867978: step 89200, loss = 0.58 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:02.326083: step 89210, loss = 0.61 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:02.646214: step 89220, loss = 0.53 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:02.966610: step 89230, loss = 0.67 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:03.288040: step 89240, loss = 0.63 (8145.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:03.610974: step 89250, loss = 0.51 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:03.932650: step 89260, loss = 0.49 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:04.257933: step 89270, loss = 0.58 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:04.577683: step 89280, loss = 0.57 (7884.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:04.899472: step 89290, loss = 0.58 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:05.222480: step 89300, loss = 0.60 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:05.690389: step 89310, loss = 0.68 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:06.010775: step 89320, loss = 0.61 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:06.330563: step 89330, loss = 0.53 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:06.652615: step 89340, loss = 0.56 (7888.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:06.975646: step 89350, loss = 0.52 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:07.298650: step 89360, loss = 0.53 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:07.620620: step 89370, loss = 0.55 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:07.940180: step 89380, loss = 0.62 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:08.263030: step 89390, loss = 0.50 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:08.585672: step 89400, loss = 0.64 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:09.048812: step 89410, loss = 0.67 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:09.371098: step 89420, loss = 0.49 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:09.693100: step 89430, loss = 0.60 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:10.016862: step 89440, loss = 0.61 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:10.338932: step 89450, loss = 0.68 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:10.659895: step 89460, loss = 0.63 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:10.979737: step 89470, loss = 0.60 (8062.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:11.299751: step 89480, loss = 0.52 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:11.621324: step 89490, loss = 0.56 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:11.942258: step 89500, loss = 0.66 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:12.391572: step 89510, loss = 0.60 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:12.716143: step 89520, loss = 0.56 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:13.035681: step 89530, loss = 0.54 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:13.357959: step 89540, loss = 0.61 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:13.677557: step 89550, loss = 0.54 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:13.998072: step 89560, loss = 0.77 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:14.318801: step 89570, loss = 0.61 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:14.642141: step 89580, loss = 0.55 (7521.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:14.962427: step 89590, loss = 0.45 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:15.284714: step 89600, loss = 0.60 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:15.750094: step 89610, loss = 0.54 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:16.070111: step 89620, loss = 0.62 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:16.389821: step 89630, loss = 0.61 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:16.709634: step 89640, loss = 0.56 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:17.031664: step 89650, loss = 0.55 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:17.353901: step 89660, loss = 0.56 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:17.672887: step 89670, loss = 0.51 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:17.992169: step 89680, loss = 0.77 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:18.315546: step 89690, loss = 0.55 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:18.639607: step 89700, loss = 0.40 (7540.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:19.101083: step 89710, loss = 0.57 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:19.421882: step 89720, loss = 0.69 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:19.740456: step 89730, loss = 0.59 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:20.059224: step 89740, loss = 0.64 (7940.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:20.379990: step 89750, loss = 0.48 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:20.703073: step 89760, loss = 0.54 (7649.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:21.026559: step 89770, loss = 0.56 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:21.346340: step 89780, loss = 0.56 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:21.667134: step 89790, loss = 0.72 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:21.987192: step 89800, loss = 0.63 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:22.452365: step 89810, loss = 0.60 (7749.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:22.773845: step 89820, loss = 0.71 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:23.093177: step 89830, loss = 0.57 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:23.411145: step 89840, loss = 0.66 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:23.731401: step 89850, loss = 0.61 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:24.051328: step 89860, loss = 0.63 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:24.371559: step 89870, loss = 0.58 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:24.693462: step 89880, loss = 0.53 (7784.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:25.017391: step 89890, loss = 0.52 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:25.340932: step 89900, loss = 0.62 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:25.809940: step 89910, loss = 0.55 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:26.132411: step 89920, loss = 0.54 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:26.452413: step 89930, loss = 0.66 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:26.771621: step 89940, loss = 0.48 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:27.093787: step 89950, loss = 0.59 (7818.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:27.414967: step 89960, loss = 0.66 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:27.735396: step 89970, loss = 0.68 (8136.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:28.055773: step 89980, loss = 0.56 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:28.380668: step 89990, loss = 0.46 (7478.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:28.700978: step 90000, loss = 0.74 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:29.255508: step 90010, loss = 0.58 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:29.576331: step 90020, loss = 0.54 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:29.895080: step 90030, loss = 0.59 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:30.217823: step 90040, loss = 0.44 (7552.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:30.539061: step 90050, loss = 0.53 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:30.860815: step 90060, loss = 0.49 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:31.179823: step 90070, loss = 0.57 (8150.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:31.499746: step 90080, loss = 0.52 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:31.821865: step 90090, loss = 0.71 (7859.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:32.144182: step 90100, loss = 0.53 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:32.590127: step 90110, loss = 0.58 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:32.909128: step 90120, loss = 0.70 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:33.233395: step 90130, loss = 0.57 (7390.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:33.553736: step 90140, loss = 0.57 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:33.873996: step 90150, loss = 0.48 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:34.195864: step 90160, loss = 0.60 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:34.517058: step 90170, loss = 0.64 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:34.837249: step 90180, loss = 0.51 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:35.155793: step 90190, loss = 0.69 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:35.475769: step 90200, loss = 0.60 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:35.928782: step 90210, loss = 0.60 (8096.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:36.249719: step 90220, loss = 0.51 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:36.569273: step 90230, loss = 0.52 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:36.889232: step 90240, loss = 0.45 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:37.210277: step 90250, loss = 0.63 (7737.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:37.530266: step 90260, loss = 0.71 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:37.861717: step 90270, loss = 0.66 (6064.2 examples/sec; 0.021 sec/batch)
2017-09-16 16:30:38.181246: step 90280, loss = 0.52 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:38.501417: step 90290, loss = 0.60 (8140.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:38.820480: step 90300, loss = 0.57 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:39.266460: step 90310, loss = 0.61 (8127.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:39.587397: step 90320, loss = 0.58 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:39.909210: step 90330, loss = 0.58 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:40.230304: step 90340, loss = 0.57 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:40.554630: step 90350, loss = 0.64 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:40.877048: step 90360, loss = 0.69 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:41.197191: step 90370, loss = 0.55 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:41.519660: step 90380, loss = 0.47 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:41.839536: step 90390, loss = 0.73 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:42.161736: step 90400, loss = 0.73 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:42.607761: step 90410, loss = 0.57 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:42.931028: step 90420, loss = 0.66 (7574.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:30:43.252752: step 90430, loss = 0.47 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:43.572406: step 90440, loss = 0.53 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:43.894727: step 90450, loss = 0.63 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:44.215013: step 90460, loss = 0.46 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:44.533309: step 90470, loss = 0.71 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:44.855382: step 90480, loss = 0.57 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:45.175837: step 90490, loss = 0.58 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:45.494983: step 90500, loss = 0.66 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:45.953743: step 90510, loss = 0.71 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:46.275833: step 90520, loss = 0.62 (7793.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:46.596215: step 90530, loss = 0.54 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:46.917705: step 90540, loss = 0.61 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:47.235820: step 90550, loss = 0.56 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:47.554484: step 90560, loss = 0.67 (8102.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:47.874709: step 90570, loss = 0.67 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:48.195345: step 90580, loss = 0.53 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:48.515904: step 90590, loss = 0.66 (7822.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:48.836695: step 90600, loss = 0.50 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:49.292386: step 90610, loss = 0.46 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:49.614728: step 90620, loss = 0.55 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:49.938248: step 90630, loss = 0.62 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:50.260978: step 90640, loss = 0.53 (7799.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:50.584338: step 90650, loss = 0.73 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:50.905446: step 90660, loss = 0.58 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:51.224809: step 90670, loss = 0.46 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:51.545126: step 90680, loss = 0.55 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:51.865165: step 90690, loss = 0.56 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:52.187868: step 90700, loss = 0.63 (7909.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:52.649999: step 90710, loss = 0.61 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:52.970572: step 90720, loss = 0.57 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:53.293088: step 90730, loss = 0.67 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:53.614215: step 90740, loss = 0.49 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:53.934646: step 90750, loss = 0.62 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:54.257361: step 90760, loss = 0.59 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:54.577634: step 90770, loss = 0.55 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:54.903196: step 90780, loss = 0.57 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:55.222384: step 90790, loss = 0.59 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:55.542969: step 90800, loss = 0.54 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:56.001879: step 90810, loss = 0.56 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:56.321531: step 90820, loss = 0.55 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:56.644377: step 90830, loss = 0.70 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:56.964480: step 90840, loss = 0.70 (7859.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:57.285352: step 90850, loss = 0.59 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:57.603722: step 90860, loss = 0.54 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:57.923182: step 90870, loss = 0.61 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:58.243989: step 90880, loss = 0.57 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:58.565108: step 90890, loss = 0.57 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:58.885548: step 90900, loss = 0.57 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:59.348875: step 90910, loss = 0.58 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:59.669858: step 90920, loss = 0.51 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:30:59.990321: step 90930, loss = 0.52 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:00.311151: step 90940, loss = 0.54 (7747.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:00.633916: step 90950, loss = 0.66 (7967.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:00.956715: step 90960, loss = 0.55 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:01.279543: step 90970, loss = 0.70 (7253.6 examples/sec; 0.018 sec/batch)
2017-09-16 16:31:01.605906: step 90980, loss = 0.56 (7740.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:01.929592: step 90990, loss = 0.64 (7895.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:02.250883: step 91000, loss = 0.46 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:02.788716: step 91010, loss = 0.55 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:03.109313: step 91020, loss = 0.58 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:03.431471: step 91030, loss = 0.63 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:03.752597: step 91040, loss = 0.51 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:04.077463: step 91050, loss = 0.61 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:04.399128: step 91060, loss = 0.66 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:04.721741: step 91070, loss = 0.51 (7707.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:05.042991: step 91080, loss = 0.53 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:05.363612: step 91090, loss = 0.60 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:05.685371: step 91100, loss = 0.45 (7969.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:06.143621: step 91110, loss = 0.44 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:06.464463: step 91120, loss = 0.64 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:06.792345: step 91130, loss = 0.52 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:07.115725: step 91140, loss = 0.59 (7653.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:07.436598: step 91150, loss = 0.63 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:07.757978: step 91160, loss = 0.48 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:08.080283: step 91170, loss = 0.74 (8082.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:08.401064: step 91180, loss = 0.72 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:08.723754: step 91190, loss = 0.54 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:09.042288: step 91200, loss = 0.57 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:09.511302: step 91210, loss = 0.52 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:09.835189: step 91220, loss = 0.57 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:10.155793: step 91230, loss = 0.48 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:10.475455: step 91240, loss = 0.56 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:10.794119: step 91250, loss = 0.61 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:11.113426: step 91260, loss = 0.51 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:11.435923: step 91270, loss = 0.51 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:11.755658: step 91280, loss = 0.73 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:12.076512: step 91290, loss = 0.60 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:12.395995: step 91300, loss = 0.56 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:12.851874: step 91310, loss = 0.58 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:13.172057: step 91320, loss = 0.51 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:13.491536: step 91330, loss = 0.56 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:13.812400: step 91340, loss = 0.71 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:14.131023: step 91350, loss = 0.65 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:14.451787: step 91360, loss = 0.41 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:14.771475: step 91370, loss = 0.65 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:15.091367: step 91380, loss = 0.70 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:15.411862: step 91390, loss = 0.53 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:15.731653: step 91400, loss = 0.56 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:16.189914: step 91410, loss = 0.55 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:16.509486: step 91420, loss = 0.53 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:16.827414: step 91430, loss = 0.58 (8130.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:17.147757: step 91440, loss = 0.67 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:17.468313: step 91450, loss = 0.48 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:17.793370: step 91460, loss = 0.61 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:18.114567: step 91470, loss = 0.59 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:18.435646: step 91480, loss = 0.75 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:18.756585: step 91490, loss = 0.61 (7910.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:19.075225: step 91500, loss = 0.74 (8170.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:19.527963: step 91510, loss = 0.61 (7859.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:19.847822: step 91520, loss = 0.68 (8003.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:20.172834: step 91530, loss = 0.54 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:20.492316: step 91540, loss = 0.52 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:20.814333: step 91550, loss = 0.64 (7842.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:21.134350: step 91560, loss = 0.57 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:21.456456: step 91570, loss = 0.51 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:21.776229: step 91580, loss = 0.55 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:22.096428: step 91590, loss = 0.70 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:22.416310: step 91600, loss = 0.48 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:22.875659: step 91610, loss = 0.55 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:23.197523: step 91620, loss = 0.64 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:23.520184: step 91630, loss = 0.70 (7648.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:23.842263: step 91640, loss = 0.55 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:24.163104: step 91650, loss = 0.66 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:24.484863: step 91660, loss = 0.53 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:24.804835: step 91670, loss = 0.62 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:25.125156: step 91680, loss = 0.71 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:25.444956: step 91690, loss = 0.49 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:25.763360: step 91700, loss = 0.53 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:26.227145: step 91710, loss = 0.63 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:26.545576: step 91720, loss = 0.47 (8189.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:26.865765: step 91730, loss = 0.48 (7822.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:27.186696: step 91740, loss = 0.67 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:27.507632: step 91750, loss = 0.53 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:27.835131: step 91760, loss = 0.55 (6922.7 examples/sec; 0.018 sec/batch)
2017-09-16 16:31:28.154744: step 91770, loss = 0.57 (8041.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:28.474663: step 91780, loss = 0.56 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:28.794054: step 91790, loss = 0.59 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:29.115313: step 91800, loss = 0.52 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:29.568105: step 91810, loss = 0.74 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:29.888444: step 91820, loss = 0.55 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:30.207548: step 91830, loss = 0.54 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:30.528937: step 91840, loss = 0.64 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:30.848427: step 91850, loss = 0.52 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:31.170388: step 91860, loss = 0.61 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:31.489965: step 91870, loss = 0.52 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:31.809894: step 91880, loss = 0.50 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:32.131384: step 91890, loss = 0.61 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:32.455272: step 91900, loss = 0.63 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:32.914858: step 91910, loss = 0.67 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:33.237215: step 91920, loss = 0.56 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:33.557363: step 91930, loss = 0.80 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:33.877807: step 91940, loss = 0.66 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:34.197529: step 91950, loss = 0.69 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:34.516828: step 91960, loss = 0.62 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:34.836633: step 91970, loss = 0.54 (7824.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:35.156128: step 91980, loss = 0.63 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:35.477242: step 91990, loss = 0.55 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:35.798185: step 92000, loss = 0.54 (7511.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:36.359758: step 92010, loss = 0.69 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:36.679766: step 92020, loss = 0.58 (8137.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:36.999312: step 92030, loss = 0.56 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:37.319920: step 92040, loss = 0.49 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:37.643866: step 92050, loss = 0.51 (7314.1 examples/sec; 0.018 sec/batch)
2017-09-16 16:31:37.970409: step 92060, loss = 0.64 (7823.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:38.292203: step 92070, loss = 0.52 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:38.613251: step 92080, loss = 0.55 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:38.933488: step 92090, loss = 0.61 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:39.253803: step 92100, loss = 0.54 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:39.707791: step 92110, loss = 0.49 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:40.027924: step 92120, loss = 0.74 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:40.348552: step 92130, loss = 0.52 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:40.670509: step 92140, loss = 0.69 (7488.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:31:40.992358: step 92150, loss = 0.55 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:41.314884: step 92160, loss = 0.65 (7833.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:41.636537: step 92170, loss = 0.52 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:41.956130: step 92180, loss = 0.51 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:42.276147: step 92190, loss = 0.60 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:42.596866: step 92200, loss = 0.56 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:43.045029: step 92210, loss = 0.57 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:43.364241: step 92220, loss = 0.54 (7985.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:43.683879: step 92230, loss = 0.55 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:44.006944: step 92240, loss = 0.67 (7839.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:44.327368: step 92250, loss = 0.50 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:44.645845: step 92260, loss = 0.75 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:44.964967: step 92270, loss = 0.56 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:45.282587: step 92280, loss = 0.58 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:45.603230: step 92290, loss = 0.53 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:45.923917: step 92300, loss = 0.63 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:46.376539: step 92310, loss = 0.52 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:46.696173: step 92320, loss = 0.64 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:47.017382: step 92330, loss = 0.55 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:47.336653: step 92340, loss = 0.52 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:47.657697: step 92350, loss = 0.62 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:47.979792: step 92360, loss = 0.55 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:48.299281: step 92370, loss = 0.49 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:48.619538: step 92380, loss = 0.51 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:48.940435: step 92390, loss = 0.57 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:49.259241: step 92400, loss = 0.62 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:49.710646: step 92410, loss = 0.53 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:50.030668: step 92420, loss = 0.62 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:50.351616: step 92430, loss = 0.54 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:50.671952: step 92440, loss = 0.72 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:50.993345: step 92450, loss = 0.77 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:51.313247: step 92460, loss = 0.63 (8040.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:51.632934: step 92470, loss = 0.63 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:51.954232: step 92480, loss = 0.54 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:52.274011: step 92490, loss = 0.51 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:52.596405: step 92500, loss = 0.53 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:53.066132: step 92510, loss = 0.59 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:53.387909: step 92520, loss = 0.60 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:53.708021: step 92530, loss = 0.58 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:54.028846: step 92540, loss = 0.69 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:54.349423: step 92550, loss = 0.46 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:54.669140: step 92560, loss = 0.59 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:54.989023: step 92570, loss = 0.46 (8087.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:55.313111: step 92580, loss = 0.66 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:55.633465: step 92590, loss = 0.55 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:55.952736: step 92600, loss = 0.45 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:56.404403: step 92610, loss = 0.54 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:56.725564: step 92620, loss = 0.46 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:57.046708: step 92630, loss = 0.66 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:57.366745: step 92640, loss = 0.57 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:57.687157: step 92650, loss = 0.71 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:58.007185: step 92660, loss = 0.47 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:58.328348: step 92670, loss = 0.59 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:58.650502: step 92680, loss = 0.42 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:58.972463: step 92690, loss = 0.50 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:59.294546: step 92700, loss = 0.59 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:31:59.756756: step 92710, loss = 0.64 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:00.078278: step 92720, loss = 0.50 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:00.400655: step 92730, loss = 0.63 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:00.719592: step 92740, loss = 0.57 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:01.041592: step 92750, loss = 0.72 (7614.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:32:01.364167: step 92760, loss = 0.61 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:01.686648: step 92770, loss = 0.72 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:02.007199: step 92780, loss = 0.62 (7793.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:02.327576: step 92790, loss = 0.61 (8059.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:02.647801: step 92800, loss = 0.45 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:03.108011: step 92810, loss = 0.51 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:03.427139: step 92820, loss = 0.52 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:03.746724: step 92830, loss = 0.52 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:04.071252: step 92840, loss = 0.58 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:04.393131: step 92850, loss = 0.57 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:04.718903: step 92860, loss = 0.70 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:05.038392: step 92870, loss = 0.56 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:05.357283: step 92880, loss = 0.42 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:05.679390: step 92890, loss = 0.55 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:05.999668: step 92900, loss = 0.48 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:06.456659: step 92910, loss = 0.57 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:06.776208: step 92920, loss = 0.68 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:07.097171: step 92930, loss = 0.52 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:07.418470: step 92940, loss = 0.53 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:07.738636: step 92950, loss = 0.60 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:08.065419: step 92960, loss = 0.63 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:08.385750: step 92970, loss = 0.49 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:08.709325: step 92980, loss = 0.44 (7780.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:09.027834: step 92990, loss = 0.65 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:09.348922: step 93000, loss = 0.50 (7530.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:32:09.902165: step 93010, loss = 0.60 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:10.225383: step 93020, loss = 0.54 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:10.547123: step 93030, loss = 0.47 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:10.870091: step 93040, loss = 0.57 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:11.189975: step 93050, loss = 0.58 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:11.510731: step 93060, loss = 0.52 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:11.830626: step 93070, loss = 0.56 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:12.150210: step 93080, loss = 0.45 (8130.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:12.469673: step 93090, loss = 0.56 (7897.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:12.788281: step 93100, loss = 0.60 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:13.247902: step 93110, loss = 0.60 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:13.567051: step 93120, loss = 0.49 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:13.887472: step 93130, loss = 0.66 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:14.206340: step 93140, loss = 0.56 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:14.527955: step 93150, loss = 0.49 (7648.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:32:14.849007: step 93160, loss = 0.63 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:15.172908: step 93170, loss = 0.62 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:15.494014: step 93180, loss = 0.63 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:15.814170: step 93190, loss = 0.61 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:16.134743: step 93200, loss = 0.63 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:16.594808: step 93210, loss = 0.61 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:16.916468: step 93220, loss = 0.57 (7936.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:17.239789: step 93230, loss = 0.54 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:17.559914: step 93240, loss = 0.77 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:17.879469: step 93250, loss = 0.67 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:18.200029: step 93260, loss = 0.70 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:18.518200: step 93270, loss = 0.52 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:18.837616: step 93280, loss = 0.66 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:19.157564: step 93290, loss = 0.60 (8116.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:19.479555: step 93300, loss = 0.54 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:19.940774: step 93310, loss = 0.54 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:20.262208: step 93320, loss = 0.61 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:20.585538: step 93330, loss = 0.52 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:20.906662: step 93340, loss = 0.42 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:21.224731: step 93350, loss = 0.48 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:21.544105: step 93360, loss = 0.50 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:21.865852: step 93370, loss = 0.61 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:22.185739: step 93380, loss = 0.54 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:22.507012: step 93390, loss = 0.61 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:22.828669: step 93400, loss = 0.57 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:23.276847: step 93410, loss = 0.46 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:23.596274: step 93420, loss = 0.62 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:23.918858: step 93430, loss = 0.43 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:24.239905: step 93440, loss = 0.61 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:24.558523: step 93450, loss = 0.62 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:24.881423: step 93460, loss = 0.66 (7806.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:25.203029: step 93470, loss = 0.58 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:25.522004: step 93480, loss = 0.58 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:25.840757: step 93490, loss = 0.55 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:26.159247: step 93500, loss = 0.59 (7938.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:26.616665: step 93510, loss = 0.55 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:26.935292: step 93520, loss = 0.48 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:27.255692: step 93530, loss = 0.63 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:27.575159: step 93540, loss = 0.68 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:27.895036: step 93550, loss = 0.58 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:28.213892: step 93560, loss = 0.50 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:28.532968: step 93570, loss = 0.52 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:28.854723: step 93580, loss = 0.51 (8107.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:29.175720: step 93590, loss = 0.52 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:29.495832: step 93600, loss = 0.60 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:29.955988: step 93610, loss = 0.51 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:30.279131: step 93620, loss = 0.45 (7734.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:32:30.599372: step 93630, loss = 0.46 (8163.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:30.919523: step 93640, loss = 0.49 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:31.240140: step 93650, loss = 0.71 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:31.562708: step 93660, loss = 0.59 (8163.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:31.883864: step 93670, loss = 0.70 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:32.204010: step 93680, loss = 0.66 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:32.522332: step 93690, loss = 0.50 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:32.842267: step 93700, loss = 0.61 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:33.293714: step 93710, loss = 0.48 (7819.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:33.618094: step 93720, loss = 0.58 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:33.940170: step 93730, loss = 0.54 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:34.260071: step 93740, loss = 0.74 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:34.580624: step 93750, loss = 0.57 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:34.901209: step 93760, loss = 0.51 (7803.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:35.220021: step 93770, loss = 0.68 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:35.539374: step 93780, loss = 0.54 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:35.858482: step 93790, loss = 0.55 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:36.178434: step 93800, loss = 0.64 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:36.641823: step 93810, loss = 0.56 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:36.964645: step 93820, loss = 0.65 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:37.283767: step 93830, loss = 0.57 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:37.603222: step 93840, loss = 0.56 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:37.925018: step 93850, loss = 0.52 (7823.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:38.245311: step 93860, loss = 0.71 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:38.565219: step 93870, loss = 0.50 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:38.884208: step 93880, loss = 0.56 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:39.205629: step 93890, loss = 0.50 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:39.527340: step 93900, loss = 0.67 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:39.985335: step 93910, loss = 0.53 (7887.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:40.305020: step 93920, loss = 0.58 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:40.626115: step 93930, loss = 0.64 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:40.947516: step 93940, loss = 0.50 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:41.269244: step 93950, loss = 0.59 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:41.590786: step 93960, loss = 0.53 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:41.911339: step 93970, loss = 0.44 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:42.230416: step 93980, loss = 0.71 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:42.551524: step 93990, loss = 0.59 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:42.873949: step 94000, loss = 0.60 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:43.441268: step 94010, loss = 0.55 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:43.759576: step 94020, loss = 0.52 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:44.081757: step 94030, loss = 0.63 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:44.402913: step 94040, loss = 0.57 (8011.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:44.723787: step 94050, loss = 0.57 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:45.042133: step 94060, loss = 0.56 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:45.363504: step 94070, loss = 0.67 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:45.684055: step 94080, loss = 0.61 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:46.005378: step 94090, loss = 0.52 (7992.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:46.325876: step 94100, loss = 0.47 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:46.784668: step 94110, loss = 0.51 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:47.104628: step 94120, loss = 0.54 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:47.428449: step 94130, loss = 0.66 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:47.747790: step 94140, loss = 0.59 (8102.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:48.069785: step 94150, loss = 0.55 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:48.390920: step 94160, loss = 0.56 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:48.709981: step 94170, loss = 0.66 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:49.035316: step 94180, loss = 0.53 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:49.358028: step 94190, loss = 0.57 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:49.681889: step 94200, loss = 0.53 (7767.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:50.143247: step 94210, loss = 0.48 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:50.465860: step 94220, loss = 0.70 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:50.789659: step 94230, loss = 0.59 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:51.110727: step 94240, loss = 0.69 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:51.433110: step 94250, loss = 0.57 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:51.753046: step 94260, loss = 0.60 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:52.071868: step 94270, loss = 0.41 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:52.391173: step 94280, loss = 0.64 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:52.715302: step 94290, loss = 0.60 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:53.035751: step 94300, loss = 0.59 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:53.495557: step 94310, loss = 0.55 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:53.816139: step 94320, loss = 0.56 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:54.137205: step 94330, loss = 0.68 (8149.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:54.460001: step 94340, loss = 0.44 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:54.783137: step 94350, loss = 0.55 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:55.100715: step 94360, loss = 0.57 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:55.420596: step 94370, loss = 0.58 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:55.742624: step 94380, loss = 0.77 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:56.064960: step 94390, loss = 0.72 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:56.386519: step 94400, loss = 0.57 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:56.851336: step 94410, loss = 0.54 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:57.173931: step 94420, loss = 0.70 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:57.494695: step 94430, loss = 0.73 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:57.819151: step 94440, loss = 0.48 (7619.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:32:58.140709: step 94450, loss = 0.55 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:58.464485: step 94460, loss = 0.56 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:58.786377: step 94470, loss = 0.43 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:59.106679: step 94480, loss = 0.68 (7957.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:59.426215: step 94490, loss = 0.59 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:32:59.746625: step 94500, loss = 0.54 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:00.206281: step 94510, loss = 0.52 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:00.527695: step 94520, loss = 0.55 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:00.849236: step 94530, loss = 0.50 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:01.168848: step 94540, loss = 0.49 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:01.489682: step 94550, loss = 0.59 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:01.810661: step 94560, loss = 0.63 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:02.131227: step 94570, loss = 0.64 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:02.451535: step 94580, loss = 0.59 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:02.772834: step 94590, loss = 0.53 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:03.092262: step 94600, loss = 0.49 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:03.540758: step 94610, loss = 0.63 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:03.861530: step 94620, loss = 0.49 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:04.182766: step 94630, loss = 0.71 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:04.503909: step 94640, loss = 0.52 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:04.824064: step 94650, loss = 0.67 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:05.142928: step 94660, loss = 0.69 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:05.463247: step 94670, loss = 0.60 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:05.784374: step 94680, loss = 0.57 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:06.104905: step 94690, loss = 0.54 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:06.424564: step 94700, loss = 0.52 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:06.883749: step 94710, loss = 0.57 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:07.205130: step 94720, loss = 0.81 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:07.527390: step 94730, loss = 0.55 (8098.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:07.849403: step 94740, loss = 0.69 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:08.173193: step 94750, loss = 0.54 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:08.493364: step 94760, loss = 0.68 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:08.814234: step 94770, loss = 0.42 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:09.134228: step 94780, loss = 0.50 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:09.452596: step 94790, loss = 0.67 (7944.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:09.771563: step 94800, loss = 0.53 (8161.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:10.238008: step 94810, loss = 0.76 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:10.560860: step 94820, loss = 0.58 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:10.882103: step 94830, loss = 0.59 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:11.201188: step 94840, loss = 0.54 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:11.522270: step 94850, loss = 0.56 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:11.841823: step 94860, loss = 0.44 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:12.161324: step 94870, loss = 0.60 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:12.480376: step 94880, loss = 0.64 (8167.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:12.799964: step 94890, loss = 0.56 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:13.119392: step 94900, loss = 0.54 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:13.572040: step 94910, loss = 0.60 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:13.892754: step 94920, loss = 0.51 (7815.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:14.212048: step 94930, loss = 0.57 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:14.536323: step 94940, loss = 0.52 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:14.855606: step 94950, loss = 0.65 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:15.175845: step 94960, loss = 0.60 (7798.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:15.497161: step 94970, loss = 0.70 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:15.818487: step 94980, loss = 0.58 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:16.138927: step 94990, loss = 0.63 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:16.458875: step 95000, loss = 0.57 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:17.023099: step 95010, loss = 0.50 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:17.343067: step 95020, loss = 0.55 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:17.667665: step 95030, loss = 0.77 (7804.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:17.986113: step 95040, loss = 0.74 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:18.306155: step 95050, loss = 0.60 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:18.624867: step 95060, loss = 0.60 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:18.944368: step 95070, loss = 0.59 (8130.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:19.262371: step 95080, loss = 0.53 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:19.582474: step 95090, loss = 0.62 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:19.903009: step 95100, loss = 0.52 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:20.355163: step 95110, loss = 0.61 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:20.673983: step 95120, loss = 0.59 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:20.996936: step 95130, loss = 0.57 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:21.318300: step 95140, loss = 0.56 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:21.639196: step 95150, loss = 0.46 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:21.959872: step 95160, loss = 0.49 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:22.280114: step 95170, loss = 0.64 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:22.603632: step 95180, loss = 0.57 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:22.925532: step 95190, loss = 0.57 (7676.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:33:23.245285: step 95200, loss = 0.59 (7568.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:33:23.697322: step 95210, loss = 0.67 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:24.018256: step 95220, loss = 0.68 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:24.338769: step 95230, loss = 0.51 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:24.660130: step 95240, loss = 0.63 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:24.981524: step 95250, loss = 0.61 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:25.302746: step 95260, loss = 0.50 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:25.627000: step 95270, loss = 0.51 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:25.947696: step 95280, loss = 0.61 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:26.268947: step 95290, loss = 0.52 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:26.587844: step 95300, loss = 0.56 (8075.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:27.044607: step 95310, loss = 0.37 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:27.365798: step 95320, loss = 0.58 (8018.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:27.683789: step 95330, loss = 0.59 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:28.007679: step 95340, loss = 0.59 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:28.331524: step 95350, loss = 0.61 (7964.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:28.651099: step 95360, loss = 0.72 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:28.973806: step 95370, loss = 0.54 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:29.298758: step 95380, loss = 0.69 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:29.619322: step 95390, loss = 0.64 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:29.939433: step 95400, loss = 0.46 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:30.397930: step 95410, loss = 0.53 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:30.715650: step 95420, loss = 0.59 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:31.040298: step 95430, loss = 0.64 (7776.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:31.359842: step 95440, loss = 0.52 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:31.677596: step 95450, loss = 0.53 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:31.998142: step 95460, loss = 0.55 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:32.318414: step 95470, loss = 0.65 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:32.638662: step 95480, loss = 0.50 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:32.959705: step 95490, loss = 0.71 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:33.278344: step 95500, loss = 0.45 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:33.740211: step 95510, loss = 0.62 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:34.060275: step 95520, loss = 0.55 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:34.380802: step 95530, loss = 0.51 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:34.704545: step 95540, loss = 0.51 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:35.025602: step 95550, loss = 0.66 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:35.346220: step 95560, loss = 0.54 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:35.667969: step 95570, loss = 0.52 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:35.986962: step 95580, loss = 0.62 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:36.308944: step 95590, loss = 0.72 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:36.627734: step 95600, loss = 0.52 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:37.089704: step 95610, loss = 0.54 (8153.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:37.415407: step 95620, loss = 0.55 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:37.737233: step 95630, loss = 0.62 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:38.058408: step 95640, loss = 0.64 (7937.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:38.379644: step 95650, loss = 0.58 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:38.699938: step 95660, loss = 0.45 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:39.021092: step 95670, loss = 0.56 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:39.344249: step 95680, loss = 0.64 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:39.665074: step 95690, loss = 0.56 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:39.986819: step 95700, loss = 0.58 (8033.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:40.443413: step 95710, loss = 0.62 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:40.765079: step 95720, loss = 0.62 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:41.086891: step 95730, loss = 0.50 (7911.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:41.406472: step 95740, loss = 0.52 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:41.726880: step 95750, loss = 0.62 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:42.047092: step 95760, loss = 0.72 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:42.369234: step 95770, loss = 0.57 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:42.689899: step 95780, loss = 0.48 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:43.008484: step 95790, loss = 0.59 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:43.327757: step 95800, loss = 0.51 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:43.778856: step 95810, loss = 0.77 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:44.099386: step 95820, loss = 0.47 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:44.419798: step 95830, loss = 0.61 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:44.738731: step 95840, loss = 0.69 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:45.058709: step 95850, loss = 0.59 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:45.375942: step 95860, loss = 0.64 (8131.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:45.697185: step 95870, loss = 0.56 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:46.018260: step 95880, loss = 0.62 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:46.337294: step 95890, loss = 0.63 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:46.657913: step 95900, loss = 0.61 (8085.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:47.111275: step 95910, loss = 0.65 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:47.430651: step 95920, loss = 0.74 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:47.749719: step 95930, loss = 0.49 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:48.071105: step 95940, loss = 0.58 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:48.394295: step 95950, loss = 0.55 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:48.714735: step 95960, loss = 0.61 (7804.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:49.034292: step 95970, loss = 0.57 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:49.356634: step 95980, loss = 0.47 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:49.676114: step 95990, loss = 0.65 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:49.996902: step 96000, loss = 0.53 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:50.554107: step 96010, loss = 0.59 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:50.871351: step 96020, loss = 0.58 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:51.192520: step 96030, loss = 0.63 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:51.513806: step 96040, loss = 0.62 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:51.831946: step 96050, loss = 0.49 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:52.155843: step 96060, loss = 0.57 (8026.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:52.477192: step 96070, loss = 0.41 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:52.798016: step 96080, loss = 0.64 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:53.117856: step 96090, loss = 0.54 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:53.437896: step 96100, loss = 0.59 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:53.898686: step 96110, loss = 0.65 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:54.219647: step 96120, loss = 0.54 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:54.538729: step 96130, loss = 0.50 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:54.859364: step 96140, loss = 0.57 (7970.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:55.179713: step 96150, loss = 0.60 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:55.501590: step 96160, loss = 0.53 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:55.824440: step 96170, loss = 0.61 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:56.148014: step 96180, loss = 0.57 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:56.467604: step 96190, loss = 0.46 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:56.787851: step 96200, loss = 0.58 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:57.249244: step 96210, loss = 0.52 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:57.570210: step 96220, loss = 0.62 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:57.891701: step 96230, loss = 0.56 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:58.214030: step 96240, loss = 0.69 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:58.536295: step 96250, loss = 0.61 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:58.860129: step 96260, loss = 0.66 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:59.181222: step 96270, loss = 0.81 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:59.507343: step 96280, loss = 0.49 (7830.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:33:59.825449: step 96290, loss = 0.46 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:00.143820: step 96300, loss = 0.62 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:00.612584: step 96310, loss = 0.54 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:00.933369: step 96320, loss = 0.64 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:01.251772: step 96330, loss = 0.47 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:01.573279: step 96340, loss = 0.59 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:01.895456: step 96350, loss = 0.49 (7552.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:02.216552: step 96360, loss = 0.54 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:02.538117: step 96370, loss = 0.58 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:02.857315: step 96380, loss = 0.66 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:03.177261: step 96390, loss = 0.58 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:03.497405: step 96400, loss = 0.56 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:03.946882: step 96410, loss = 0.56 (7684.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:04.265039: step 96420, loss = 0.64 (8157.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:04.584464: step 96430, loss = 0.47 (7940.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:04.906849: step 96440, loss = 0.60 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:05.227890: step 96450, loss = 0.69 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:05.548201: step 96460, loss = 0.56 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:05.869083: step 96470, loss = 0.50 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:06.188715: step 96480, loss = 0.75 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:06.509202: step 96490, loss = 0.61 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:06.829588: step 96500, loss = 0.57 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:07.292201: step 96510, loss = 0.60 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:07.612322: step 96520, loss = 0.61 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:07.933104: step 96530, loss = 0.64 (7959.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:08.254509: step 96540, loss = 0.59 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:08.572883: step 96550, loss = 0.62 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:08.891905: step 96560, loss = 0.57 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:09.211850: step 96570, loss = 0.51 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:09.530960: step 96580, loss = 0.57 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:09.852204: step 96590, loss = 0.71 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:10.172713: step 96600, loss = 0.64 (7709.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:10.626172: step 96610, loss = 0.55 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:10.945786: step 96620, loss = 0.58 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:11.267565: step 96630, loss = 0.51 (8010.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:11.587477: step 96640, loss = 0.70 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:11.908563: step 96650, loss = 0.61 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:12.226842: step 96660, loss = 0.59 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:12.548222: step 96670, loss = 0.56 (7834.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:12.866931: step 96680, loss = 0.41 (7798.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:13.187469: step 96690, loss = 0.55 (7972.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:13.508637: step 96700, loss = 0.67 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:13.970535: step 96710, loss = 0.53 (7919.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:14.290183: step 96720, loss = 0.46 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:14.612514: step 96730, loss = 0.60 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:14.934673: step 96740, loss = 0.56 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:15.256532: step 96750, loss = 0.63 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:15.580443: step 96760, loss = 0.68 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:15.904593: step 96770, loss = 0.51 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:16.224359: step 96780, loss = 0.69 (8139.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:16.544454: step 96790, loss = 0.57 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:16.865740: step 96800, loss = 0.56 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:17.322131: step 96810, loss = 0.49 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:17.646345: step 96820, loss = 0.64 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:17.971687: step 96830, loss = 0.57 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:18.291089: step 96840, loss = 0.56 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:18.614191: step 96850, loss = 0.52 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:18.934042: step 96860, loss = 0.72 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:19.253604: step 96870, loss = 0.75 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:19.574386: step 96880, loss = 0.52 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:19.895239: step 96890, loss = 0.59 (7895.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:20.216955: step 96900, loss = 0.64 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:20.666303: step 96910, loss = 0.51 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:20.986183: step 96920, loss = 0.61 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:21.307949: step 96930, loss = 0.53 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:21.628180: step 96940, loss = 0.59 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:21.950444: step 96950, loss = 0.48 (8153.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:22.272541: step 96960, loss = 0.62 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:22.593739: step 96970, loss = 0.59 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:22.915349: step 96980, loss = 0.54 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:23.234055: step 96990, loss = 0.46 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:23.555334: step 97000, loss = 0.73 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:24.155478: step 97010, loss = 0.55 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:24.476106: step 97020, loss = 0.52 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:24.797928: step 97030, loss = 0.62 (7809.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:25.121587: step 97040, loss = 0.51 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:25.443973: step 97050, loss = 0.55 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:25.763946: step 97060, loss = 0.53 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:26.084907: step 97070, loss = 0.49 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:26.406131: step 97080, loss = 0.55 (8121.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:26.727328: step 97090, loss = 0.55 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:27.049846: step 97100, loss = 0.52 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:27.504165: step 97110, loss = 0.58 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:27.824707: step 97120, loss = 0.63 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:28.147542: step 97130, loss = 0.54 (7754.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:28.471006: step 97140, loss = 0.64 (7897.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:28.793240: step 97150, loss = 0.56 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:29.113428: step 97160, loss = 0.48 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:29.433446: step 97170, loss = 0.68 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:29.753879: step 97180, loss = 0.55 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:30.075385: step 97190, loss = 0.64 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:30.398330: step 97200, loss = 0.48 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:30.862018: step 97210, loss = 0.60 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:31.182463: step 97220, loss = 0.55 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:31.502430: step 97230, loss = 0.62 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:31.825432: step 97240, loss = 0.71 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:32.147798: step 97250, loss = 0.58 (7888.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:32.467904: step 97260, loss = 0.67 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:32.789893: step 97270, loss = 0.56 (7790.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:33.110760: step 97280, loss = 0.50 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:33.433658: step 97290, loss = 0.52 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:33.756745: step 97300, loss = 0.61 (7814.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:34.211107: step 97310, loss = 0.54 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:34.530422: step 97320, loss = 0.59 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:34.851455: step 97330, loss = 0.72 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:35.171473: step 97340, loss = 0.52 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:35.492077: step 97350, loss = 0.83 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:35.811845: step 97360, loss = 0.52 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:36.131415: step 97370, loss = 0.55 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:36.450789: step 97380, loss = 0.71 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:36.772118: step 97390, loss = 0.49 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:37.092241: step 97400, loss = 0.51 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:37.547960: step 97410, loss = 0.52 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:37.866825: step 97420, loss = 0.61 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:38.187949: step 97430, loss = 0.55 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:38.509483: step 97440, loss = 0.66 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:38.832034: step 97450, loss = 0.44 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:39.156513: step 97460, loss = 0.60 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:39.479467: step 97470, loss = 0.55 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:39.800703: step 97480, loss = 0.58 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:40.120949: step 97490, loss = 0.55 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:40.440583: step 97500, loss = 0.52 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:40.908290: step 97510, loss = 0.53 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:41.228309: step 97520, loss = 0.58 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:41.549156: step 97530, loss = 0.55 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:41.869200: step 97540, loss = 0.63 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:42.189177: step 97550, loss = 0.51 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:42.509366: step 97560, loss = 0.50 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:42.828253: step 97570, loss = 0.52 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:43.148410: step 97580, loss = 0.52 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:43.469367: step 97590, loss = 0.54 (8168.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:43.789251: step 97600, loss = 0.47 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:44.236045: step 97610, loss = 0.64 (7987.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:44.558219: step 97620, loss = 0.57 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:44.879515: step 97630, loss = 0.60 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:45.200370: step 97640, loss = 0.54 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:45.520736: step 97650, loss = 0.50 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:45.842640: step 97660, loss = 0.57 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:46.164181: step 97670, loss = 0.56 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:46.483700: step 97680, loss = 0.50 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:46.803582: step 97690, loss = 0.52 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:47.124591: step 97700, loss = 0.53 (7788.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:47.573764: step 97710, loss = 0.60 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:47.895693: step 97720, loss = 0.51 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:48.214715: step 97730, loss = 0.61 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:48.535774: step 97740, loss = 0.58 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:48.859433: step 97750, loss = 0.52 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:49.179963: step 97760, loss = 0.52 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:49.501579: step 97770, loss = 0.41 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:49.824273: step 97780, loss = 0.54 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:50.145653: step 97790, loss = 0.46 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:50.467103: step 97800, loss = 0.53 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:50.933068: step 97810, loss = 0.51 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:51.251971: step 97820, loss = 0.55 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:51.572154: step 97830, loss = 0.57 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:51.890949: step 97840, loss = 0.55 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:52.212025: step 97850, loss = 0.51 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:52.531113: step 97860, loss = 0.71 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:52.852076: step 97870, loss = 0.55 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:53.172471: step 97880, loss = 0.44 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:53.496058: step 97890, loss = 0.58 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:53.816159: step 97900, loss = 0.61 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:54.264703: step 97910, loss = 0.55 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:54.585200: step 97920, loss = 0.60 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:54.909375: step 97930, loss = 0.47 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:55.230584: step 97940, loss = 0.63 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:55.550556: step 97950, loss = 0.67 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:55.870276: step 97960, loss = 0.53 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:56.191725: step 97970, loss = 0.60 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:56.515988: step 97980, loss = 0.57 (7393.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:56.842229: step 97990, loss = 0.45 (7630.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:57.162956: step 98000, loss = 0.56 (7882.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:57.722264: step 98010, loss = 0.53 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:58.043114: step 98020, loss = 0.51 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:58.364799: step 98030, loss = 0.55 (7745.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:58.684498: step 98040, loss = 0.51 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:59.009869: step 98050, loss = 0.48 (7588.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:34:59.331903: step 98060, loss = 0.59 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:59.652384: step 98070, loss = 0.43 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:34:59.970176: step 98080, loss = 0.54 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:00.290746: step 98090, loss = 0.67 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:00.613641: step 98100, loss = 0.62 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:01.066848: step 98110, loss = 0.63 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:01.387002: step 98120, loss = 0.60 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:01.709551: step 98130, loss = 0.55 (7919.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:02.032431: step 98140, loss = 0.60 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:02.353527: step 98150, loss = 0.49 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:02.674184: step 98160, loss = 0.56 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:03.000733: step 98170, loss = 0.50 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:03.323510: step 98180, loss = 0.73 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:03.642359: step 98190, loss = 0.54 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:03.963320: step 98200, loss = 0.42 (7805.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:04.424229: step 98210, loss = 0.81 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:04.748459: step 98220, loss = 0.54 (8167.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:05.073531: step 98230, loss = 0.60 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:05.395300: step 98240, loss = 0.82 (7873.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:05.715655: step 98250, loss = 0.55 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:06.035772: step 98260, loss = 0.51 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:06.357849: step 98270, loss = 0.63 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:06.679965: step 98280, loss = 0.57 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:07.003220: step 98290, loss = 0.62 (7799.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:07.326075: step 98300, loss = 0.78 (7824.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:07.770693: step 98310, loss = 0.61 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:08.090272: step 98320, loss = 0.56 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:08.410183: step 98330, loss = 0.68 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:08.732707: step 98340, loss = 0.55 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:09.053308: step 98350, loss = 0.62 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:09.374270: step 98360, loss = 0.53 (7807.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:09.694053: step 98370, loss = 0.60 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:10.015872: step 98380, loss = 0.40 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:10.334698: step 98390, loss = 0.53 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:10.655358: step 98400, loss = 0.50 (8103.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:11.114123: step 98410, loss = 0.42 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:11.436333: step 98420, loss = 0.70 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:11.755672: step 98430, loss = 0.49 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:12.075891: step 98440, loss = 0.60 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:12.396338: step 98450, loss = 0.59 (7936.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:12.717875: step 98460, loss = 0.54 (7892.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:13.038303: step 98470, loss = 0.47 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:13.357253: step 98480, loss = 0.56 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:13.679026: step 98490, loss = 0.47 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:13.999903: step 98500, loss = 0.49 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:14.461505: step 98510, loss = 0.53 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:14.784854: step 98520, loss = 0.56 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:15.107539: step 98530, loss = 0.62 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:15.428553: step 98540, loss = 0.43 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:15.751775: step 98550, loss = 0.51 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:16.075660: step 98560, loss = 0.44 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:16.396529: step 98570, loss = 0.58 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:16.719137: step 98580, loss = 0.53 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:17.038902: step 98590, loss = 0.69 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:17.361238: step 98600, loss = 0.61 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:17.832038: step 98610, loss = 0.64 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:18.152127: step 98620, loss = 0.63 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:18.472466: step 98630, loss = 0.53 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:18.792065: step 98640, loss = 0.62 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:19.113445: step 98650, loss = 0.55 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:19.434554: step 98660, loss = 0.69 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:19.755449: step 98670, loss = 0.60 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:20.076634: step 98680, loss = 0.73 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:20.401256: step 98690, loss = 0.61 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:20.720572: step 98700, loss = 0.76 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:21.166719: step 98710, loss = 0.50 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:21.489997: step 98720, loss = 0.52 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:21.811178: step 98730, loss = 0.56 (7900.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:22.131857: step 98740, loss = 0.67 (8060.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:22.455276: step 98750, loss = 0.51 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:22.774475: step 98760, loss = 0.61 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:23.093239: step 98770, loss = 0.56 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:23.416539: step 98780, loss = 0.59 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:23.736457: step 98790, loss = 0.49 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:24.057218: step 98800, loss = 0.56 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:24.511097: step 98810, loss = 0.64 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:24.832246: step 98820, loss = 0.49 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:25.151383: step 98830, loss = 0.59 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:25.472868: step 98840, loss = 0.58 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:25.793280: step 98850, loss = 0.49 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:26.114636: step 98860, loss = 0.65 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:26.433666: step 98870, loss = 0.52 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:26.752118: step 98880, loss = 0.76 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:27.074514: step 98890, loss = 0.58 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:27.395255: step 98900, loss = 0.51 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:27.855260: step 98910, loss = 0.49 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:28.175195: step 98920, loss = 0.56 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:28.496919: step 98930, loss = 0.57 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:28.817743: step 98940, loss = 0.69 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:29.138291: step 98950, loss = 0.59 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:29.457660: step 98960, loss = 0.71 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:29.778025: step 98970, loss = 0.45 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:30.098554: step 98980, loss = 0.54 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:30.418979: step 98990, loss = 0.55 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:30.742367: step 99000, loss = 0.61 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:31.295624: step 99010, loss = 0.62 (7535.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:35:31.622845: step 99020, loss = 0.73 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:31.942647: step 99030, loss = 0.55 (7882.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:32.264423: step 99040, loss = 0.38 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:32.584454: step 99050, loss = 0.46 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:32.904981: step 99060, loss = 0.57 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:33.225790: step 99070, loss = 0.58 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:33.548240: step 99080, loss = 0.66 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:33.868312: step 99090, loss = 0.68 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:34.188781: step 99100, loss = 0.63 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:34.647562: step 99110, loss = 0.56 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:34.969632: step 99120, loss = 0.56 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:35.289733: step 99130, loss = 0.51 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:35.611470: step 99140, loss = 0.54 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:35.933952: step 99150, loss = 0.50 (7970.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:36.254845: step 99160, loss = 0.53 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:36.575340: step 99170, loss = 0.60 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:36.894855: step 99180, loss = 0.47 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:37.213855: step 99190, loss = 0.63 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:37.535868: step 99200, loss = 0.67 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:37.994823: step 99210, loss = 0.54 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:38.315628: step 99220, loss = 0.54 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:38.634949: step 99230, loss = 0.53 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:38.953603: step 99240, loss = 0.51 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:39.278238: step 99250, loss = 0.58 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:39.598797: step 99260, loss = 0.51 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:39.921440: step 99270, loss = 0.50 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:40.242862: step 99280, loss = 0.49 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:40.567574: step 99290, loss = 0.64 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:40.889550: step 99300, loss = 0.61 (7666.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:35:41.342928: step 99310, loss = 0.51 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:41.665646: step 99320, loss = 0.62 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:41.988113: step 99330, loss = 0.62 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:42.309448: step 99340, loss = 0.62 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:42.631898: step 99350, loss = 0.56 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:42.954728: step 99360, loss = 0.62 (7797.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:43.277110: step 99370, loss = 0.46 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:43.604637: step 99380, loss = 0.48 (6878.8 examples/sec; 0.019 sec/batch)
2017-09-16 16:35:43.929311: step 99390, loss = 0.71 (8088.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:44.250195: step 99400, loss = 0.60 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:44.711853: step 99410, loss = 0.47 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:45.032604: step 99420, loss = 0.52 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:45.356997: step 99430, loss = 0.68 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:45.675621: step 99440, loss = 0.54 (8062.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:45.995759: step 99450, loss = 0.52 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:46.317133: step 99460, loss = 0.54 (7602.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:35:46.638514: step 99470, loss = 0.59 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:46.958614: step 99480, loss = 0.49 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:47.279196: step 99490, loss = 0.62 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:47.599841: step 99500, loss = 0.64 (7929.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:48.057813: step 99510, loss = 0.49 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:48.377698: step 99520, loss = 0.57 (7556.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:35:48.704545: step 99530, loss = 0.64 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:49.025678: step 99540, loss = 0.64 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:49.348583: step 99550, loss = 0.54 (7351.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:35:49.669018: step 99560, loss = 0.59 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:49.994262: step 99570, loss = 0.52 (7831.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:50.314504: step 99580, loss = 0.51 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:50.631411: step 99590, loss = 0.64 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:50.953716: step 99600, loss = 0.45 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:51.405112: step 99610, loss = 0.44 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:51.727469: step 99620, loss = 0.63 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:52.048071: step 99630, loss = 0.51 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:52.367092: step 99640, loss = 0.60 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:52.688009: step 99650, loss = 0.57 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:53.008049: step 99660, loss = 0.53 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:53.328197: step 99670, loss = 0.54 (8137.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:53.647595: step 99680, loss = 0.64 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:53.967523: step 99690, loss = 0.71 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:54.287514: step 99700, loss = 0.56 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:54.737330: step 99710, loss = 0.59 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:55.057473: step 99720, loss = 0.55 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:55.378240: step 99730, loss = 0.51 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:55.699093: step 99740, loss = 0.48 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:56.017336: step 99750, loss = 0.52 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:56.337284: step 99760, loss = 0.54 (7838.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:56.657898: step 99770, loss = 0.67 (7919.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:56.977560: step 99780, loss = 0.54 (8011.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:57.298077: step 99790, loss = 0.53 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:57.620516: step 99800, loss = 0.72 (8101.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:58.069553: step 99810, loss = 0.51 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:58.391347: step 99820, loss = 0.52 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:58.712067: step 99830, loss = 0.57 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:59.031463: step 99840, loss = 0.59 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:59.349788: step 99850, loss = 0.69 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:59.672178: step 99860, loss = 0.56 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:35:59.991126: step 99870, loss = 0.48 (8060.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:00.315607: step 99880, loss = 0.63 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:00.640752: step 99890, loss = 0.64 (7707.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:00.963776: step 99900, loss = 0.56 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:01.424588: step 99910, loss = 0.56 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:01.746671: step 99920, loss = 0.81 (7450.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:02.069965: step 99930, loss = 0.62 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:02.393082: step 99940, loss = 0.61 (7829.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:02.712873: step 99950, loss = 0.46 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:03.033195: step 99960, loss = 0.62 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:03.354547: step 99970, loss = 0.50 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:03.677478: step 99980, loss = 0.72 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:03.999913: step 99990, loss = 0.62 (7653.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:04.322955: step 100000, loss = 0.60 (7830.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:04.868502: step 100010, loss = 0.53 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:05.189445: step 100020, loss = 0.48 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:05.514157: step 100030, loss = 0.53 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:05.836628: step 100040, loss = 0.54 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:06.165337: step 100050, loss = 0.58 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:06.486259: step 100060, loss = 0.60 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:06.808243: step 100070, loss = 0.49 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:07.128977: step 100080, loss = 0.62 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:07.455162: step 100090, loss = 0.61 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:07.776602: step 100100, loss = 0.59 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:08.239553: step 100110, loss = 0.59 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:08.559686: step 100120, loss = 0.65 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:08.879455: step 100130, loss = 0.57 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:09.202861: step 100140, loss = 0.44 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:09.524428: step 100150, loss = 0.70 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:09.845090: step 100160, loss = 0.61 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:10.164278: step 100170, loss = 0.58 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:10.485214: step 100180, loss = 0.62 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:10.804798: step 100190, loss = 0.52 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:11.124127: step 100200, loss = 0.58 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:11.583096: step 100210, loss = 0.51 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:11.904420: step 100220, loss = 0.52 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:12.227405: step 100230, loss = 0.56 (7873.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:12.547072: step 100240, loss = 0.61 (8154.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:12.866464: step 100250, loss = 0.51 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:13.185025: step 100260, loss = 0.59 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:13.506135: step 100270, loss = 0.54 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:13.826028: step 100280, loss = 0.58 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:14.145813: step 100290, loss = 0.55 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:14.467062: step 100300, loss = 0.48 (7846.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:14.935964: step 100310, loss = 0.54 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:15.256792: step 100320, loss = 0.44 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:15.579898: step 100330, loss = 0.58 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:15.899552: step 100340, loss = 0.48 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:16.221410: step 100350, loss = 0.54 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:16.541315: step 100360, loss = 0.50 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:16.862194: step 100370, loss = 0.75 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:17.182428: step 100380, loss = 0.57 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:17.502872: step 100390, loss = 0.65 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:17.822613: step 100400, loss = 0.72 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:18.285932: step 100410, loss = 0.63 (8085.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:18.605315: step 100420, loss = 0.55 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:18.926504: step 100430, loss = 0.54 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:19.247047: step 100440, loss = 0.57 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:19.568110: step 100450, loss = 0.66 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:19.889225: step 100460, loss = 0.70 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:20.207678: step 100470, loss = 0.61 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:20.528084: step 100480, loss = 0.54 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:20.847406: step 100490, loss = 0.64 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:21.168921: step 100500, loss = 0.66 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:21.628044: step 100510, loss = 0.56 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:21.948951: step 100520, loss = 0.48 (7775.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:22.270292: step 100530, loss = 0.59 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:22.591493: step 100540, loss = 0.52 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:22.912184: step 100550, loss = 0.48 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:23.232989: step 100560, loss = 0.64 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:23.553916: step 100570, loss = 0.48 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:23.877306: step 100580, loss = 0.48 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:24.201382: step 100590, loss = 0.59 (7792.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:24.526625: step 100600, loss = 0.60 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:24.985874: step 100610, loss = 0.60 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:25.307406: step 100620, loss = 0.61 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:25.626577: step 100630, loss = 0.64 (7857.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:25.947102: step 100640, loss = 0.56 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:26.267404: step 100650, loss = 0.55 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:26.587709: step 100660, loss = 0.49 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:26.907475: step 100670, loss = 0.56 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:27.229604: step 100680, loss = 0.47 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:27.548940: step 100690, loss = 0.58 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:27.869626: step 100700, loss = 0.51 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:28.321500: step 100710, loss = 0.72 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:28.640430: step 100720, loss = 0.53 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:28.960608: step 100730, loss = 0.52 (7908.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:29.284354: step 100740, loss = 0.66 (7292.2 examples/sec; 0.018 sec/batch)
2017-09-16 16:36:29.604903: step 100750, loss = 0.60 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:29.924009: step 100760, loss = 0.63 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:30.244422: step 100770, loss = 0.42 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:30.564934: step 100780, loss = 0.63 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:30.883983: step 100790, loss = 0.51 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:31.205751: step 100800, loss = 0.63 (7936.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:31.658822: step 100810, loss = 0.46 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:31.986403: step 100820, loss = 0.50 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:32.307670: step 100830, loss = 0.48 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:32.627753: step 100840, loss = 0.56 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:32.951162: step 100850, loss = 0.55 (7455.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:33.272920: step 100860, loss = 0.50 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:33.592762: step 100870, loss = 0.67 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:33.912800: step 100880, loss = 0.53 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:34.235946: step 100890, loss = 0.57 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:34.558296: step 100900, loss = 0.51 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:35.016777: step 100910, loss = 0.54 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:35.338674: step 100920, loss = 0.50 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:35.659466: step 100930, loss = 0.56 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:35.983180: step 100940, loss = 0.64 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:36.303904: step 100950, loss = 0.58 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:36.626024: step 100960, loss = 0.65 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:36.947572: step 100970, loss = 0.72 (7779.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:37.267379: step 100980, loss = 0.56 (8146.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:37.588247: step 100990, loss = 0.61 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:37.912560: step 101000, loss = 0.67 (8145.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:38.474826: step 101010, loss = 0.62 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:38.794800: step 101020, loss = 0.55 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:39.115302: step 101030, loss = 0.50 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:39.442050: step 101040, loss = 0.58 (7441.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:39.760972: step 101050, loss = 0.64 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:40.082154: step 101060, loss = 0.63 (8069.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:40.402288: step 101070, loss = 0.47 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:40.725336: step 101080, loss = 0.62 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:41.045967: step 101090, loss = 0.52 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:41.367604: step 101100, loss = 0.54 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:41.820546: step 101110, loss = 0.58 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:42.140281: step 101120, loss = 0.52 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:42.460331: step 101130, loss = 0.49 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:42.785699: step 101140, loss = 0.80 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:43.107645: step 101150, loss = 0.56 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:43.428989: step 101160, loss = 0.64 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:43.752769: step 101170, loss = 0.54 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:44.074397: step 101180, loss = 0.54 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:44.393643: step 101190, loss = 0.59 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:44.714939: step 101200, loss = 0.58 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:45.177685: step 101210, loss = 0.56 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:45.500502: step 101220, loss = 0.55 (7883.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:45.820877: step 101230, loss = 0.57 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:46.142738: step 101240, loss = 0.53 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:46.462572: step 101250, loss = 0.68 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:46.782690: step 101260, loss = 0.65 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:47.104290: step 101270, loss = 0.60 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:47.426330: step 101280, loss = 0.54 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:47.748830: step 101290, loss = 0.55 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:48.066961: step 101300, loss = 0.59 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:48.528420: step 101310, loss = 0.57 (8010.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:48.846189: step 101320, loss = 0.50 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:49.167074: step 101330, loss = 0.60 (7845.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:49.485554: step 101340, loss = 0.56 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:49.806000: step 101350, loss = 0.59 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:50.128364: step 101360, loss = 0.55 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:50.448669: step 101370, loss = 0.53 (8128.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:50.766470: step 101380, loss = 0.72 (8102.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:51.088535: step 101390, loss = 0.43 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:51.405784: step 101400, loss = 0.48 (8139.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:51.850976: step 101410, loss = 0.62 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:52.172013: step 101420, loss = 0.53 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:52.492650: step 101430, loss = 0.52 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:52.813663: step 101440, loss = 0.58 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:53.131938: step 101450, loss = 0.54 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:53.454494: step 101460, loss = 0.57 (7652.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:36:53.775660: step 101470, loss = 0.62 (7822.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:54.100923: step 101480, loss = 0.74 (7781.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:54.421954: step 101490, loss = 0.54 (7823.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:54.741606: step 101500, loss = 0.56 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:55.204239: step 101510, loss = 0.55 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:55.525256: step 101520, loss = 0.60 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:55.846585: step 101530, loss = 0.42 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:56.165598: step 101540, loss = 0.65 (8111.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:56.488108: step 101550, loss = 0.56 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:56.809793: step 101560, loss = 0.55 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:57.131320: step 101570, loss = 0.58 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:57.451806: step 101580, loss = 0.56 (8161.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:57.772551: step 101590, loss = 0.66 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:58.094215: step 101600, loss = 0.68 (7803.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:58.559094: step 101610, loss = 0.60 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:58.878850: step 101620, loss = 0.55 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:59.198077: step 101630, loss = 0.64 (7789.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:59.518072: step 101640, loss = 0.58 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:36:59.841726: step 101650, loss = 0.64 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:00.168823: step 101660, loss = 0.59 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:00.489212: step 101670, loss = 0.63 (7903.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:00.811508: step 101680, loss = 0.51 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:01.131419: step 101690, loss = 0.48 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:01.450756: step 101700, loss = 0.61 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:01.913764: step 101710, loss = 0.53 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:02.236903: step 101720, loss = 0.56 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:02.555096: step 101730, loss = 0.67 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:02.874562: step 101740, loss = 0.65 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:03.197920: step 101750, loss = 0.54 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:03.523135: step 101760, loss = 0.47 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:03.849391: step 101770, loss = 0.46 (7749.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:37:04.170125: step 101780, loss = 0.54 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:04.493570: step 101790, loss = 0.59 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:04.815125: step 101800, loss = 0.62 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:05.277088: step 101810, loss = 0.58 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:05.597068: step 101820, loss = 0.70 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:05.918414: step 101830, loss = 0.56 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:06.241570: step 101840, loss = 0.60 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:06.563771: step 101850, loss = 0.47 (8001.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:06.883955: step 101860, loss = 0.60 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:07.205547: step 101870, loss = 0.47 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:07.527708: step 101880, loss = 0.60 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:07.849639: step 101890, loss = 0.48 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:08.168980: step 101900, loss = 0.52 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:08.616172: step 101910, loss = 0.57 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:08.938005: step 101920, loss = 0.85 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:09.258956: step 101930, loss = 0.46 (7876.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:09.578889: step 101940, loss = 0.54 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:09.899716: step 101950, loss = 0.49 (7836.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:10.222025: step 101960, loss = 0.59 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:10.542649: step 101970, loss = 0.66 (7963.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:10.863192: step 101980, loss = 0.59 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:11.185821: step 101990, loss = 0.57 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:11.506384: step 102000, loss = 0.49 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:12.059135: step 102010, loss = 0.51 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:12.379628: step 102020, loss = 0.52 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:12.699630: step 102030, loss = 0.48 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:13.021992: step 102040, loss = 0.52 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:13.342628: step 102050, loss = 0.56 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:13.661394: step 102060, loss = 0.58 (8127.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:13.981109: step 102070, loss = 0.51 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:14.301274: step 102080, loss = 0.60 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:14.621077: step 102090, loss = 0.59 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:14.942410: step 102100, loss = 0.51 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:15.401099: step 102110, loss = 0.55 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:15.720346: step 102120, loss = 0.63 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:16.041734: step 102130, loss = 0.55 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:16.360962: step 102140, loss = 0.52 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:16.682396: step 102150, loss = 0.53 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:17.006115: step 102160, loss = 0.62 (7775.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:17.327802: step 102170, loss = 0.78 (7796.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:17.653232: step 102180, loss = 0.47 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:17.971632: step 102190, loss = 0.47 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:18.292437: step 102200, loss = 0.54 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:18.749742: step 102210, loss = 0.65 (8144.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:19.071028: step 102220, loss = 0.52 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:19.389662: step 102230, loss = 0.52 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:19.712376: step 102240, loss = 0.44 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:20.032576: step 102250, loss = 0.58 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:20.351889: step 102260, loss = 0.64 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:20.673430: step 102270, loss = 0.66 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:20.993007: step 102280, loss = 0.64 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:21.312018: step 102290, loss = 0.77 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:21.634065: step 102300, loss = 0.57 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:22.098302: step 102310, loss = 0.55 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:22.416471: step 102320, loss = 0.45 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:22.737349: step 102330, loss = 0.51 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:23.056072: step 102340, loss = 0.52 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:23.375189: step 102350, loss = 0.54 (7982.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:23.695127: step 102360, loss = 0.58 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:24.014847: step 102370, loss = 0.54 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:24.337844: step 102380, loss = 0.60 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:24.657096: step 102390, loss = 0.61 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:24.979039: step 102400, loss = 0.59 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:25.437805: step 102410, loss = 0.57 (7866.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:25.757555: step 102420, loss = 0.67 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:26.077936: step 102430, loss = 0.61 (7925.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:26.399173: step 102440, loss = 0.50 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:26.725973: step 102450, loss = 0.66 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:27.047958: step 102460, loss = 0.55 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:27.370986: step 102470, loss = 0.63 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:27.692157: step 102480, loss = 0.64 (7795.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:28.013494: step 102490, loss = 0.61 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:28.334278: step 102500, loss = 0.51 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:28.793402: step 102510, loss = 0.64 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:29.113180: step 102520, loss = 0.61 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:29.438770: step 102530, loss = 0.52 (7428.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:37:29.764493: step 102540, loss = 0.47 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:30.086778: step 102550, loss = 0.61 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:30.406218: step 102560, loss = 0.71 (7976.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:30.726180: step 102570, loss = 0.65 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:31.046192: step 102580, loss = 0.53 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:31.365980: step 102590, loss = 0.73 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:31.688087: step 102600, loss = 0.58 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:32.146782: step 102610, loss = 0.50 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:32.468350: step 102620, loss = 0.62 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:32.789231: step 102630, loss = 0.55 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:33.108461: step 102640, loss = 0.51 (8137.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:33.425720: step 102650, loss = 0.63 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:33.745412: step 102660, loss = 0.63 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:34.065666: step 102670, loss = 0.54 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:34.387808: step 102680, loss = 0.55 (8093.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:34.710005: step 102690, loss = 0.50 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:35.030657: step 102700, loss = 0.62 (7796.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:35.488289: step 102710, loss = 0.72 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:35.807712: step 102720, loss = 0.46 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:36.128430: step 102730, loss = 0.59 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:36.449741: step 102740, loss = 0.48 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:36.768857: step 102750, loss = 0.59 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:37.088719: step 102760, loss = 0.59 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:37.407937: step 102770, loss = 0.54 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:37.727798: step 102780, loss = 0.45 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:38.048879: step 102790, loss = 0.57 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:38.370974: step 102800, loss = 0.57 (7577.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:37:38.827253: step 102810, loss = 0.54 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:39.149486: step 102820, loss = 0.65 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:39.470448: step 102830, loss = 0.53 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:39.792436: step 102840, loss = 0.53 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:40.112072: step 102850, loss = 0.67 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:40.432644: step 102860, loss = 0.63 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:40.752858: step 102870, loss = 0.60 (8160.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:41.071692: step 102880, loss = 0.55 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:41.391802: step 102890, loss = 0.67 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:41.711887: step 102900, loss = 0.50 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:42.167652: step 102910, loss = 0.55 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:42.489051: step 102920, loss = 0.49 (8104.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:42.808742: step 102930, loss = 0.51 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:43.132766: step 102940, loss = 0.54 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:43.452153: step 102950, loss = 0.52 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:43.774612: step 102960, loss = 0.71 (7784.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:44.095312: step 102970, loss = 0.46 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:44.415859: step 102980, loss = 0.45 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:44.736643: step 102990, loss = 0.73 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:45.055630: step 103000, loss = 0.72 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:45.600888: step 103010, loss = 0.53 (7688.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:37:45.921504: step 103020, loss = 0.63 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:46.245258: step 103030, loss = 0.57 (7474.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:37:46.566326: step 103040, loss = 0.62 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:46.888202: step 103050, loss = 0.54 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:47.211842: step 103060, loss = 0.61 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:47.532916: step 103070, loss = 0.59 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:47.853995: step 103080, loss = 0.47 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:48.174997: step 103090, loss = 0.55 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:48.497194: step 103100, loss = 0.67 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:48.953218: step 103110, loss = 0.51 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:49.277566: step 103120, loss = 0.63 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:49.603644: step 103130, loss = 0.59 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:49.924013: step 103140, loss = 0.55 (7774.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:50.243185: step 103150, loss = 0.55 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:50.564766: step 103160, loss = 0.44 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:50.887923: step 103170, loss = 0.49 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:51.209768: step 103180, loss = 0.56 (7898.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:51.535090: step 103190, loss = 0.53 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:51.858676: step 103200, loss = 0.48 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:52.318362: step 103210, loss = 0.68 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:52.641504: step 103220, loss = 0.56 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:52.964892: step 103230, loss = 0.52 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:53.286043: step 103240, loss = 0.60 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:53.610790: step 103250, loss = 0.61 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:53.931278: step 103260, loss = 0.56 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:54.252875: step 103270, loss = 0.49 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:54.574544: step 103280, loss = 0.53 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:54.895784: step 103290, loss = 0.52 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:55.217214: step 103300, loss = 0.47 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:55.680016: step 103310, loss = 0.56 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:56.000292: step 103320, loss = 0.54 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:56.321068: step 103330, loss = 0.45 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:56.640764: step 103340, loss = 0.60 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:56.961074: step 103350, loss = 0.53 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:57.283039: step 103360, loss = 0.76 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:57.602657: step 103370, loss = 0.58 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:57.922524: step 103380, loss = 0.64 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:58.241689: step 103390, loss = 0.56 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:58.562336: step 103400, loss = 0.51 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:59.024150: step 103410, loss = 0.54 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:59.345435: step 103420, loss = 0.63 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:59.664776: step 103430, loss = 0.54 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:37:59.985377: step 103440, loss = 0.61 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:00.304275: step 103450, loss = 0.48 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:00.622767: step 103460, loss = 0.54 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:00.944834: step 103470, loss = 0.52 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:01.265668: step 103480, loss = 0.53 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:01.585386: step 103490, loss = 0.46 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:01.906569: step 103500, loss = 0.55 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:02.361422: step 103510, loss = 0.43 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:02.680950: step 103520, loss = 0.65 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:03.004770: step 103530, loss = 0.56 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:03.325412: step 103540, loss = 0.68 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:03.647565: step 103550, loss = 0.56 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:03.970331: step 103560, loss = 0.59 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:04.291062: step 103570, loss = 0.51 (7882.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:04.611244: step 103580, loss = 0.51 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:04.931499: step 103590, loss = 0.44 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:05.252453: step 103600, loss = 0.60 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:05.706241: step 103610, loss = 0.60 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:06.026833: step 103620, loss = 0.68 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:06.347132: step 103630, loss = 0.55 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:06.666400: step 103640, loss = 0.43 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:06.987183: step 103650, loss = 0.51 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:07.318962: step 103660, loss = 0.53 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:07.638569: step 103670, loss = 0.63 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:07.958202: step 103680, loss = 0.52 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:08.277839: step 103690, loss = 0.57 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:08.597802: step 103700, loss = 0.55 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:09.060671: step 103710, loss = 0.59 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:09.382134: step 103720, loss = 0.57 (7963.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:09.702807: step 103730, loss = 0.51 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:10.025863: step 103740, loss = 0.47 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:10.344673: step 103750, loss = 0.50 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:10.667442: step 103760, loss = 0.52 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:10.993549: step 103770, loss = 0.53 (7460.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:11.313800: step 103780, loss = 0.48 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:11.635909: step 103790, loss = 0.50 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:11.956621: step 103800, loss = 0.64 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:12.417032: step 103810, loss = 0.59 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:12.739470: step 103820, loss = 0.54 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:13.058857: step 103830, loss = 0.49 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:13.379072: step 103840, loss = 0.54 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:13.703155: step 103850, loss = 0.54 (7603.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:14.026124: step 103860, loss = 0.51 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:14.349827: step 103870, loss = 0.64 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:14.672775: step 103880, loss = 0.61 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:14.998757: step 103890, loss = 0.52 (7972.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:15.318607: step 103900, loss = 0.57 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:15.777394: step 103910, loss = 0.58 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:16.096200: step 103920, loss = 0.48 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:16.418393: step 103930, loss = 0.49 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:16.737924: step 103940, loss = 0.51 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:17.057929: step 103950, loss = 0.63 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:17.379721: step 103960, loss = 0.60 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:17.700590: step 103970, loss = 0.50 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:18.021283: step 103980, loss = 0.62 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:18.341377: step 103990, loss = 0.58 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:18.662775: step 104000, loss = 0.58 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:19.267065: step 104010, loss = 0.68 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:19.587656: step 104020, loss = 0.62 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:19.907413: step 104030, loss = 0.66 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:20.228438: step 104040, loss = 0.49 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:20.551289: step 104050, loss = 0.53 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:20.871451: step 104060, loss = 0.49 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:21.193465: step 104070, loss = 0.48 (8150.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:21.514046: step 104080, loss = 0.70 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:21.836875: step 104090, loss = 0.39 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:22.157845: step 104100, loss = 0.62 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:22.612882: step 104110, loss = 0.64 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:22.932094: step 104120, loss = 0.57 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:23.255937: step 104130, loss = 0.54 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:23.578200: step 104140, loss = 0.40 (7895.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:23.898386: step 104150, loss = 0.58 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:24.221109: step 104160, loss = 0.51 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:24.541325: step 104170, loss = 0.58 (7800.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:24.864028: step 104180, loss = 0.71 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:25.184572: step 104190, loss = 0.63 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:25.514141: step 104200, loss = 0.61 (7432.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:25.972535: step 104210, loss = 0.60 (7824.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:26.293186: step 104220, loss = 0.46 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:26.612523: step 104230, loss = 0.54 (7935.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:26.933222: step 104240, loss = 0.59 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:27.256125: step 104250, loss = 0.62 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:27.577040: step 104260, loss = 0.58 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:27.897134: step 104270, loss = 0.64 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:28.222999: step 104280, loss = 0.51 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:28.543983: step 104290, loss = 0.54 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:28.864838: step 104300, loss = 0.50 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:29.324813: step 104310, loss = 0.48 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:29.645602: step 104320, loss = 0.58 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:29.966746: step 104330, loss = 0.61 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:30.293461: step 104340, loss = 0.68 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:30.614312: step 104350, loss = 0.34 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:30.935552: step 104360, loss = 0.59 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:31.259399: step 104370, loss = 0.47 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:31.579301: step 104380, loss = 0.58 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:31.903001: step 104390, loss = 0.60 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:32.223734: step 104400, loss = 0.53 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:32.685917: step 104410, loss = 0.57 (7836.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:33.009129: step 104420, loss = 0.72 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:33.334065: step 104430, loss = 0.56 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:33.658801: step 104440, loss = 0.55 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:33.980170: step 104450, loss = 0.64 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:34.305051: step 104460, loss = 0.72 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:34.625354: step 104470, loss = 0.57 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:34.947140: step 104480, loss = 0.55 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:35.266448: step 104490, loss = 0.57 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:35.585935: step 104500, loss = 0.51 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:36.042575: step 104510, loss = 0.52 (7608.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:36.364676: step 104520, loss = 0.57 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:36.686312: step 104530, loss = 0.62 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:37.008444: step 104540, loss = 0.54 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:37.329092: step 104550, loss = 0.53 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:37.651282: step 104560, loss = 0.49 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:37.973480: step 104570, loss = 0.52 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:38.292832: step 104580, loss = 0.60 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:38.613349: step 104590, loss = 0.57 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:38.933938: step 104600, loss = 0.58 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:39.388564: step 104610, loss = 0.58 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:39.707914: step 104620, loss = 0.62 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:40.027331: step 104630, loss = 0.53 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:40.346529: step 104640, loss = 0.51 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:40.668829: step 104650, loss = 0.51 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:40.987986: step 104660, loss = 0.63 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:41.312789: step 104670, loss = 0.62 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:41.633061: step 104680, loss = 0.59 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:41.956554: step 104690, loss = 0.62 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:42.277765: step 104700, loss = 0.52 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:42.725657: step 104710, loss = 0.58 (7841.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:43.046603: step 104720, loss = 0.65 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:43.366303: step 104730, loss = 0.58 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:43.686530: step 104740, loss = 0.56 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:44.008210: step 104750, loss = 0.60 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:44.330452: step 104760, loss = 0.53 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:44.649873: step 104770, loss = 0.54 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:44.970177: step 104780, loss = 0.50 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:45.292346: step 104790, loss = 0.52 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:45.614521: step 104800, loss = 0.54 (7837.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:46.075073: step 104810, loss = 0.50 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:46.396229: step 104820, loss = 0.51 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:46.716293: step 104830, loss = 0.57 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:47.037382: step 104840, loss = 0.61 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:47.359426: step 104850, loss = 0.67 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:47.680726: step 104860, loss = 0.67 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:48.001126: step 104870, loss = 0.57 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:48.320794: step 104880, loss = 0.42 (7873.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:48.643656: step 104890, loss = 0.56 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:48.964942: step 104900, loss = 0.56 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:49.415417: step 104910, loss = 0.59 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:49.736728: step 104920, loss = 0.51 (8130.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:50.056712: step 104930, loss = 0.62 (8097.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:50.379856: step 104940, loss = 0.62 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:50.702476: step 104950, loss = 0.57 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:51.025387: step 104960, loss = 0.61 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:51.343808: step 104970, loss = 0.53 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:51.668542: step 104980, loss = 0.43 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:51.991798: step 104990, loss = 0.55 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:52.317000: step 105000, loss = 0.59 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:52.867784: step 105010, loss = 0.73 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:53.188642: step 105020, loss = 0.49 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:53.508789: step 105030, loss = 0.53 (7814.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:53.831602: step 105040, loss = 0.54 (7746.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:54.153412: step 105050, loss = 0.51 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:54.476759: step 105060, loss = 0.50 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:54.797207: step 105070, loss = 0.56 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:55.119552: step 105080, loss = 0.55 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:55.438468: step 105090, loss = 0.51 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:55.760795: step 105100, loss = 0.65 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:56.222617: step 105110, loss = 0.67 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:56.542013: step 105120, loss = 0.53 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:56.865208: step 105130, loss = 0.64 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:57.183487: step 105140, loss = 0.69 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:57.509502: step 105150, loss = 0.54 (7839.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:57.832947: step 105160, loss = 0.60 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:58.154898: step 105170, loss = 0.59 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:58.476750: step 105180, loss = 0.60 (8041.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:58.798845: step 105190, loss = 0.63 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:59.123354: step 105200, loss = 0.62 (7545.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:38:59.586455: step 105210, loss = 0.62 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:38:59.907738: step 105220, loss = 0.58 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:00.229684: step 105230, loss = 0.76 (7964.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:00.552851: step 105240, loss = 0.77 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:00.873895: step 105250, loss = 0.59 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:01.198400: step 105260, loss = 0.52 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:01.521430: step 105270, loss = 0.59 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:01.844185: step 105280, loss = 0.58 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:02.165736: step 105290, loss = 0.63 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:02.486352: step 105300, loss = 0.62 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:02.943014: step 105310, loss = 0.59 (7768.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:03.263517: step 105320, loss = 0.62 (7837.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:03.586377: step 105330, loss = 0.57 (7884.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:03.911577: step 105340, loss = 0.52 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:04.230518: step 105350, loss = 0.61 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:04.549952: step 105360, loss = 0.58 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:04.870881: step 105370, loss = 0.58 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:05.190268: step 105380, loss = 0.53 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:05.514194: step 105390, loss = 0.52 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:05.832933: step 105400, loss = 0.71 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:06.292116: step 105410, loss = 0.58 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:06.611891: step 105420, loss = 0.51 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:06.931943: step 105430, loss = 0.64 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:07.252280: step 105440, loss = 0.52 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:07.571655: step 105450, loss = 0.48 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:07.893118: step 105460, loss = 0.66 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:08.212474: step 105470, loss = 0.50 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:08.535112: step 105480, loss = 0.74 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:08.857658: step 105490, loss = 0.54 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:09.178639: step 105500, loss = 0.52 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:09.625843: step 105510, loss = 0.67 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:09.950539: step 105520, loss = 0.52 (7633.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:10.273582: step 105530, loss = 0.56 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:10.593842: step 105540, loss = 0.60 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:10.914433: step 105550, loss = 0.66 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:11.233236: step 105560, loss = 0.50 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:11.554049: step 105570, loss = 0.66 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:11.875985: step 105580, loss = 0.58 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:12.199354: step 105590, loss = 0.73 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:12.522938: step 105600, loss = 0.59 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:12.979377: step 105610, loss = 0.65 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:13.300253: step 105620, loss = 0.65 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:13.619875: step 105630, loss = 0.58 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:13.941490: step 105640, loss = 0.64 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:14.261883: step 105650, loss = 0.68 (7687.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:14.586164: step 105660, loss = 0.71 (7543.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:14.906723: step 105670, loss = 0.62 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:15.227935: step 105680, loss = 0.57 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:15.547656: step 105690, loss = 0.54 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:15.869555: step 105700, loss = 0.65 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:16.326731: step 105710, loss = 0.58 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:16.648827: step 105720, loss = 0.69 (7773.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:16.968231: step 105730, loss = 0.51 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:17.288479: step 105740, loss = 0.56 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:17.610027: step 105750, loss = 0.52 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:17.929848: step 105760, loss = 0.64 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:18.249160: step 105770, loss = 0.66 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:18.569357: step 105780, loss = 0.64 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:18.887846: step 105790, loss = 0.55 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:19.206972: step 105800, loss = 0.53 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:19.667599: step 105810, loss = 0.44 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:19.989933: step 105820, loss = 0.48 (7664.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:20.314303: step 105830, loss = 0.50 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:20.634416: step 105840, loss = 0.56 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:20.955625: step 105850, loss = 0.59 (7840.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:21.276621: step 105860, loss = 0.51 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:21.599133: step 105870, loss = 0.55 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:21.920576: step 105880, loss = 0.69 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:22.241693: step 105890, loss = 0.54 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:22.562512: step 105900, loss = 0.56 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:23.025578: step 105910, loss = 0.61 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:23.350295: step 105920, loss = 0.63 (7819.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:23.672999: step 105930, loss = 0.67 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:23.993594: step 105940, loss = 0.63 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:24.313813: step 105950, loss = 0.46 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:24.632937: step 105960, loss = 0.49 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:24.951832: step 105970, loss = 0.53 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:25.272312: step 105980, loss = 0.57 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:25.590987: step 105990, loss = 0.65 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:25.910020: step 106000, loss = 0.68 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:26.452962: step 106010, loss = 0.58 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:26.776205: step 106020, loss = 0.52 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:27.099012: step 106030, loss = 0.62 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:27.421837: step 106040, loss = 0.49 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:27.744667: step 106050, loss = 0.69 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:28.065971: step 106060, loss = 0.56 (7892.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:28.386967: step 106070, loss = 0.44 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:28.706966: step 106080, loss = 0.53 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:29.027959: step 106090, loss = 0.55 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:29.347228: step 106100, loss = 0.58 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:29.815079: step 106110, loss = 0.54 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:30.134944: step 106120, loss = 0.73 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:30.456851: step 106130, loss = 0.57 (7462.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:30.777418: step 106140, loss = 0.63 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:31.098930: step 106150, loss = 0.43 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:31.420967: step 106160, loss = 0.62 (7822.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:31.739538: step 106170, loss = 0.67 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:32.068423: step 106180, loss = 0.52 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:32.393491: step 106190, loss = 0.47 (7567.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:39:32.716625: step 106200, loss = 0.68 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:33.180269: step 106210, loss = 0.46 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:33.500180: step 106220, loss = 0.56 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:33.818982: step 106230, loss = 0.47 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:34.141393: step 106240, loss = 0.67 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:34.462560: step 106250, loss = 0.58 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:34.781441: step 106260, loss = 0.56 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:35.100850: step 106270, loss = 0.53 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:35.421600: step 106280, loss = 0.44 (8156.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:35.740951: step 106290, loss = 0.67 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:36.061575: step 106300, loss = 0.55 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:36.528221: step 106310, loss = 0.52 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:36.849512: step 106320, loss = 0.60 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:37.170388: step 106330, loss = 0.74 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:37.489696: step 106340, loss = 0.49 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:37.810053: step 106350, loss = 0.49 (8085.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:38.128661: step 106360, loss = 0.65 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:38.449050: step 106370, loss = 0.53 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:38.767247: step 106380, loss = 0.59 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:39.085988: step 106390, loss = 0.46 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:39.406800: step 106400, loss = 0.55 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:39.865108: step 106410, loss = 0.45 (7963.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:40.192135: step 106420, loss = 0.65 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:40.516474: step 106430, loss = 0.56 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:40.838502: step 106440, loss = 0.50 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:41.160865: step 106450, loss = 0.64 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:41.480745: step 106460, loss = 0.59 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:41.803142: step 106470, loss = 0.65 (8147.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:42.123576: step 106480, loss = 0.52 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:42.444195: step 106490, loss = 0.55 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:42.764900: step 106500, loss = 0.68 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:43.216102: step 106510, loss = 0.43 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:43.536987: step 106520, loss = 0.53 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:43.855346: step 106530, loss = 0.52 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:44.176149: step 106540, loss = 0.58 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:44.496570: step 106550, loss = 0.58 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:44.817655: step 106560, loss = 0.75 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:45.139411: step 106570, loss = 0.48 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:45.459849: step 106580, loss = 0.58 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:45.781670: step 106590, loss = 0.56 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:46.100998: step 106600, loss = 0.52 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:46.564448: step 106610, loss = 0.62 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:46.884190: step 106620, loss = 0.63 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:47.204478: step 106630, loss = 0.54 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:47.523627: step 106640, loss = 0.63 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:47.846012: step 106650, loss = 0.55 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:48.165452: step 106660, loss = 0.62 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:48.485950: step 106670, loss = 0.68 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:48.804669: step 106680, loss = 0.54 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:49.125613: step 106690, loss = 0.50 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:49.448667: step 106700, loss = 0.59 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:49.906620: step 106710, loss = 0.51 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:50.229767: step 106720, loss = 0.72 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:50.551452: step 106730, loss = 0.47 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:50.871482: step 106740, loss = 0.64 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:51.193454: step 106750, loss = 0.61 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:51.519373: step 106760, loss = 0.55 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:51.843457: step 106770, loss = 0.55 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:52.164163: step 106780, loss = 0.63 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:52.486598: step 106790, loss = 0.54 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:52.808140: step 106800, loss = 0.58 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:53.266899: step 106810, loss = 0.47 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:53.588623: step 106820, loss = 0.55 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:53.912919: step 106830, loss = 0.53 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:54.234719: step 106840, loss = 0.53 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:54.554441: step 106850, loss = 0.81 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:54.876024: step 106860, loss = 0.51 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:55.196291: step 106870, loss = 0.66 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:55.521740: step 106880, loss = 0.62 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:55.841402: step 106890, loss = 0.44 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:56.162743: step 106900, loss = 0.59 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:56.616465: step 106910, loss = 0.61 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:56.937902: step 106920, loss = 0.53 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:57.257228: step 106930, loss = 0.53 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:57.575567: step 106940, loss = 0.49 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:57.897519: step 106950, loss = 0.51 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:58.220282: step 106960, loss = 0.69 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:58.540817: step 106970, loss = 0.66 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:58.864404: step 106980, loss = 0.53 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:59.186212: step 106990, loss = 0.71 (7776.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:39:59.507907: step 107000, loss = 0.60 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:00.062784: step 107010, loss = 0.50 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:00.381590: step 107020, loss = 0.43 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:00.701489: step 107030, loss = 0.59 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:01.023649: step 107040, loss = 0.67 (7784.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:01.347342: step 107050, loss = 0.51 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:01.668873: step 107060, loss = 0.57 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:01.989756: step 107070, loss = 0.65 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:02.309139: step 107080, loss = 0.49 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:02.632533: step 107090, loss = 0.66 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:02.953633: step 107100, loss = 0.61 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:03.401564: step 107110, loss = 0.52 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:03.721958: step 107120, loss = 0.71 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:04.044045: step 107130, loss = 0.65 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:04.363328: step 107140, loss = 0.58 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:04.686489: step 107150, loss = 0.62 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:05.006409: step 107160, loss = 0.64 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:05.326673: step 107170, loss = 0.58 (7956.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:05.647785: step 107180, loss = 0.49 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:05.968007: step 107190, loss = 0.67 (8130.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:06.286919: step 107200, loss = 0.61 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:06.748107: step 107210, loss = 0.56 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:07.067673: step 107220, loss = 0.61 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:07.386563: step 107230, loss = 0.48 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:07.704706: step 107240, loss = 0.48 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:08.023850: step 107250, loss = 0.58 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:08.343670: step 107260, loss = 0.51 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:08.664283: step 107270, loss = 0.56 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:08.986529: step 107280, loss = 0.54 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:09.310811: step 107290, loss = 0.53 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:09.631711: step 107300, loss = 0.53 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:10.088886: step 107310, loss = 0.55 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:10.410083: step 107320, loss = 0.58 (7689.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:40:10.728136: step 107330, loss = 0.65 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:11.047745: step 107340, loss = 0.66 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:11.369095: step 107350, loss = 0.55 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:11.689218: step 107360, loss = 0.52 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:12.012131: step 107370, loss = 0.62 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:12.336524: step 107380, loss = 0.71 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:12.655442: step 107390, loss = 0.57 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:12.977111: step 107400, loss = 0.80 (7964.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:13.434621: step 107410, loss = 0.59 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:13.756182: step 107420, loss = 0.53 (7809.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:14.077854: step 107430, loss = 0.46 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:14.396774: step 107440, loss = 0.64 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:14.717754: step 107450, loss = 0.51 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:15.037878: step 107460, loss = 0.52 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:15.357917: step 107470, loss = 0.60 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:15.677858: step 107480, loss = 0.65 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:15.996708: step 107490, loss = 0.54 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:16.317097: step 107500, loss = 0.53 (7773.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:16.776699: step 107510, loss = 0.61 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:17.097425: step 107520, loss = 0.54 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:17.418387: step 107530, loss = 0.61 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:17.740024: step 107540, loss = 0.59 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:18.061817: step 107550, loss = 0.57 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:18.381860: step 107560, loss = 0.64 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:18.702438: step 107570, loss = 0.61 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:19.024734: step 107580, loss = 0.58 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:19.343460: step 107590, loss = 0.50 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:19.663751: step 107600, loss = 0.68 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:20.124939: step 107610, loss = 0.53 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:20.445391: step 107620, loss = 0.52 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:20.768166: step 107630, loss = 0.46 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:21.092855: step 107640, loss = 0.50 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:21.415026: step 107650, loss = 0.53 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:21.736824: step 107660, loss = 0.75 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:22.056702: step 107670, loss = 0.49 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:22.379658: step 107680, loss = 0.47 (7781.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:22.700719: step 107690, loss = 0.55 (8062.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:23.021445: step 107700, loss = 0.68 (7801.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:23.483484: step 107710, loss = 0.71 (7805.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:23.802925: step 107720, loss = 0.55 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:24.126814: step 107730, loss = 0.56 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:24.448668: step 107740, loss = 0.56 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:24.767941: step 107750, loss = 0.52 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:25.087083: step 107760, loss = 0.54 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:25.408864: step 107770, loss = 0.58 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:25.731288: step 107780, loss = 0.63 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:26.052384: step 107790, loss = 0.54 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:26.373586: step 107800, loss = 0.64 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:26.827430: step 107810, loss = 0.55 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:27.148413: step 107820, loss = 0.61 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:27.475585: step 107830, loss = 0.63 (7134.9 examples/sec; 0.018 sec/batch)
2017-09-16 16:40:27.797278: step 107840, loss = 0.64 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:28.120793: step 107850, loss = 0.54 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:28.443825: step 107860, loss = 0.54 (8006.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:28.762911: step 107870, loss = 0.65 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:29.084723: step 107880, loss = 0.57 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:29.409534: step 107890, loss = 0.55 (7607.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:40:29.729684: step 107900, loss = 0.56 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:30.184734: step 107910, loss = 0.53 (7940.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:30.507117: step 107920, loss = 0.67 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:30.827925: step 107930, loss = 0.42 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:31.148393: step 107940, loss = 0.58 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:31.470521: step 107950, loss = 0.59 (7882.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:31.792437: step 107960, loss = 0.69 (7623.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:40:32.111400: step 107970, loss = 0.51 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:32.435038: step 107980, loss = 0.49 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:32.754211: step 107990, loss = 0.49 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:33.076706: step 108000, loss = 0.54 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:33.628956: step 108010, loss = 0.65 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:33.951783: step 108020, loss = 0.51 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:34.272778: step 108030, loss = 0.58 (7890.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:34.593940: step 108040, loss = 0.65 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:34.920528: step 108050, loss = 0.58 (7441.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:40:35.243935: step 108060, loss = 0.61 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:35.563341: step 108070, loss = 0.49 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:35.885673: step 108080, loss = 0.60 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:36.206781: step 108090, loss = 0.53 (7899.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:36.526621: step 108100, loss = 0.54 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:36.986531: step 108110, loss = 0.47 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:37.309076: step 108120, loss = 0.69 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:37.630195: step 108130, loss = 0.57 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:37.949683: step 108140, loss = 0.60 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:38.267645: step 108150, loss = 0.47 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:38.585603: step 108160, loss = 0.71 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:38.906636: step 108170, loss = 0.64 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:39.228747: step 108180, loss = 0.55 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:39.551430: step 108190, loss = 0.64 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:39.870615: step 108200, loss = 0.54 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:40.324501: step 108210, loss = 0.63 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:40.647415: step 108220, loss = 0.52 (7492.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:40:40.967319: step 108230, loss = 0.50 (8141.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:41.285827: step 108240, loss = 0.46 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:41.605829: step 108250, loss = 0.58 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:41.926416: step 108260, loss = 0.59 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:42.246733: step 108270, loss = 0.55 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:42.568390: step 108280, loss = 0.52 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:42.888693: step 108290, loss = 0.54 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:43.209376: step 108300, loss = 0.55 (7781.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:43.667199: step 108310, loss = 0.52 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:43.988643: step 108320, loss = 0.66 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:44.307932: step 108330, loss = 0.67 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:44.628227: step 108340, loss = 0.49 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:44.948807: step 108350, loss = 0.49 (7795.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:45.271302: step 108360, loss = 0.59 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:45.591563: step 108370, loss = 0.54 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:45.910318: step 108380, loss = 0.59 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:46.229324: step 108390, loss = 0.53 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:46.548963: step 108400, loss = 0.52 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:47.011893: step 108410, loss = 0.56 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:47.332358: step 108420, loss = 0.54 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:47.651106: step 108430, loss = 0.55 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:47.972921: step 108440, loss = 0.56 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:48.293296: step 108450, loss = 0.52 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:48.612986: step 108460, loss = 0.54 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:48.933651: step 108470, loss = 0.56 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:49.255659: step 108480, loss = 0.57 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:49.576387: step 108490, loss = 0.85 (7955.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:49.894613: step 108500, loss = 0.47 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:50.355165: step 108510, loss = 0.58 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:50.677398: step 108520, loss = 0.64 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:50.999403: step 108530, loss = 0.56 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:51.318467: step 108540, loss = 0.66 (7952.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:51.639057: step 108550, loss = 0.54 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:51.959855: step 108560, loss = 0.41 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:52.281259: step 108570, loss = 0.64 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:52.602903: step 108580, loss = 0.49 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:52.926384: step 108590, loss = 0.62 (7818.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:53.248588: step 108600, loss = 0.54 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:53.710143: step 108610, loss = 0.49 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:54.031391: step 108620, loss = 0.70 (7939.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:54.352122: step 108630, loss = 0.48 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:54.671299: step 108640, loss = 0.50 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:54.993159: step 108650, loss = 0.59 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:55.312885: step 108660, loss = 0.76 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:55.632428: step 108670, loss = 0.58 (8165.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:55.952201: step 108680, loss = 0.59 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:56.273313: step 108690, loss = 0.55 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:56.597683: step 108700, loss = 0.56 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:57.053870: step 108710, loss = 0.62 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:57.372445: step 108720, loss = 0.60 (8129.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:57.692690: step 108730, loss = 0.51 (7856.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:58.014137: step 108740, loss = 0.61 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:58.335661: step 108750, loss = 0.55 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:58.657865: step 108760, loss = 0.48 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:58.977895: step 108770, loss = 0.72 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:59.298741: step 108780, loss = 0.57 (7813.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:59.618742: step 108790, loss = 0.52 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:40:59.937506: step 108800, loss = 0.71 (8156.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:00.392719: step 108810, loss = 0.68 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:00.711178: step 108820, loss = 0.52 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:01.031107: step 108830, loss = 0.59 (8170.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:01.349380: step 108840, loss = 0.52 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:01.668958: step 108850, loss = 0.49 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:01.988073: step 108860, loss = 0.60 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:02.308706: step 108870, loss = 0.58 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:02.631348: step 108880, loss = 0.51 (7812.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:02.949948: step 108890, loss = 0.47 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:03.272266: step 108900, loss = 0.51 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:03.725039: step 108910, loss = 0.50 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:04.044468: step 108920, loss = 0.49 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:04.364121: step 108930, loss = 0.56 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:04.683573: step 108940, loss = 0.45 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:05.005144: step 108950, loss = 0.56 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:05.325401: step 108960, loss = 0.48 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:05.644804: step 108970, loss = 0.59 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:05.965240: step 108980, loss = 0.52 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:06.286370: step 108990, loss = 0.56 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:06.609273: step 109000, loss = 0.70 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:07.157309: step 109010, loss = 0.54 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:07.477968: step 109020, loss = 0.64 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:07.800929: step 109030, loss = 0.56 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:08.119785: step 109040, loss = 0.63 (8014.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:08.443927: step 109050, loss = 0.61 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:08.764294: step 109060, loss = 0.62 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:09.086495: step 109070, loss = 0.58 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:09.407728: step 109080, loss = 0.49 (7955.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:09.726800: step 109090, loss = 0.60 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:10.046275: step 109100, loss = 0.48 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:10.501201: step 109110, loss = 0.55 (7850.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:10.822542: step 109120, loss = 0.58 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:11.144725: step 109130, loss = 0.61 (7926.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:11.466207: step 109140, loss = 0.55 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:11.788659: step 109150, loss = 0.52 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:12.109972: step 109160, loss = 0.60 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:12.430324: step 109170, loss = 0.57 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:12.750560: step 109180, loss = 0.66 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:13.072200: step 109190, loss = 0.63 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:13.393740: step 109200, loss = 0.52 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:13.856563: step 109210, loss = 0.50 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:14.177554: step 109220, loss = 0.64 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:14.496224: step 109230, loss = 0.52 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:14.818977: step 109240, loss = 0.60 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:15.139609: step 109250, loss = 0.55 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:15.460161: step 109260, loss = 0.55 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:15.779738: step 109270, loss = 0.52 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:16.101637: step 109280, loss = 0.58 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:16.421372: step 109290, loss = 0.67 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:16.742302: step 109300, loss = 0.42 (7838.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:17.207300: step 109310, loss = 0.67 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:17.531710: step 109320, loss = 0.58 (7467.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:41:17.852908: step 109330, loss = 0.50 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:18.173315: step 109340, loss = 0.46 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:18.493894: step 109350, loss = 0.61 (7827.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:18.813165: step 109360, loss = 0.59 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:19.131920: step 109370, loss = 0.71 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:19.454178: step 109380, loss = 0.58 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:19.777829: step 109390, loss = 0.64 (7989.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:20.098023: step 109400, loss = 0.55 (8103.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:20.552548: step 109410, loss = 0.54 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:20.874454: step 109420, loss = 0.46 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:21.192848: step 109430, loss = 0.40 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:21.514122: step 109440, loss = 0.39 (7819.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:21.833778: step 109450, loss = 0.59 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:22.152847: step 109460, loss = 0.50 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:22.472208: step 109470, loss = 0.60 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:22.793515: step 109480, loss = 0.61 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:23.113515: step 109490, loss = 0.59 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:23.434047: step 109500, loss = 0.68 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:23.892601: step 109510, loss = 0.53 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:24.212186: step 109520, loss = 0.58 (7877.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:24.533384: step 109530, loss = 0.58 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:24.853803: step 109540, loss = 0.55 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:25.173142: step 109550, loss = 0.51 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:25.492967: step 109560, loss = 0.52 (7933.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:25.812182: step 109570, loss = 0.64 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:26.132450: step 109580, loss = 0.68 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:26.450808: step 109590, loss = 0.44 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:26.771038: step 109600, loss = 0.52 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:27.236513: step 109610, loss = 0.56 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:27.558960: step 109620, loss = 0.61 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:27.879758: step 109630, loss = 0.68 (8104.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:28.200772: step 109640, loss = 0.51 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:28.521985: step 109650, loss = 0.49 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:28.841587: step 109660, loss = 0.56 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:29.161642: step 109670, loss = 0.64 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:29.482142: step 109680, loss = 0.48 (7704.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:41:29.802709: step 109690, loss = 0.67 (8027.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:30.124694: step 109700, loss = 0.54 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:30.577524: step 109710, loss = 0.59 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:30.900012: step 109720, loss = 0.53 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:31.219892: step 109730, loss = 0.63 (7955.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:31.541299: step 109740, loss = 0.55 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:31.864883: step 109750, loss = 0.55 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:32.186230: step 109760, loss = 0.57 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:32.506147: step 109770, loss = 0.64 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:32.828523: step 109780, loss = 0.53 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:33.147764: step 109790, loss = 0.50 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:33.469239: step 109800, loss = 0.62 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:33.928997: step 109810, loss = 0.58 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:34.250142: step 109820, loss = 0.42 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:34.571430: step 109830, loss = 0.67 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:34.892598: step 109840, loss = 0.55 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:35.213117: step 109850, loss = 0.58 (8020.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:35.534575: step 109860, loss = 0.53 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:35.854480: step 109870, loss = 0.52 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:36.173744: step 109880, loss = 0.50 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:36.493296: step 109890, loss = 0.52 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:36.810565: step 109900, loss = 0.66 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:37.271427: step 109910, loss = 0.67 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:37.592741: step 109920, loss = 0.71 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:37.913346: step 109930, loss = 0.67 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:38.232783: step 109940, loss = 0.46 (8156.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:38.553118: step 109950, loss = 0.42 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:38.873289: step 109960, loss = 0.56 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:39.192326: step 109970, loss = 0.53 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:39.513119: step 109980, loss = 0.43 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:39.833274: step 109990, loss = 0.57 (7862.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:40.152960: step 110000, loss = 0.53 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:40.696166: step 110010, loss = 0.57 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:41.014844: step 110020, loss = 0.69 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:41.334395: step 110030, loss = 0.67 (8160.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:41.654699: step 110040, loss = 0.58 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:41.974480: step 110050, loss = 0.61 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:42.293186: step 110060, loss = 0.47 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:42.614252: step 110070, loss = 0.64 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:42.934358: step 110080, loss = 0.55 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:43.252189: step 110090, loss = 0.63 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:43.572596: step 110100, loss = 0.71 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:44.021718: step 110110, loss = 0.57 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:44.340943: step 110120, loss = 0.56 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:44.661335: step 110130, loss = 0.48 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:44.981696: step 110140, loss = 0.59 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:45.305859: step 110150, loss = 0.50 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:45.627869: step 110160, loss = 0.62 (7889.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:45.947485: step 110170, loss = 0.52 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:46.267051: step 110180, loss = 0.62 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:46.585500: step 110190, loss = 0.60 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:46.906198: step 110200, loss = 0.76 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:47.368359: step 110210, loss = 0.48 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:47.690619: step 110220, loss = 0.53 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:48.010273: step 110230, loss = 0.63 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:48.333600: step 110240, loss = 0.58 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:48.657026: step 110250, loss = 0.55 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:48.977194: step 110260, loss = 0.58 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:49.298160: step 110270, loss = 0.56 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:49.616502: step 110280, loss = 0.55 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:49.936181: step 110290, loss = 0.57 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:50.255891: step 110300, loss = 0.53 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:50.713056: step 110310, loss = 0.62 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:51.030789: step 110320, loss = 0.57 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:51.353881: step 110330, loss = 0.58 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:51.675086: step 110340, loss = 0.57 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:51.993644: step 110350, loss = 0.62 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:52.313708: step 110360, loss = 0.63 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:52.632694: step 110370, loss = 0.57 (8120.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:52.953083: step 110380, loss = 0.63 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:53.273381: step 110390, loss = 0.53 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:53.591475: step 110400, loss = 0.56 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:54.045401: step 110410, loss = 0.59 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:54.364223: step 110420, loss = 0.50 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:54.683020: step 110430, loss = 0.61 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:55.002098: step 110440, loss = 0.67 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:55.320218: step 110450, loss = 0.56 (7783.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:55.641492: step 110460, loss = 0.51 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:55.963346: step 110470, loss = 0.59 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:56.285052: step 110480, loss = 0.52 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:56.607022: step 110490, loss = 0.48 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:56.926201: step 110500, loss = 0.61 (8082.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:57.381997: step 110510, loss = 0.57 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:57.703888: step 110520, loss = 0.53 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:58.022879: step 110530, loss = 0.63 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:58.343927: step 110540, loss = 0.50 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:58.663726: step 110550, loss = 0.54 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:58.982463: step 110560, loss = 0.58 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:59.305463: step 110570, loss = 0.61 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:59.624773: step 110580, loss = 0.55 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:41:59.946150: step 110590, loss = 0.60 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:00.265729: step 110600, loss = 0.58 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:00.727110: step 110610, loss = 0.66 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:01.047387: step 110620, loss = 0.53 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:01.367121: step 110630, loss = 0.48 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:01.685419: step 110640, loss = 0.60 (8187.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:02.009149: step 110650, loss = 0.46 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:02.329247: step 110660, loss = 0.49 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:02.650953: step 110670, loss = 0.68 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:02.973973: step 110680, loss = 0.51 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:03.292964: step 110690, loss = 0.54 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:03.614897: step 110700, loss = 0.50 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:04.069781: step 110710, loss = 0.72 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:04.393318: step 110720, loss = 0.57 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:04.714333: step 110730, loss = 0.52 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:05.035556: step 110740, loss = 0.56 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:05.354731: step 110750, loss = 0.56 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:05.674368: step 110760, loss = 0.63 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:05.994743: step 110770, loss = 0.51 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:06.313733: step 110780, loss = 0.63 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:06.636832: step 110790, loss = 0.60 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:06.954928: step 110800, loss = 0.69 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:07.413945: step 110810, loss = 0.54 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:07.732151: step 110820, loss = 0.56 (8146.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:08.050745: step 110830, loss = 0.55 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:08.373514: step 110840, loss = 0.58 (7686.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:08.692311: step 110850, loss = 0.49 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:09.014351: step 110860, loss = 0.61 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:09.333115: step 110870, loss = 0.48 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:09.652816: step 110880, loss = 0.58 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:09.974870: step 110890, loss = 0.80 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:10.294933: step 110900, loss = 0.78 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:10.751335: step 110910, loss = 0.69 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:11.074097: step 110920, loss = 0.56 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:11.394323: step 110930, loss = 0.51 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:11.714936: step 110940, loss = 0.64 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:12.039092: step 110950, loss = 0.54 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:12.359984: step 110960, loss = 0.61 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:12.680338: step 110970, loss = 0.80 (8101.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:13.003385: step 110980, loss = 0.43 (7774.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:13.323295: step 110990, loss = 0.81 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:13.644674: step 111000, loss = 0.62 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:14.207686: step 111010, loss = 0.43 (7808.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:14.529577: step 111020, loss = 0.54 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:14.848634: step 111030, loss = 0.60 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:15.172025: step 111040, loss = 0.41 (7518.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:15.493890: step 111050, loss = 0.49 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:15.813615: step 111060, loss = 0.50 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:16.133961: step 111070, loss = 0.55 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:16.452548: step 111080, loss = 0.53 (8150.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:16.772976: step 111090, loss = 0.52 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:17.092893: step 111100, loss = 0.63 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:17.549711: step 111110, loss = 0.68 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:17.868358: step 111120, loss = 0.55 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:18.190038: step 111130, loss = 0.62 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:18.513485: step 111140, loss = 0.49 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:18.833973: step 111150, loss = 0.49 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:19.156109: step 111160, loss = 0.50 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:19.474563: step 111170, loss = 0.40 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:19.794474: step 111180, loss = 0.59 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:20.114454: step 111190, loss = 0.65 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:20.435258: step 111200, loss = 0.63 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:20.899165: step 111210, loss = 0.54 (7574.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:21.222598: step 111220, loss = 0.45 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:21.542452: step 111230, loss = 0.61 (8078.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:21.863469: step 111240, loss = 0.61 (8141.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:22.182842: step 111250, loss = 0.46 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:22.503375: step 111260, loss = 0.68 (7845.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:22.823094: step 111270, loss = 0.51 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:23.143129: step 111280, loss = 0.63 (8107.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:23.464675: step 111290, loss = 0.74 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:23.786848: step 111300, loss = 0.57 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:24.238575: step 111310, loss = 0.43 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:24.559501: step 111320, loss = 0.56 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:24.880614: step 111330, loss = 0.56 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:25.200712: step 111340, loss = 0.46 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:25.520297: step 111350, loss = 0.57 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:25.842097: step 111360, loss = 0.60 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:26.163790: step 111370, loss = 0.59 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:26.484143: step 111380, loss = 0.69 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:26.803685: step 111390, loss = 0.55 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:27.124333: step 111400, loss = 0.57 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:27.570362: step 111410, loss = 0.60 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:27.888564: step 111420, loss = 0.47 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:28.209164: step 111430, loss = 0.49 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:28.528177: step 111440, loss = 0.70 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:28.848070: step 111450, loss = 0.49 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:29.168019: step 111460, loss = 0.60 (8142.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:29.488343: step 111470, loss = 0.74 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:29.806166: step 111480, loss = 0.61 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:30.125960: step 111490, loss = 0.55 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:30.445065: step 111500, loss = 0.56 (8159.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:30.904125: step 111510, loss = 0.72 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:31.224153: step 111520, loss = 0.62 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:31.543508: step 111530, loss = 0.58 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:31.864549: step 111540, loss = 0.62 (7773.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:32.183934: step 111550, loss = 0.55 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:32.508177: step 111560, loss = 0.57 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:32.827656: step 111570, loss = 0.53 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:33.148239: step 111580, loss = 0.54 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:33.469327: step 111590, loss = 0.70 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:33.790195: step 111600, loss = 0.65 (7925.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:34.250480: step 111610, loss = 0.53 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:34.569995: step 111620, loss = 0.62 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:34.890386: step 111630, loss = 0.57 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:35.213668: step 111640, loss = 0.62 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:35.539384: step 111650, loss = 0.46 (8033.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:35.860830: step 111660, loss = 0.57 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:36.187699: step 111670, loss = 0.56 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:36.506800: step 111680, loss = 0.61 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:36.826867: step 111690, loss = 0.53 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:37.145758: step 111700, loss = 0.53 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:37.611920: step 111710, loss = 0.60 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:37.937244: step 111720, loss = 0.52 (7538.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:38.259661: step 111730, loss = 0.52 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:38.582344: step 111740, loss = 0.54 (7828.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:38.902836: step 111750, loss = 0.62 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:39.224422: step 111760, loss = 0.59 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:39.542648: step 111770, loss = 0.69 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:39.862441: step 111780, loss = 0.51 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:40.182436: step 111790, loss = 0.53 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:40.503112: step 111800, loss = 0.56 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:40.967257: step 111810, loss = 0.48 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:41.287472: step 111820, loss = 0.67 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:41.607278: step 111830, loss = 0.56 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:41.928635: step 111840, loss = 0.57 (7519.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:42.250072: step 111850, loss = 0.56 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:42.568291: step 111860, loss = 0.48 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:42.888670: step 111870, loss = 0.67 (7795.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:43.209909: step 111880, loss = 0.59 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:43.529643: step 111890, loss = 0.59 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:43.852774: step 111900, loss = 0.46 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:44.305977: step 111910, loss = 0.55 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:44.631140: step 111920, loss = 0.61 (7572.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:44.953769: step 111930, loss = 0.70 (7977.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:45.274145: step 111940, loss = 0.53 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:45.595353: step 111950, loss = 0.56 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:45.916276: step 111960, loss = 0.59 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:46.235523: step 111970, loss = 0.55 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:46.559763: step 111980, loss = 0.60 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:46.880453: step 111990, loss = 0.58 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:47.205573: step 112000, loss = 0.57 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:47.802048: step 112010, loss = 0.55 (7753.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:42:48.123654: step 112020, loss = 0.54 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:48.443322: step 112030, loss = 0.71 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:48.763876: step 112040, loss = 0.63 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:49.085661: step 112050, loss = 0.58 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:49.406874: step 112060, loss = 0.61 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:49.729879: step 112070, loss = 0.55 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:50.052502: step 112080, loss = 0.52 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:50.372523: step 112090, loss = 0.47 (7860.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:50.698644: step 112100, loss = 0.47 (7802.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:51.159506: step 112110, loss = 0.50 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:51.480828: step 112120, loss = 0.60 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:51.802777: step 112130, loss = 0.55 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:52.125521: step 112140, loss = 0.58 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:52.446549: step 112150, loss = 0.64 (7827.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:52.770533: step 112160, loss = 0.64 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:53.092164: step 112170, loss = 0.58 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:53.412467: step 112180, loss = 0.54 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:53.733588: step 112190, loss = 0.51 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:54.053163: step 112200, loss = 0.67 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:54.508371: step 112210, loss = 0.40 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:54.830013: step 112220, loss = 0.63 (7932.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:55.150592: step 112230, loss = 0.49 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:55.470585: step 112240, loss = 0.50 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:55.792132: step 112250, loss = 0.51 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:56.111215: step 112260, loss = 0.67 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:56.431395: step 112270, loss = 0.66 (7777.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:56.752840: step 112280, loss = 0.64 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:57.072240: step 112290, loss = 0.53 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:57.393410: step 112300, loss = 0.52 (7836.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:57.848788: step 112310, loss = 0.57 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:58.169459: step 112320, loss = 0.46 (7777.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:58.487039: step 112330, loss = 0.57 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:58.807691: step 112340, loss = 0.48 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:59.128380: step 112350, loss = 0.63 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:59.448584: step 112360, loss = 0.63 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:42:59.767234: step 112370, loss = 0.44 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:00.089416: step 112380, loss = 0.73 (7938.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:00.410751: step 112390, loss = 0.56 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:00.731106: step 112400, loss = 0.55 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:01.197803: step 112410, loss = 0.51 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:01.517463: step 112420, loss = 0.65 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:01.838490: step 112430, loss = 0.64 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:02.161158: step 112440, loss = 0.56 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:02.483580: step 112450, loss = 0.50 (7806.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:02.806174: step 112460, loss = 0.70 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:03.125879: step 112470, loss = 0.61 (7910.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:03.447041: step 112480, loss = 0.60 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:03.769201: step 112490, loss = 0.52 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:04.089314: step 112500, loss = 0.56 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:04.541981: step 112510, loss = 0.39 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:04.862148: step 112520, loss = 0.61 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:05.181373: step 112530, loss = 0.61 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:05.500470: step 112540, loss = 0.64 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:05.820878: step 112550, loss = 0.58 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:06.142321: step 112560, loss = 0.44 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:06.463385: step 112570, loss = 0.54 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:06.784299: step 112580, loss = 0.52 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:07.103503: step 112590, loss = 0.47 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:07.424772: step 112600, loss = 0.61 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:07.872635: step 112610, loss = 0.55 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:08.192227: step 112620, loss = 0.54 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:08.517668: step 112630, loss = 0.60 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:08.841309: step 112640, loss = 0.63 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:09.161590: step 112650, loss = 0.60 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:09.483508: step 112660, loss = 0.61 (8120.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:09.805526: step 112670, loss = 0.61 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:10.126457: step 112680, loss = 0.63 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:10.448054: step 112690, loss = 0.59 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:10.767858: step 112700, loss = 0.77 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:11.227028: step 112710, loss = 0.48 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:11.550169: step 112720, loss = 0.45 (7911.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:11.871375: step 112730, loss = 0.53 (7788.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:12.193454: step 112740, loss = 0.52 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:12.512273: step 112750, loss = 0.51 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:12.830617: step 112760, loss = 0.53 (8096.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:13.151536: step 112770, loss = 0.66 (7635.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:43:13.475209: step 112780, loss = 0.64 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:13.797526: step 112790, loss = 0.54 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:14.117555: step 112800, loss = 0.69 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:14.571276: step 112810, loss = 0.52 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:14.891624: step 112820, loss = 0.56 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:15.211553: step 112830, loss = 0.53 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:15.532196: step 112840, loss = 0.60 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:15.851470: step 112850, loss = 0.57 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:16.176217: step 112860, loss = 0.55 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:16.495898: step 112870, loss = 0.48 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:16.822085: step 112880, loss = 0.51 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:17.142400: step 112890, loss = 0.59 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:17.461383: step 112900, loss = 0.45 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:17.912119: step 112910, loss = 0.54 (7916.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:18.230959: step 112920, loss = 0.58 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:18.556565: step 112930, loss = 0.53 (7743.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:43:18.877780: step 112940, loss = 0.54 (8162.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:19.196704: step 112950, loss = 0.57 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:19.517244: step 112960, loss = 0.59 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:19.840284: step 112970, loss = 0.62 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:20.161089: step 112980, loss = 0.65 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:20.480684: step 112990, loss = 0.61 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:20.802037: step 113000, loss = 0.57 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:21.356657: step 113010, loss = 0.57 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:21.675902: step 113020, loss = 0.55 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:21.995898: step 113030, loss = 0.56 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:22.319909: step 113040, loss = 0.57 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:22.643943: step 113050, loss = 0.57 (8077.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:22.964150: step 113060, loss = 0.57 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:23.286986: step 113070, loss = 0.52 (7808.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:23.607077: step 113080, loss = 0.51 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:23.929003: step 113090, loss = 0.60 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:24.249768: step 113100, loss = 0.55 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:24.706945: step 113110, loss = 0.60 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:25.031716: step 113120, loss = 0.83 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:25.352555: step 113130, loss = 0.58 (7823.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:25.672886: step 113140, loss = 0.51 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:25.995215: step 113150, loss = 0.47 (7803.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:26.315474: step 113160, loss = 0.51 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:26.634954: step 113170, loss = 0.42 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:26.955681: step 113180, loss = 0.56 (7856.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:27.281423: step 113190, loss = 0.64 (7947.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:27.603516: step 113200, loss = 0.58 (8007.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:28.065093: step 113210, loss = 0.58 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:28.386669: step 113220, loss = 0.62 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:28.706535: step 113230, loss = 0.56 (8057.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:29.028709: step 113240, loss = 0.49 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:29.350030: step 113250, loss = 0.64 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:29.670454: step 113260, loss = 0.57 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:29.992049: step 113270, loss = 0.58 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:30.310920: step 113280, loss = 0.60 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:30.632205: step 113290, loss = 0.52 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:30.954653: step 113300, loss = 0.64 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:31.417430: step 113310, loss = 0.49 (7847.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:31.737018: step 113320, loss = 0.51 (8066.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:32.058804: step 113330, loss = 0.59 (7863.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:32.380898: step 113340, loss = 0.50 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:32.701471: step 113350, loss = 0.55 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:33.021067: step 113360, loss = 0.65 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:33.342113: step 113370, loss = 0.62 (7825.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:33.662478: step 113380, loss = 0.58 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:33.984219: step 113390, loss = 0.56 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:34.308781: step 113400, loss = 0.44 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:34.762308: step 113410, loss = 0.71 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:35.081382: step 113420, loss = 0.52 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:35.403676: step 113430, loss = 0.58 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:35.725748: step 113440, loss = 0.62 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:36.046256: step 113450, loss = 0.40 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:36.366781: step 113460, loss = 0.54 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:36.686044: step 113470, loss = 0.47 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:37.007874: step 113480, loss = 0.61 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:37.325935: step 113490, loss = 0.57 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:37.646456: step 113500, loss = 0.49 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:38.104696: step 113510, loss = 0.43 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:38.431546: step 113520, loss = 0.50 (7679.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:43:38.753137: step 113530, loss = 0.59 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:39.074489: step 113540, loss = 0.65 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:39.398091: step 113550, loss = 0.55 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:39.721024: step 113560, loss = 0.60 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:40.043076: step 113570, loss = 0.57 (8158.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:40.364340: step 113580, loss = 0.52 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:40.686369: step 113590, loss = 0.55 (7713.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:43:41.007376: step 113600, loss = 0.53 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:41.463782: step 113610, loss = 0.64 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:41.784221: step 113620, loss = 0.67 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:42.105024: step 113630, loss = 0.65 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:42.423677: step 113640, loss = 0.54 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:42.744374: step 113650, loss = 0.53 (7731.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:43:43.064831: step 113660, loss = 0.62 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:43.383263: step 113670, loss = 0.58 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:43.703181: step 113680, loss = 0.56 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:44.023168: step 113690, loss = 0.58 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:44.342884: step 113700, loss = 0.50 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:44.792435: step 113710, loss = 0.48 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:45.110952: step 113720, loss = 0.58 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:45.429272: step 113730, loss = 0.54 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:45.750814: step 113740, loss = 0.56 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:46.069115: step 113750, loss = 0.63 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:46.388697: step 113760, loss = 0.54 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:46.708872: step 113770, loss = 0.55 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:47.027004: step 113780, loss = 0.57 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:47.348542: step 113790, loss = 0.56 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:47.667062: step 113800, loss = 0.52 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:48.123594: step 113810, loss = 0.66 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:48.443078: step 113820, loss = 0.53 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:48.761819: step 113830, loss = 0.60 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:49.082863: step 113840, loss = 0.56 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:49.400857: step 113850, loss = 0.61 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:49.723407: step 113860, loss = 0.53 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:50.042483: step 113870, loss = 0.59 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:50.363616: step 113880, loss = 0.48 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:50.685781: step 113890, loss = 0.47 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:51.004840: step 113900, loss = 0.52 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:51.459837: step 113910, loss = 0.61 (8130.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:51.781441: step 113920, loss = 0.42 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:52.101443: step 113930, loss = 0.46 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:52.421004: step 113940, loss = 0.47 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:52.740437: step 113950, loss = 0.57 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:53.060934: step 113960, loss = 0.46 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:53.381363: step 113970, loss = 0.58 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:53.702480: step 113980, loss = 0.49 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:54.024271: step 113990, loss = 0.55 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:54.344169: step 114000, loss = 0.70 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:54.904071: step 114010, loss = 0.54 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:55.226428: step 114020, loss = 0.55 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:55.546322: step 114030, loss = 0.71 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:55.865860: step 114040, loss = 0.57 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:56.186690: step 114050, loss = 0.60 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:56.510336: step 114060, loss = 0.58 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:56.832393: step 114070, loss = 0.53 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:57.151469: step 114080, loss = 0.55 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:57.470628: step 114090, loss = 0.48 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:57.791317: step 114100, loss = 0.51 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:58.246487: step 114110, loss = 0.44 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:58.567761: step 114120, loss = 0.57 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:58.887142: step 114130, loss = 0.54 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:59.210408: step 114140, loss = 0.47 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:59.536497: step 114150, loss = 0.53 (7985.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:43:59.856070: step 114160, loss = 0.65 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:00.178276: step 114170, loss = 0.58 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:00.498908: step 114180, loss = 0.75 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:00.818717: step 114190, loss = 0.54 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:01.136859: step 114200, loss = 0.60 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:01.588452: step 114210, loss = 0.50 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:01.909280: step 114220, loss = 0.70 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:02.233297: step 114230, loss = 0.68 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:02.552381: step 114240, loss = 0.56 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:02.870943: step 114250, loss = 0.56 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:03.190083: step 114260, loss = 0.53 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:03.509333: step 114270, loss = 0.64 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:03.828651: step 114280, loss = 0.62 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:04.148106: step 114290, loss = 0.58 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:04.468753: step 114300, loss = 0.62 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:04.936142: step 114310, loss = 0.50 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:05.256314: step 114320, loss = 0.42 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:05.575702: step 114330, loss = 0.45 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:05.894179: step 114340, loss = 0.59 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:06.214240: step 114350, loss = 0.68 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:06.535607: step 114360, loss = 0.62 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:06.857188: step 114370, loss = 0.51 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:07.175505: step 114380, loss = 0.58 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:07.494976: step 114390, loss = 0.51 (7823.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:07.813210: step 114400, loss = 0.65 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:08.273197: step 114410, loss = 0.49 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:08.591186: step 114420, loss = 0.54 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:08.911851: step 114430, loss = 0.46 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:09.232248: step 114440, loss = 0.46 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:09.552051: step 114450, loss = 0.53 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:09.871282: step 114460, loss = 0.48 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:10.193801: step 114470, loss = 0.59 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:10.517284: step 114480, loss = 0.61 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:10.836321: step 114490, loss = 0.49 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:11.158152: step 114500, loss = 0.51 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:11.623991: step 114510, loss = 0.73 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:11.945828: step 114520, loss = 0.54 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:12.266760: step 114530, loss = 0.48 (7829.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:12.586294: step 114540, loss = 0.52 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:12.906219: step 114550, loss = 0.67 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:13.227038: step 114560, loss = 0.61 (7808.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:13.548421: step 114570, loss = 0.55 (8072.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:13.868790: step 114580, loss = 0.55 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:14.188024: step 114590, loss = 0.71 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:14.508607: step 114600, loss = 0.65 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:14.971318: step 114610, loss = 0.55 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:15.294164: step 114620, loss = 0.60 (7813.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:15.613530: step 114630, loss = 0.42 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:15.937127: step 114640, loss = 0.57 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:16.256102: step 114650, loss = 0.54 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:16.578707: step 114660, loss = 0.58 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:16.898460: step 114670, loss = 0.60 (7766.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:17.220122: step 114680, loss = 0.63 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:17.540367: step 114690, loss = 0.56 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:17.859394: step 114700, loss = 0.46 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:18.318603: step 114710, loss = 0.51 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:18.638197: step 114720, loss = 0.56 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:18.958798: step 114730, loss = 0.60 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:19.283670: step 114740, loss = 0.59 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:19.610946: step 114750, loss = 0.58 (7639.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:44:19.931529: step 114760, loss = 0.64 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:20.251162: step 114770, loss = 0.47 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:20.572376: step 114780, loss = 0.58 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:20.892935: step 114790, loss = 0.61 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:21.211608: step 114800, loss = 0.50 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:21.672122: step 114810, loss = 0.60 (7730.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:44:21.993669: step 114820, loss = 0.59 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:22.316565: step 114830, loss = 0.53 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:22.640005: step 114840, loss = 0.65 (8178.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:22.959728: step 114850, loss = 0.54 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:23.280143: step 114860, loss = 0.50 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:23.601748: step 114870, loss = 0.68 (7898.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:23.923284: step 114880, loss = 0.51 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:24.242639: step 114890, loss = 0.49 (8090.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:24.561901: step 114900, loss = 0.59 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:25.021116: step 114910, loss = 0.53 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:25.343382: step 114920, loss = 0.70 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:25.665295: step 114930, loss = 0.74 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:25.985195: step 114940, loss = 0.57 (7942.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:26.307291: step 114950, loss = 0.54 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:26.627383: step 114960, loss = 0.55 (7846.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:26.948025: step 114970, loss = 0.54 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:27.270834: step 114980, loss = 0.49 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:27.590503: step 114990, loss = 0.47 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:27.910509: step 115000, loss = 0.65 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:28.462554: step 115010, loss = 0.61 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:28.784213: step 115020, loss = 0.49 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:29.104641: step 115030, loss = 0.48 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:29.424209: step 115040, loss = 0.66 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:29.744519: step 115050, loss = 0.62 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:30.065736: step 115060, loss = 0.52 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:30.385375: step 115070, loss = 0.50 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:30.706048: step 115080, loss = 0.59 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:31.032031: step 115090, loss = 0.62 (7768.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:31.355199: step 115100, loss = 0.56 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:31.810677: step 115110, loss = 0.55 (8136.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:32.132138: step 115120, loss = 0.60 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:32.454212: step 115130, loss = 0.45 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:32.776715: step 115140, loss = 0.53 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:33.097458: step 115150, loss = 0.55 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:33.422607: step 115160, loss = 0.57 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:33.742155: step 115170, loss = 0.56 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:34.063953: step 115180, loss = 0.56 (8074.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:34.390797: step 115190, loss = 0.52 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:34.713833: step 115200, loss = 0.59 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:35.161383: step 115210, loss = 0.67 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:35.480945: step 115220, loss = 0.56 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:35.805794: step 115230, loss = 0.53 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:36.127954: step 115240, loss = 0.52 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:36.449380: step 115250, loss = 0.49 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:36.771201: step 115260, loss = 0.49 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:37.090406: step 115270, loss = 0.50 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:37.413553: step 115280, loss = 0.44 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:37.738845: step 115290, loss = 0.60 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:38.060419: step 115300, loss = 0.49 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:38.518768: step 115310, loss = 0.40 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:38.841091: step 115320, loss = 0.48 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:39.160916: step 115330, loss = 0.61 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:39.483336: step 115340, loss = 0.69 (7875.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:39.803356: step 115350, loss = 0.53 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:40.122366: step 115360, loss = 0.58 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:40.441230: step 115370, loss = 0.49 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:40.761179: step 115380, loss = 0.53 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:41.084856: step 115390, loss = 0.61 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:41.406948: step 115400, loss = 0.58 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:41.856681: step 115410, loss = 0.51 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:42.178487: step 115420, loss = 0.46 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:42.497867: step 115430, loss = 0.55 (8137.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:42.817516: step 115440, loss = 0.62 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:43.141566: step 115450, loss = 0.46 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:43.462968: step 115460, loss = 0.68 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:43.784323: step 115470, loss = 0.63 (8143.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:44.104753: step 115480, loss = 0.42 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:44.427211: step 115490, loss = 0.45 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:44.746689: step 115500, loss = 0.55 (7875.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:45.207913: step 115510, loss = 0.54 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:45.526493: step 115520, loss = 0.59 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:45.845888: step 115530, loss = 0.52 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:46.164807: step 115540, loss = 0.53 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:46.482768: step 115550, loss = 0.61 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:46.800680: step 115560, loss = 0.62 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:47.122035: step 115570, loss = 0.57 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:47.447940: step 115580, loss = 0.61 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:47.768104: step 115590, loss = 0.68 (7787.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:48.088608: step 115600, loss = 0.74 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:48.552391: step 115610, loss = 0.62 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:48.873001: step 115620, loss = 0.48 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:49.191701: step 115630, loss = 0.49 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:49.511317: step 115640, loss = 0.46 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:49.833585: step 115650, loss = 0.56 (7784.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:50.154283: step 115660, loss = 0.63 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:50.474191: step 115670, loss = 0.55 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:50.792578: step 115680, loss = 0.65 (8190.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:51.110190: step 115690, loss = 0.64 (8104.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:51.431269: step 115700, loss = 0.54 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:51.889485: step 115710, loss = 0.54 (8164.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:52.209543: step 115720, loss = 0.47 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:52.529106: step 115730, loss = 0.56 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:52.849730: step 115740, loss = 0.64 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:53.171137: step 115750, loss = 0.70 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:53.492340: step 115760, loss = 0.59 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:53.816361: step 115770, loss = 0.60 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:54.139014: step 115780, loss = 0.46 (7612.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:44:54.458133: step 115790, loss = 0.61 (7894.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:54.777217: step 115800, loss = 0.63 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:55.237881: step 115810, loss = 0.54 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:55.558368: step 115820, loss = 0.54 (8018.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:55.877375: step 115830, loss = 0.59 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:56.195203: step 115840, loss = 0.52 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:56.514551: step 115850, loss = 0.53 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:56.836304: step 115860, loss = 0.64 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:57.158451: step 115870, loss = 0.52 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:57.481740: step 115880, loss = 0.54 (7409.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:44:57.802380: step 115890, loss = 0.57 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:58.122190: step 115900, loss = 0.56 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:58.577870: step 115910, loss = 0.62 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:58.901116: step 115920, loss = 0.47 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:59.222840: step 115930, loss = 0.57 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:59.541406: step 115940, loss = 0.73 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:44:59.860351: step 115950, loss = 0.45 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:00.180540: step 115960, loss = 0.68 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:00.499613: step 115970, loss = 0.56 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:00.819559: step 115980, loss = 0.64 (8128.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:01.139357: step 115990, loss = 0.61 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:01.459160: step 116000, loss = 0.46 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:02.003713: step 116010, loss = 0.60 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:02.323711: step 116020, loss = 0.47 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:02.642260: step 116030, loss = 0.48 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:02.965066: step 116040, loss = 0.59 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:03.283925: step 116050, loss = 0.52 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:03.610026: step 116060, loss = 0.46 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:03.933043: step 116070, loss = 0.44 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:04.250320: step 116080, loss = 0.53 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:04.570539: step 116090, loss = 0.51 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:04.891097: step 116100, loss = 0.62 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:05.338350: step 116110, loss = 0.56 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:05.656745: step 116120, loss = 0.59 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:05.975721: step 116130, loss = 0.60 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:06.297072: step 116140, loss = 0.63 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:06.616334: step 116150, loss = 0.56 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:06.936533: step 116160, loss = 0.64 (7953.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:07.255946: step 116170, loss = 0.61 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:07.579752: step 116180, loss = 0.47 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:07.901029: step 116190, loss = 0.51 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:08.220215: step 116200, loss = 0.63 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:08.678943: step 116210, loss = 0.56 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:08.997999: step 116220, loss = 0.50 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:09.319920: step 116230, loss = 0.53 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:09.639356: step 116240, loss = 0.54 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:09.962082: step 116250, loss = 0.53 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:10.282478: step 116260, loss = 0.55 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:10.601283: step 116270, loss = 0.43 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:10.921483: step 116280, loss = 0.59 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:11.241296: step 116290, loss = 0.53 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:11.562357: step 116300, loss = 0.44 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:12.015549: step 116310, loss = 0.56 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:12.336928: step 116320, loss = 0.50 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:12.655951: step 116330, loss = 0.48 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:12.974469: step 116340, loss = 0.40 (7861.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:13.294395: step 116350, loss = 0.48 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:13.615356: step 116360, loss = 0.54 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:13.940923: step 116370, loss = 0.51 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:14.263544: step 116380, loss = 0.51 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:14.583195: step 116390, loss = 0.63 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:14.905204: step 116400, loss = 0.58 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:15.363140: step 116410, loss = 0.55 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:15.684364: step 116420, loss = 0.51 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:16.002989: step 116430, loss = 0.58 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:16.322742: step 116440, loss = 0.63 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:16.641855: step 116450, loss = 0.48 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:16.964411: step 116460, loss = 0.66 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:17.287333: step 116470, loss = 0.65 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:17.607465: step 116480, loss = 0.57 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:17.929539: step 116490, loss = 0.54 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:18.249199: step 116500, loss = 0.61 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:18.714846: step 116510, loss = 0.65 (8155.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:19.034667: step 116520, loss = 0.66 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:19.352966: step 116530, loss = 0.59 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:19.672152: step 116540, loss = 0.52 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:19.990915: step 116550, loss = 0.47 (8106.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:20.312669: step 116560, loss = 0.49 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:20.632330: step 116570, loss = 0.65 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:20.954553: step 116580, loss = 0.58 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:21.274301: step 116590, loss = 0.46 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:21.594911: step 116600, loss = 0.45 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:22.044532: step 116610, loss = 0.66 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:22.367780: step 116620, loss = 0.54 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:22.688869: step 116630, loss = 0.58 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:23.007913: step 116640, loss = 0.62 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:23.328564: step 116650, loss = 0.58 (7992.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:23.649844: step 116660, loss = 0.48 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:23.971347: step 116670, loss = 0.69 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:24.294870: step 116680, loss = 0.57 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:24.614375: step 116690, loss = 0.56 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:24.933975: step 116700, loss = 0.52 (7947.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:25.394347: step 116710, loss = 0.43 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:25.713708: step 116720, loss = 0.44 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:26.033191: step 116730, loss = 0.51 (7935.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:26.354413: step 116740, loss = 0.49 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:26.677668: step 116750, loss = 0.64 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:26.996405: step 116760, loss = 0.58 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:27.316487: step 116770, loss = 0.63 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:27.638875: step 116780, loss = 0.61 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:27.958111: step 116790, loss = 0.50 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:28.278714: step 116800, loss = 0.46 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:28.735220: step 116810, loss = 0.71 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:29.055437: step 116820, loss = 0.52 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:29.374997: step 116830, loss = 0.50 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:29.695233: step 116840, loss = 0.52 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:30.018897: step 116850, loss = 0.50 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:30.340272: step 116860, loss = 0.57 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:30.661947: step 116870, loss = 0.74 (7791.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:30.981602: step 116880, loss = 0.68 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:31.301972: step 116890, loss = 0.59 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:31.626227: step 116900, loss = 0.50 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:32.085893: step 116910, loss = 0.64 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:32.407612: step 116920, loss = 0.60 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:32.727070: step 116930, loss = 0.45 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:33.046461: step 116940, loss = 0.56 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:33.366959: step 116950, loss = 0.56 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:33.687531: step 116960, loss = 0.64 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:34.007169: step 116970, loss = 0.55 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:34.327507: step 116980, loss = 0.59 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:34.648975: step 116990, loss = 0.67 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:34.971072: step 117000, loss = 0.65 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:35.532592: step 117010, loss = 0.65 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:35.852171: step 117020, loss = 0.44 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:36.172998: step 117030, loss = 0.58 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:36.491899: step 117040, loss = 0.51 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:36.811883: step 117050, loss = 0.58 (8148.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:37.132437: step 117060, loss = 0.57 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:37.452652: step 117070, loss = 0.59 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:37.771782: step 117080, loss = 0.50 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:38.094414: step 117090, loss = 0.53 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:38.417595: step 117100, loss = 0.72 (7636.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:45:38.875399: step 117110, loss = 0.46 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:39.195845: step 117120, loss = 0.58 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:39.518743: step 117130, loss = 0.57 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:39.840359: step 117140, loss = 0.64 (8128.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:40.162505: step 117150, loss = 0.46 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:40.483033: step 117160, loss = 0.64 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:40.802721: step 117170, loss = 0.50 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:41.122663: step 117180, loss = 0.43 (8044.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:41.443646: step 117190, loss = 0.56 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:41.767538: step 117200, loss = 0.49 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:42.230965: step 117210, loss = 0.59 (7499.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:45:42.558312: step 117220, loss = 0.51 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:42.878523: step 117230, loss = 0.59 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:43.201259: step 117240, loss = 0.49 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:43.521503: step 117250, loss = 0.64 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:43.843296: step 117260, loss = 0.55 (7822.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:44.167156: step 117270, loss = 0.58 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:44.488526: step 117280, loss = 0.60 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:44.813079: step 117290, loss = 0.56 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:45.133674: step 117300, loss = 0.51 (7672.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:45:45.589860: step 117310, loss = 0.63 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:45.910592: step 117320, loss = 0.59 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:46.232512: step 117330, loss = 0.44 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:46.553385: step 117340, loss = 0.49 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:46.875606: step 117350, loss = 0.58 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:47.193755: step 117360, loss = 0.67 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:47.516312: step 117370, loss = 0.42 (7647.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:45:47.836072: step 117380, loss = 0.54 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:48.162077: step 117390, loss = 0.67 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:48.483961: step 117400, loss = 0.53 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:48.944999: step 117410, loss = 0.56 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:49.264990: step 117420, loss = 0.67 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:49.586998: step 117430, loss = 0.57 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:49.910261: step 117440, loss = 0.62 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:50.231902: step 117450, loss = 0.61 (8042.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:50.550580: step 117460, loss = 0.68 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:50.871843: step 117470, loss = 0.56 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:51.191420: step 117480, loss = 0.59 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:51.513086: step 117490, loss = 0.58 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:51.833261: step 117500, loss = 0.61 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:52.292761: step 117510, loss = 0.57 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:52.614205: step 117520, loss = 0.52 (7974.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:52.937060: step 117530, loss = 0.41 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:53.257968: step 117540, loss = 0.56 (7776.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:53.580179: step 117550, loss = 0.56 (7468.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:45:53.904291: step 117560, loss = 0.51 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:54.226668: step 117570, loss = 0.53 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:54.547154: step 117580, loss = 0.43 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:54.867159: step 117590, loss = 0.67 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:55.188391: step 117600, loss = 0.54 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:55.644127: step 117610, loss = 0.63 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:55.964857: step 117620, loss = 0.62 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:56.285259: step 117630, loss = 0.63 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:56.606187: step 117640, loss = 0.64 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:56.932967: step 117650, loss = 0.57 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:57.252565: step 117660, loss = 0.62 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:57.576989: step 117670, loss = 0.74 (7804.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:57.897195: step 117680, loss = 0.58 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:58.215621: step 117690, loss = 0.72 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:58.538847: step 117700, loss = 0.51 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:58.989421: step 117710, loss = 0.55 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:59.311732: step 117720, loss = 0.49 (8149.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:59.637404: step 117730, loss = 0.62 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:45:59.959296: step 117740, loss = 0.64 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:00.279304: step 117750, loss = 0.72 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:00.601246: step 117760, loss = 0.57 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:00.923281: step 117770, loss = 0.57 (7798.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:01.242259: step 117780, loss = 0.68 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:01.565820: step 117790, loss = 0.48 (7825.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:01.886166: step 117800, loss = 0.54 (7842.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:02.345534: step 117810, loss = 0.60 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:02.667661: step 117820, loss = 0.68 (7923.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:02.989016: step 117830, loss = 0.58 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:03.308083: step 117840, loss = 0.58 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:03.627608: step 117850, loss = 0.56 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:03.947281: step 117860, loss = 0.48 (7880.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:04.267959: step 117870, loss = 0.46 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:04.586968: step 117880, loss = 0.53 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:04.906327: step 117890, loss = 0.69 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:05.226467: step 117900, loss = 0.50 (7943.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:05.688623: step 117910, loss = 0.65 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:06.008458: step 117920, loss = 0.70 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:06.328655: step 117930, loss = 0.46 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:06.653847: step 117940, loss = 0.55 (7680.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:46:06.976858: step 117950, loss = 0.64 (8102.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:07.297027: step 117960, loss = 0.54 (7971.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:07.620015: step 117970, loss = 0.60 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:07.940211: step 117980, loss = 0.58 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:08.259615: step 117990, loss = 0.55 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:08.579541: step 118000, loss = 0.55 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:09.124302: step 118010, loss = 0.63 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:09.447079: step 118020, loss = 0.50 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:09.767504: step 118030, loss = 0.56 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:10.089744: step 118040, loss = 0.60 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:10.408987: step 118050, loss = 0.44 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:10.729182: step 118060, loss = 0.56 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:11.048482: step 118070, loss = 0.61 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:11.370376: step 118080, loss = 0.64 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:11.693006: step 118090, loss = 0.59 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:12.011078: step 118100, loss = 0.52 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:12.470072: step 118110, loss = 0.62 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:12.789836: step 118120, loss = 0.67 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:13.108806: step 118130, loss = 0.50 (8012.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:13.428498: step 118140, loss = 0.61 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:13.748864: step 118150, loss = 0.57 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:14.071028: step 118160, loss = 0.53 (7795.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:14.392659: step 118170, loss = 0.45 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:14.713487: step 118180, loss = 0.57 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:15.033448: step 118190, loss = 0.57 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:15.355948: step 118200, loss = 0.58 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:15.815831: step 118210, loss = 0.49 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:16.134810: step 118220, loss = 0.50 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:16.457443: step 118230, loss = 0.52 (8163.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:16.777672: step 118240, loss = 0.58 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:17.103018: step 118250, loss = 0.53 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:17.424225: step 118260, loss = 0.60 (7808.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:17.744128: step 118270, loss = 0.64 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:18.065635: step 118280, loss = 0.56 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:18.388645: step 118290, loss = 0.47 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:18.708288: step 118300, loss = 0.67 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:19.173606: step 118310, loss = 0.57 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:19.499173: step 118320, loss = 0.69 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:19.822394: step 118330, loss = 0.61 (7963.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:20.141872: step 118340, loss = 0.59 (7859.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:20.462947: step 118350, loss = 0.57 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:20.783352: step 118360, loss = 0.75 (7989.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:21.102976: step 118370, loss = 0.65 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:21.423247: step 118380, loss = 0.58 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:21.743344: step 118390, loss = 0.58 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:22.064712: step 118400, loss = 0.48 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:22.524955: step 118410, loss = 0.44 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:22.845841: step 118420, loss = 0.65 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:23.164894: step 118430, loss = 0.67 (7916.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:23.488534: step 118440, loss = 0.58 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:23.808001: step 118450, loss = 0.52 (8141.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:24.130409: step 118460, loss = 0.53 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:24.450636: step 118470, loss = 0.69 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:24.770402: step 118480, loss = 0.48 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:25.090222: step 118490, loss = 0.62 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:25.409048: step 118500, loss = 0.49 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:25.875274: step 118510, loss = 0.54 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:26.198153: step 118520, loss = 0.63 (8138.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:26.520497: step 118530, loss = 0.58 (8151.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:26.840311: step 118540, loss = 0.63 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:27.162606: step 118550, loss = 0.58 (7863.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:27.482276: step 118560, loss = 0.62 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:27.802483: step 118570, loss = 0.50 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:28.122994: step 118580, loss = 0.67 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:28.443918: step 118590, loss = 0.59 (7833.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:28.764419: step 118600, loss = 0.47 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:29.215624: step 118610, loss = 0.53 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:29.536201: step 118620, loss = 0.47 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:29.862955: step 118630, loss = 0.69 (7747.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:46:30.182740: step 118640, loss = 0.63 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:30.504426: step 118650, loss = 0.56 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:30.823938: step 118660, loss = 0.59 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:31.144500: step 118670, loss = 0.53 (7818.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:31.464643: step 118680, loss = 0.50 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:31.784685: step 118690, loss = 0.61 (7977.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:32.103962: step 118700, loss = 0.66 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:32.557184: step 118710, loss = 0.61 (7976.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:32.876968: step 118720, loss = 0.67 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:33.195384: step 118730, loss = 0.55 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:33.515736: step 118740, loss = 0.56 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:33.835063: step 118750, loss = 0.55 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:34.155727: step 118760, loss = 0.55 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:34.476356: step 118770, loss = 0.58 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:34.797606: step 118780, loss = 0.59 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:35.117227: step 118790, loss = 0.53 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:35.435741: step 118800, loss = 0.44 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:35.892557: step 118810, loss = 0.53 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:36.212634: step 118820, loss = 0.46 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:36.534521: step 118830, loss = 0.65 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:36.855436: step 118840, loss = 0.48 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:37.176283: step 118850, loss = 0.47 (7840.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:37.495582: step 118860, loss = 0.54 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:37.815936: step 118870, loss = 0.48 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:38.138422: step 118880, loss = 0.71 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:38.457042: step 118890, loss = 0.47 (8140.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:38.780984: step 118900, loss = 0.47 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:39.247985: step 118910, loss = 0.56 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:39.567451: step 118920, loss = 0.59 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:39.887245: step 118930, loss = 0.54 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:40.208568: step 118940, loss = 0.56 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:40.527425: step 118950, loss = 0.60 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:40.848920: step 118960, loss = 0.57 (7872.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:41.170044: step 118970, loss = 0.57 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:41.491185: step 118980, loss = 0.52 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:41.811090: step 118990, loss = 0.53 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:42.130008: step 119000, loss = 0.62 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:42.741046: step 119010, loss = 0.52 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:43.059975: step 119020, loss = 0.49 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:43.379244: step 119030, loss = 0.54 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:43.700947: step 119040, loss = 0.51 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:44.019612: step 119050, loss = 0.48 (7944.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:44.339754: step 119060, loss = 0.62 (7947.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:44.661238: step 119070, loss = 0.50 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:44.982233: step 119080, loss = 0.57 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:45.301683: step 119090, loss = 0.50 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:45.621012: step 119100, loss = 0.58 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:46.077950: step 119110, loss = 0.53 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:46.396146: step 119120, loss = 0.63 (8169.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:46.714242: step 119130, loss = 0.60 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:47.036581: step 119140, loss = 0.44 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:47.357932: step 119150, loss = 0.40 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:47.677751: step 119160, loss = 0.53 (7964.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:47.997184: step 119170, loss = 0.50 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:48.323073: step 119180, loss = 0.44 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:48.646781: step 119190, loss = 0.47 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:48.966192: step 119200, loss = 0.53 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:49.431882: step 119210, loss = 0.69 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:49.751611: step 119220, loss = 0.60 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:50.071720: step 119230, loss = 0.56 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:50.391113: step 119240, loss = 0.54 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:50.712417: step 119250, loss = 0.63 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:51.033473: step 119260, loss = 0.49 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:51.355652: step 119270, loss = 0.51 (7922.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:51.674063: step 119280, loss = 0.51 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:51.993298: step 119290, loss = 0.50 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:52.312621: step 119300, loss = 0.63 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:52.770650: step 119310, loss = 0.55 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:53.093378: step 119320, loss = 0.53 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:53.412201: step 119330, loss = 0.60 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:53.734676: step 119340, loss = 0.49 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:54.056967: step 119350, loss = 0.63 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:54.377429: step 119360, loss = 0.47 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:54.697691: step 119370, loss = 0.51 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:55.018747: step 119380, loss = 0.60 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:55.339754: step 119390, loss = 0.49 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:55.660399: step 119400, loss = 0.52 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:56.118430: step 119410, loss = 0.63 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:56.437968: step 119420, loss = 0.53 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:56.757732: step 119430, loss = 0.44 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:57.078530: step 119440, loss = 0.70 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:57.398201: step 119450, loss = 0.71 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:57.719155: step 119460, loss = 0.62 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:58.040008: step 119470, loss = 0.58 (7796.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:58.363874: step 119480, loss = 0.54 (7828.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:58.685308: step 119490, loss = 0.66 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:59.009999: step 119500, loss = 0.56 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:59.461819: step 119510, loss = 0.60 (8075.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:46:59.783581: step 119520, loss = 0.63 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:00.102592: step 119530, loss = 0.56 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:00.422388: step 119540, loss = 0.53 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:00.742074: step 119550, loss = 0.53 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:01.062696: step 119560, loss = 0.69 (7908.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:01.383851: step 119570, loss = 0.58 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:01.705518: step 119580, loss = 0.38 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:02.028909: step 119590, loss = 0.61 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:02.351056: step 119600, loss = 0.55 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:02.808594: step 119610, loss = 0.60 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:03.128606: step 119620, loss = 0.63 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:03.450066: step 119630, loss = 0.60 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:03.770472: step 119640, loss = 0.64 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:04.089163: step 119650, loss = 0.68 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:04.418149: step 119660, loss = 0.59 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:04.739056: step 119670, loss = 0.57 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:05.060455: step 119680, loss = 0.57 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:05.383144: step 119690, loss = 0.57 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:05.701715: step 119700, loss = 0.54 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:06.149066: step 119710, loss = 0.49 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:06.469653: step 119720, loss = 0.49 (7867.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:06.788162: step 119730, loss = 0.44 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:07.108414: step 119740, loss = 0.50 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:07.427837: step 119750, loss = 0.46 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:07.750859: step 119760, loss = 0.57 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:08.076133: step 119770, loss = 0.47 (7651.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:47:08.396005: step 119780, loss = 0.53 (8059.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:08.716513: step 119790, loss = 0.50 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:09.037322: step 119800, loss = 0.60 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:09.497394: step 119810, loss = 0.60 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:09.818143: step 119820, loss = 0.60 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:10.138644: step 119830, loss = 0.58 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:10.460788: step 119840, loss = 0.62 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:10.780768: step 119850, loss = 0.71 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:11.106985: step 119860, loss = 0.52 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:11.430067: step 119870, loss = 0.60 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:11.750453: step 119880, loss = 0.42 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:12.068653: step 119890, loss = 0.70 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:12.387679: step 119900, loss = 0.59 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:12.875457: step 119910, loss = 0.57 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:13.195593: step 119920, loss = 0.47 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:13.516654: step 119930, loss = 0.44 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:13.839661: step 119940, loss = 0.58 (7879.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:14.161320: step 119950, loss = 0.55 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:14.481722: step 119960, loss = 0.62 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:14.800396: step 119970, loss = 0.63 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:15.120207: step 119980, loss = 0.70 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:15.439252: step 119990, loss = 0.54 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:15.762017: step 120000, loss = 0.64 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:16.308620: step 120010, loss = 0.55 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:16.630086: step 120020, loss = 0.53 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:16.952221: step 120030, loss = 0.58 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:17.271316: step 120040, loss = 0.57 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:17.591033: step 120050, loss = 0.57 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:17.911924: step 120060, loss = 0.48 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:18.239407: step 120070, loss = 0.61 (7451.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:47:18.563764: step 120080, loss = 0.50 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:18.884797: step 120090, loss = 0.65 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:19.207137: step 120100, loss = 0.49 (7876.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:19.668867: step 120110, loss = 0.61 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:19.989522: step 120120, loss = 0.60 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:20.311757: step 120130, loss = 0.65 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:20.630192: step 120140, loss = 0.61 (8146.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:20.951504: step 120150, loss = 0.57 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:21.272264: step 120160, loss = 0.54 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:21.594127: step 120170, loss = 0.58 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:21.918595: step 120180, loss = 0.62 (7638.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:47:22.238731: step 120190, loss = 0.45 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:22.560073: step 120200, loss = 0.63 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:23.021709: step 120210, loss = 0.57 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:23.342383: step 120220, loss = 0.48 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:23.664498: step 120230, loss = 0.67 (7789.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:23.984710: step 120240, loss = 0.60 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:24.305790: step 120250, loss = 0.51 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:24.625633: step 120260, loss = 0.64 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:24.944122: step 120270, loss = 0.61 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:25.263595: step 120280, loss = 0.46 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:25.584718: step 120290, loss = 0.59 (7779.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:25.903889: step 120300, loss = 0.64 (8017.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:26.362216: step 120310, loss = 0.66 (7902.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:26.681233: step 120320, loss = 0.61 (8132.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:27.001183: step 120330, loss = 0.58 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:27.322250: step 120340, loss = 0.56 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:27.641573: step 120350, loss = 0.62 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:27.964915: step 120360, loss = 0.56 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:28.289337: step 120370, loss = 0.61 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:28.611905: step 120380, loss = 0.47 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:28.932963: step 120390, loss = 0.57 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:29.256196: step 120400, loss = 0.58 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:29.707006: step 120410, loss = 0.52 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:30.027040: step 120420, loss = 0.61 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:30.345724: step 120430, loss = 0.47 (8151.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:30.664492: step 120440, loss = 0.65 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:30.986697: step 120450, loss = 0.58 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:31.309767: step 120460, loss = 0.54 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:31.632310: step 120470, loss = 0.43 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:31.952012: step 120480, loss = 0.51 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:32.271665: step 120490, loss = 0.65 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:32.590633: step 120500, loss = 0.59 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:33.047782: step 120510, loss = 0.54 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:33.369221: step 120520, loss = 0.57 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:33.686208: step 120530, loss = 0.49 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:34.007313: step 120540, loss = 0.48 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:34.326226: step 120550, loss = 0.61 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:34.647567: step 120560, loss = 0.69 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:34.967711: step 120570, loss = 0.55 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:35.291871: step 120580, loss = 0.67 (7905.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:35.613199: step 120590, loss = 0.49 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:35.933859: step 120600, loss = 0.58 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:36.404268: step 120610, loss = 0.61 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:36.722685: step 120620, loss = 0.64 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:37.042301: step 120630, loss = 0.56 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:37.364111: step 120640, loss = 0.52 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:37.688184: step 120650, loss = 0.56 (8127.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:38.010765: step 120660, loss = 0.50 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:38.331484: step 120670, loss = 0.60 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:38.655675: step 120680, loss = 0.59 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:38.976401: step 120690, loss = 0.53 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:39.295659: step 120700, loss = 0.46 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:39.754288: step 120710, loss = 0.59 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:40.075280: step 120720, loss = 0.55 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:40.400942: step 120730, loss = 0.49 (7780.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:40.719297: step 120740, loss = 0.58 (8089.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:41.039971: step 120750, loss = 0.55 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:41.360710: step 120760, loss = 0.62 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:41.682511: step 120770, loss = 0.41 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:42.003743: step 120780, loss = 0.48 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:42.327446: step 120790, loss = 0.47 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:42.646700: step 120800, loss = 0.52 (7910.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:43.109657: step 120810, loss = 0.45 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:43.429924: step 120820, loss = 0.55 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:43.755797: step 120830, loss = 0.50 (7553.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:47:44.075749: step 120840, loss = 0.46 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:44.397649: step 120850, loss = 0.55 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:44.717386: step 120860, loss = 0.66 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:45.038969: step 120870, loss = 0.57 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:45.358181: step 120880, loss = 0.60 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:45.679842: step 120890, loss = 0.64 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:45.999917: step 120900, loss = 0.48 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:46.453366: step 120910, loss = 0.55 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:46.772824: step 120920, loss = 0.52 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:47.093656: step 120930, loss = 0.64 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:47.415243: step 120940, loss = 0.53 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:47.737389: step 120950, loss = 0.51 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:48.058023: step 120960, loss = 0.46 (7683.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:47:48.380715: step 120970, loss = 0.53 (7280.6 examples/sec; 0.018 sec/batch)
2017-09-16 16:47:48.702675: step 120980, loss = 0.52 (8069.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:49.022076: step 120990, loss = 0.57 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:49.345855: step 121000, loss = 0.47 (6913.1 examples/sec; 0.019 sec/batch)
2017-09-16 16:47:49.890224: step 121010, loss = 0.54 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:50.210696: step 121020, loss = 0.47 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:50.528842: step 121030, loss = 0.56 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:50.852037: step 121040, loss = 0.52 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:51.172466: step 121050, loss = 0.59 (7843.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:51.493248: step 121060, loss = 0.51 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:51.812622: step 121070, loss = 0.58 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:52.134949: step 121080, loss = 0.60 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:52.453255: step 121090, loss = 0.61 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:52.776002: step 121100, loss = 0.66 (7846.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:53.233632: step 121110, loss = 0.58 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:53.555449: step 121120, loss = 0.45 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:53.874649: step 121130, loss = 0.51 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:54.193777: step 121140, loss = 0.51 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:54.516903: step 121150, loss = 0.54 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:54.838376: step 121160, loss = 0.68 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:55.161763: step 121170, loss = 0.66 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:55.482304: step 121180, loss = 0.63 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:55.805084: step 121190, loss = 0.58 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:56.125933: step 121200, loss = 0.49 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:56.579647: step 121210, loss = 0.41 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:56.900455: step 121220, loss = 0.57 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:57.220184: step 121230, loss = 0.43 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:57.542920: step 121240, loss = 0.60 (8026.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:57.864457: step 121250, loss = 0.57 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:58.183860: step 121260, loss = 0.60 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:58.503840: step 121270, loss = 0.63 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:58.823027: step 121280, loss = 0.50 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:59.143675: step 121290, loss = 0.60 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:59.464145: step 121300, loss = 0.54 (8105.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:47:59.924737: step 121310, loss = 0.73 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:00.246474: step 121320, loss = 0.56 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:00.565296: step 121330, loss = 0.50 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:00.888474: step 121340, loss = 0.56 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:01.207740: step 121350, loss = 0.55 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:01.529930: step 121360, loss = 0.46 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:01.848798: step 121370, loss = 0.60 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:02.170302: step 121380, loss = 0.49 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:02.488943: step 121390, loss = 0.50 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:02.813013: step 121400, loss = 0.57 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:03.268687: step 121410, loss = 0.51 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:03.587991: step 121420, loss = 0.67 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:03.908597: step 121430, loss = 0.57 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:04.228466: step 121440, loss = 0.46 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:04.548152: step 121450, loss = 0.67 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:04.868440: step 121460, loss = 0.48 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:05.185149: step 121470, loss = 0.43 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:05.503401: step 121480, loss = 0.56 (8157.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:05.824515: step 121490, loss = 0.48 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:06.144510: step 121500, loss = 0.61 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:06.607593: step 121510, loss = 0.59 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:06.929773: step 121520, loss = 0.56 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:07.250351: step 121530, loss = 0.51 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:07.569920: step 121540, loss = 0.57 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:07.887748: step 121550, loss = 0.42 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:08.208933: step 121560, loss = 0.59 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:08.527671: step 121570, loss = 0.64 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:08.847083: step 121580, loss = 0.57 (8121.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:09.169156: step 121590, loss = 0.53 (7548.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:48:09.489584: step 121600, loss = 0.50 (7787.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:09.941242: step 121610, loss = 0.55 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:10.262195: step 121620, loss = 0.62 (7817.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:10.583738: step 121630, loss = 0.58 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:10.903830: step 121640, loss = 0.62 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:11.223257: step 121650, loss = 0.60 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:11.542895: step 121660, loss = 0.42 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:11.862049: step 121670, loss = 0.64 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:12.188571: step 121680, loss = 0.55 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:12.508849: step 121690, loss = 0.44 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:12.830052: step 121700, loss = 0.53 (7872.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:13.292396: step 121710, loss = 0.44 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:13.613084: step 121720, loss = 0.54 (7947.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:13.932199: step 121730, loss = 0.47 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:14.252975: step 121740, loss = 0.59 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:14.570888: step 121750, loss = 0.55 (7949.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:14.891261: step 121760, loss = 0.51 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:15.212706: step 121770, loss = 0.55 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:15.533377: step 121780, loss = 0.47 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:15.853261: step 121790, loss = 0.45 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:16.171096: step 121800, loss = 0.57 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:16.617451: step 121810, loss = 0.40 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:16.938770: step 121820, loss = 0.50 (7769.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:17.259337: step 121830, loss = 0.53 (8063.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:17.579755: step 121840, loss = 0.43 (8088.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:17.899748: step 121850, loss = 0.56 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:18.218568: step 121860, loss = 0.48 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:18.538834: step 121870, loss = 0.57 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:18.861282: step 121880, loss = 0.53 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:19.182276: step 121890, loss = 0.59 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:19.503864: step 121900, loss = 0.49 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:19.962108: step 121910, loss = 0.43 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:20.280128: step 121920, loss = 0.56 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:20.603132: step 121930, loss = 0.48 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:20.923107: step 121940, loss = 0.48 (7856.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:21.241115: step 121950, loss = 0.69 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:21.562930: step 121960, loss = 0.72 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:21.882153: step 121970, loss = 0.58 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:22.203820: step 121980, loss = 0.49 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:22.525495: step 121990, loss = 0.51 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:22.845014: step 122000, loss = 0.52 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:23.396523: step 122010, loss = 0.59 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:23.719384: step 122020, loss = 0.57 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:24.043440: step 122030, loss = 0.50 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:24.363826: step 122040, loss = 0.59 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:24.684285: step 122050, loss = 0.46 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:25.004904: step 122060, loss = 0.54 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:25.325302: step 122070, loss = 0.57 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:25.646047: step 122080, loss = 0.53 (7846.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:25.966719: step 122090, loss = 0.60 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:26.287870: step 122100, loss = 0.61 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:26.748533: step 122110, loss = 0.55 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:27.068046: step 122120, loss = 0.53 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:27.387905: step 122130, loss = 0.58 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:27.709289: step 122140, loss = 0.73 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:28.032779: step 122150, loss = 0.48 (7835.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:28.353847: step 122160, loss = 0.64 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:28.674178: step 122170, loss = 0.53 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:28.995366: step 122180, loss = 0.56 (8034.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:29.319793: step 122190, loss = 0.53 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:29.640566: step 122200, loss = 0.51 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:30.097631: step 122210, loss = 0.58 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:30.418677: step 122220, loss = 0.61 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:30.738241: step 122230, loss = 0.56 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:31.058601: step 122240, loss = 0.52 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:31.381590: step 122250, loss = 0.67 (7935.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:31.703281: step 122260, loss = 0.54 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:32.024110: step 122270, loss = 0.54 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:32.344034: step 122280, loss = 0.51 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:32.666902: step 122290, loss = 0.64 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:32.987372: step 122300, loss = 0.59 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:33.446547: step 122310, loss = 0.67 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:33.767366: step 122320, loss = 0.55 (7963.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:34.091737: step 122330, loss = 0.56 (7671.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:48:34.411517: step 122340, loss = 0.44 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:34.731603: step 122350, loss = 0.64 (7811.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:35.053822: step 122360, loss = 0.54 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:35.372584: step 122370, loss = 0.48 (7925.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:35.694596: step 122380, loss = 0.56 (7754.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:48:36.017880: step 122390, loss = 0.56 (7502.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:48:36.338201: step 122400, loss = 0.51 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:36.796220: step 122410, loss = 0.53 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:37.117161: step 122420, loss = 0.53 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:37.436506: step 122430, loss = 0.65 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:37.755859: step 122440, loss = 0.52 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:38.077471: step 122450, loss = 0.49 (7972.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:38.396216: step 122460, loss = 0.50 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:38.716113: step 122470, loss = 0.58 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:39.035726: step 122480, loss = 0.54 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:39.356132: step 122490, loss = 0.53 (8057.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:39.676805: step 122500, loss = 0.55 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:40.137202: step 122510, loss = 0.55 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:40.456136: step 122520, loss = 0.45 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:40.775666: step 122530, loss = 0.59 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:41.100283: step 122540, loss = 0.57 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:41.421180: step 122550, loss = 0.65 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:41.740956: step 122560, loss = 0.61 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:42.061432: step 122570, loss = 0.60 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:42.380179: step 122580, loss = 0.59 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:42.702269: step 122590, loss = 0.60 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:43.022470: step 122600, loss = 0.67 (7974.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:43.482269: step 122610, loss = 0.54 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:43.800605: step 122620, loss = 0.59 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:44.118487: step 122630, loss = 0.47 (8128.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:44.437956: step 122640, loss = 0.55 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:44.757288: step 122650, loss = 0.63 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:45.075650: step 122660, loss = 0.66 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:45.394723: step 122670, loss = 0.56 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:45.714199: step 122680, loss = 0.59 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:46.033004: step 122690, loss = 0.49 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:46.352010: step 122700, loss = 0.53 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:46.813744: step 122710, loss = 0.61 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:47.133267: step 122720, loss = 0.59 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:47.455938: step 122730, loss = 0.62 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:47.775967: step 122740, loss = 0.50 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:48.095353: step 122750, loss = 0.64 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:48.413821: step 122760, loss = 0.63 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:48.736432: step 122770, loss = 0.56 (8196.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:49.058244: step 122780, loss = 0.62 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:49.379867: step 122790, loss = 0.51 (7966.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:49.698948: step 122800, loss = 0.59 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:50.160755: step 122810, loss = 0.46 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:50.479408: step 122820, loss = 0.53 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:50.800127: step 122830, loss = 0.85 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:51.120817: step 122840, loss = 0.55 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:51.443309: step 122850, loss = 0.56 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:51.765833: step 122860, loss = 0.56 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:52.088957: step 122870, loss = 0.56 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:52.410483: step 122880, loss = 0.56 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:52.731690: step 122890, loss = 0.57 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:53.052659: step 122900, loss = 0.51 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:53.500878: step 122910, loss = 0.50 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:53.820276: step 122920, loss = 0.57 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:54.140203: step 122930, loss = 0.58 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:54.461045: step 122940, loss = 0.60 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:54.780810: step 122950, loss = 0.54 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:55.100812: step 122960, loss = 0.53 (8117.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:55.421957: step 122970, loss = 0.51 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:55.742044: step 122980, loss = 0.69 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:56.062079: step 122990, loss = 0.56 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:56.381744: step 123000, loss = 0.57 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:56.939900: step 123010, loss = 0.58 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:57.260958: step 123020, loss = 0.48 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:57.581948: step 123030, loss = 0.66 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:57.900068: step 123040, loss = 0.59 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:58.220080: step 123050, loss = 0.50 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:58.540339: step 123060, loss = 0.61 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:58.860953: step 123070, loss = 0.52 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:59.180051: step 123080, loss = 0.66 (7894.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:59.500050: step 123090, loss = 0.62 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:48:59.818969: step 123100, loss = 0.47 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:00.277861: step 123110, loss = 0.60 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:00.595998: step 123120, loss = 0.44 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:00.915353: step 123130, loss = 0.59 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:01.235869: step 123140, loss = 0.56 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:01.556094: step 123150, loss = 0.59 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:01.876784: step 123160, loss = 0.52 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:02.196219: step 123170, loss = 0.59 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:02.515191: step 123180, loss = 0.61 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:02.835725: step 123190, loss = 0.49 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:03.156031: step 123200, loss = 0.53 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:03.614880: step 123210, loss = 0.54 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:03.934897: step 123220, loss = 0.52 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:04.254207: step 123230, loss = 0.62 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:04.573800: step 123240, loss = 0.59 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:04.893594: step 123250, loss = 0.56 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:05.214045: step 123260, loss = 0.53 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:05.535175: step 123270, loss = 0.63 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:05.854969: step 123280, loss = 0.58 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:06.175172: step 123290, loss = 0.61 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:06.494758: step 123300, loss = 0.63 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:06.950345: step 123310, loss = 0.61 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:07.270551: step 123320, loss = 0.52 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:07.589481: step 123330, loss = 0.63 (8135.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:07.909155: step 123340, loss = 0.58 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:08.233584: step 123350, loss = 0.55 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:08.557152: step 123360, loss = 0.59 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:08.877223: step 123370, loss = 0.44 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:09.200034: step 123380, loss = 0.62 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:09.520183: step 123390, loss = 0.49 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:09.838821: step 123400, loss = 0.49 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:10.288542: step 123410, loss = 0.55 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:10.607784: step 123420, loss = 0.59 (8144.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:10.927565: step 123430, loss = 0.56 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:11.246791: step 123440, loss = 0.59 (7899.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:11.565227: step 123450, loss = 0.56 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:11.883408: step 123460, loss = 0.52 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:12.203160: step 123470, loss = 0.63 (8027.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:12.523471: step 123480, loss = 0.51 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:12.843393: step 123490, loss = 0.56 (8112.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:13.163224: step 123500, loss = 0.56 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:13.620959: step 123510, loss = 0.57 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:13.941686: step 123520, loss = 0.64 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:14.259881: step 123530, loss = 0.61 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:14.580156: step 123540, loss = 0.49 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:14.900366: step 123550, loss = 0.52 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:15.220756: step 123560, loss = 0.50 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:15.539924: step 123570, loss = 0.54 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:15.860362: step 123580, loss = 0.52 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:16.180119: step 123590, loss = 0.47 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:16.499668: step 123600, loss = 0.67 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:16.960494: step 123610, loss = 0.69 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:17.281890: step 123620, loss = 0.60 (7776.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:17.601104: step 123630, loss = 0.53 (7967.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:17.921009: step 123640, loss = 0.64 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:18.242001: step 123650, loss = 0.55 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:18.560947: step 123660, loss = 0.62 (8177.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:18.879347: step 123670, loss = 0.58 (8158.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:19.205419: step 123680, loss = 0.55 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:19.528294: step 123690, loss = 0.48 (7723.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:49:19.849111: step 123700, loss = 0.54 (8094.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:20.309582: step 123710, loss = 0.53 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:20.630199: step 123720, loss = 0.51 (7823.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:20.950047: step 123730, loss = 0.56 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:21.269841: step 123740, loss = 0.48 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:21.589928: step 123750, loss = 0.59 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:21.913294: step 123760, loss = 0.52 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:22.237165: step 123770, loss = 0.64 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:22.557312: step 123780, loss = 0.50 (8166.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:22.877666: step 123790, loss = 0.58 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:23.196330: step 123800, loss = 0.52 (7982.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:23.654673: step 123810, loss = 0.60 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:23.975835: step 123820, loss = 0.67 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:24.299897: step 123830, loss = 0.49 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:24.628766: step 123840, loss = 0.59 (7387.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:49:24.947255: step 123850, loss = 0.53 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:25.267231: step 123860, loss = 0.45 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:25.588086: step 123870, loss = 0.58 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:25.907190: step 123880, loss = 0.53 (7882.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:26.225414: step 123890, loss = 0.62 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:26.548322: step 123900, loss = 0.69 (8002.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:27.019883: step 123910, loss = 0.59 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:27.339837: step 123920, loss = 0.52 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:27.663024: step 123930, loss = 0.68 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:27.983995: step 123940, loss = 0.55 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:28.305406: step 123950, loss = 0.65 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:28.625997: step 123960, loss = 0.67 (7805.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:28.944384: step 123970, loss = 0.52 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:29.263504: step 123980, loss = 0.60 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:29.582486: step 123990, loss = 0.54 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:29.906524: step 124000, loss = 0.61 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:30.462263: step 124010, loss = 0.56 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:30.781276: step 124020, loss = 0.67 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:31.100615: step 124030, loss = 0.57 (8124.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:31.421192: step 124040, loss = 0.51 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:31.742802: step 124050, loss = 0.55 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:32.063162: step 124060, loss = 0.64 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:32.384222: step 124070, loss = 0.60 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:32.706019: step 124080, loss = 0.57 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:33.024459: step 124090, loss = 0.62 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:33.343787: step 124100, loss = 0.59 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:33.800366: step 124110, loss = 0.54 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:34.120244: step 124120, loss = 0.47 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:34.442158: step 124130, loss = 0.56 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:34.761838: step 124140, loss = 0.67 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:35.082771: step 124150, loss = 0.53 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:35.403656: step 124160, loss = 0.56 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:35.723495: step 124170, loss = 0.53 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:36.043045: step 124180, loss = 0.71 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:36.363730: step 124190, loss = 0.54 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:36.687693: step 124200, loss = 0.61 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:37.139070: step 124210, loss = 0.61 (7875.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:37.459744: step 124220, loss = 0.55 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:37.779301: step 124230, loss = 0.62 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:38.098668: step 124240, loss = 0.51 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:38.417910: step 124250, loss = 0.50 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:38.739475: step 124260, loss = 0.52 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:39.059749: step 124270, loss = 0.46 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:39.378317: step 124280, loss = 0.56 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:39.702438: step 124290, loss = 0.43 (7350.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:49:40.021672: step 124300, loss = 0.71 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:40.473060: step 124310, loss = 0.61 (7834.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:40.793177: step 124320, loss = 0.64 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:41.114268: step 124330, loss = 0.51 (7771.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:41.433859: step 124340, loss = 0.54 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:41.752573: step 124350, loss = 0.62 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:42.074572: step 124360, loss = 0.66 (7725.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:49:42.392615: step 124370, loss = 0.48 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:42.714518: step 124380, loss = 0.55 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:43.033326: step 124390, loss = 0.54 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:43.354266: step 124400, loss = 0.70 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:43.809733: step 124410, loss = 0.59 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:44.131478: step 124420, loss = 0.64 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:44.450782: step 124430, loss = 0.47 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:44.770855: step 124440, loss = 0.58 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:45.091256: step 124450, loss = 0.48 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:45.409383: step 124460, loss = 0.53 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:45.726909: step 124470, loss = 0.61 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:46.045117: step 124480, loss = 0.56 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:46.364606: step 124490, loss = 0.54 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:46.685045: step 124500, loss = 0.50 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:47.129050: step 124510, loss = 0.56 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:47.451372: step 124520, loss = 0.54 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:47.775221: step 124530, loss = 0.48 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:48.095155: step 124540, loss = 0.57 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:48.413415: step 124550, loss = 0.61 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:48.734163: step 124560, loss = 0.55 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:49.054332: step 124570, loss = 0.62 (7833.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:49.376846: step 124580, loss = 0.49 (7617.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:49:49.695466: step 124590, loss = 0.48 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:50.017573: step 124600, loss = 0.62 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:50.472969: step 124610, loss = 0.67 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:50.796977: step 124620, loss = 0.65 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:51.116771: step 124630, loss = 0.50 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:51.434793: step 124640, loss = 0.61 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:51.754678: step 124650, loss = 0.57 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:52.075956: step 124660, loss = 0.51 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:52.400134: step 124670, loss = 0.52 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:52.720811: step 124680, loss = 0.68 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:53.038799: step 124690, loss = 0.56 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:53.357386: step 124700, loss = 0.54 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:53.808122: step 124710, loss = 0.55 (7853.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:54.126364: step 124720, loss = 0.58 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:54.445935: step 124730, loss = 0.55 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:54.764369: step 124740, loss = 0.64 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:55.086129: step 124750, loss = 0.53 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:55.404049: step 124760, loss = 0.65 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:55.723133: step 124770, loss = 0.58 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:56.043905: step 124780, loss = 0.52 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:56.362817: step 124790, loss = 0.60 (8020.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:56.681994: step 124800, loss = 0.52 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:57.135227: step 124810, loss = 0.55 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:57.454166: step 124820, loss = 0.60 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:57.773081: step 124830, loss = 0.53 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:58.092936: step 124840, loss = 0.55 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:58.415415: step 124850, loss = 0.72 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:58.734464: step 124860, loss = 0.65 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:59.056163: step 124870, loss = 0.67 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:59.377304: step 124880, loss = 0.63 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:49:59.699032: step 124890, loss = 0.48 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:00.022108: step 124900, loss = 0.56 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:00.480226: step 124910, loss = 0.68 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:00.803581: step 124920, loss = 0.58 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:01.123631: step 124930, loss = 0.65 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:01.442904: step 124940, loss = 0.52 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:01.766574: step 124950, loss = 0.54 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:02.084945: step 124960, loss = 0.63 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:02.403999: step 124970, loss = 0.52 (8006.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:02.723683: step 124980, loss = 0.56 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:03.044751: step 124990, loss = 0.58 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:03.363729: step 125000, loss = 0.52 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:03.912110: step 125010, loss = 0.60 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:04.230821: step 125020, loss = 0.67 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:04.552306: step 125030, loss = 0.71 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:04.870819: step 125040, loss = 0.45 (8172.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:05.190290: step 125050, loss = 0.58 (7802.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:05.511387: step 125060, loss = 0.61 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:05.830380: step 125070, loss = 0.61 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:06.150610: step 125080, loss = 0.60 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:06.471133: step 125090, loss = 0.56 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:06.792104: step 125100, loss = 0.45 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:07.244450: step 125110, loss = 0.68 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:07.565385: step 125120, loss = 0.52 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:07.885972: step 125130, loss = 0.54 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:08.206904: step 125140, loss = 0.55 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:08.527740: step 125150, loss = 0.49 (7817.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:08.848749: step 125160, loss = 0.59 (8155.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:09.168900: step 125170, loss = 0.63 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:09.489452: step 125180, loss = 0.55 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:09.810002: step 125190, loss = 0.60 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:10.128368: step 125200, loss = 0.58 (7988.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:10.578503: step 125210, loss = 0.59 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:10.899969: step 125220, loss = 0.68 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:11.218679: step 125230, loss = 0.69 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:11.541519: step 125240, loss = 0.57 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:11.860114: step 125250, loss = 0.57 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:12.181252: step 125260, loss = 0.58 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:12.502775: step 125270, loss = 0.57 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:12.821942: step 125280, loss = 0.47 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:13.140453: step 125290, loss = 0.60 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:13.461697: step 125300, loss = 0.57 (7821.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:13.920777: step 125310, loss = 0.45 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:14.240407: step 125320, loss = 0.54 (7808.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:14.559019: step 125330, loss = 0.68 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:14.879534: step 125340, loss = 0.55 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:15.197510: step 125350, loss = 0.48 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:15.518763: step 125360, loss = 0.48 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:15.841860: step 125370, loss = 0.57 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:16.161059: step 125380, loss = 0.49 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:16.479585: step 125390, loss = 0.48 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:16.799949: step 125400, loss = 0.59 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:17.245531: step 125410, loss = 0.48 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:17.565160: step 125420, loss = 0.53 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:17.884416: step 125430, loss = 0.57 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:18.203562: step 125440, loss = 0.53 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:18.522106: step 125450, loss = 0.53 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:18.842008: step 125460, loss = 0.49 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:19.161746: step 125470, loss = 0.71 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:19.482800: step 125480, loss = 0.55 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:19.804205: step 125490, loss = 0.58 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:20.123388: step 125500, loss = 0.64 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:20.582045: step 125510, loss = 0.52 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:20.901007: step 125520, loss = 0.48 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:21.220796: step 125530, loss = 0.63 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:21.540388: step 125540, loss = 0.60 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:21.859350: step 125550, loss = 0.60 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:22.179356: step 125560, loss = 0.62 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:22.500888: step 125570, loss = 0.61 (7446.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:22.822068: step 125580, loss = 0.63 (7778.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:23.145352: step 125590, loss = 0.58 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:23.465443: step 125600, loss = 0.62 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:23.917187: step 125610, loss = 0.54 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:24.236737: step 125620, loss = 0.49 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:24.554736: step 125630, loss = 0.64 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:24.876195: step 125640, loss = 0.60 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:25.196810: step 125650, loss = 0.52 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:25.519913: step 125660, loss = 0.53 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:25.843299: step 125670, loss = 0.64 (7814.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:26.162989: step 125680, loss = 0.71 (8158.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:26.484189: step 125690, loss = 0.71 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:26.804572: step 125700, loss = 0.60 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:27.263907: step 125710, loss = 0.60 (7797.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:27.584779: step 125720, loss = 0.61 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:27.907704: step 125730, loss = 0.55 (7456.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:28.228422: step 125740, loss = 0.57 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:28.548206: step 125750, loss = 0.74 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:28.868170: step 125760, loss = 0.52 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:29.186769: step 125770, loss = 0.55 (7742.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:29.504879: step 125780, loss = 0.68 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:29.826256: step 125790, loss = 0.56 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:30.146090: step 125800, loss = 0.46 (7872.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:30.599738: step 125810, loss = 0.51 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:30.924002: step 125820, loss = 0.48 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:31.246400: step 125830, loss = 0.48 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:31.564538: step 125840, loss = 0.53 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:31.883600: step 125850, loss = 0.57 (8146.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:32.203969: step 125860, loss = 0.52 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:32.522959: step 125870, loss = 0.72 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:32.841881: step 125880, loss = 0.59 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:33.159993: step 125890, loss = 0.58 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:33.481908: step 125900, loss = 0.63 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:33.944286: step 125910, loss = 0.49 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:34.262130: step 125920, loss = 0.42 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:34.584311: step 125930, loss = 0.52 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:34.904600: step 125940, loss = 0.62 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:35.224401: step 125950, loss = 0.65 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:35.545855: step 125960, loss = 0.49 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:35.866872: step 125970, loss = 0.55 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:36.185380: step 125980, loss = 0.49 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:36.507147: step 125990, loss = 0.60 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:36.828729: step 126000, loss = 0.63 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:37.439111: step 126010, loss = 0.54 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:37.760467: step 126020, loss = 0.78 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:38.081338: step 126030, loss = 0.46 (7493.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:38.403892: step 126040, loss = 0.71 (8141.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:38.726743: step 126050, loss = 0.57 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:39.044673: step 126060, loss = 0.55 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:39.369504: step 126070, loss = 0.53 (7786.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:39.689376: step 126080, loss = 0.57 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:40.008743: step 126090, loss = 0.66 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:40.329026: step 126100, loss = 0.46 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:40.787626: step 126110, loss = 0.55 (7676.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:41.109058: step 126120, loss = 0.56 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:41.433443: step 126130, loss = 0.63 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:41.751729: step 126140, loss = 0.58 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:42.072418: step 126150, loss = 0.52 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:42.392876: step 126160, loss = 0.61 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:42.712099: step 126170, loss = 0.58 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:43.033874: step 126180, loss = 0.57 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:43.354353: step 126190, loss = 0.40 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:43.680165: step 126200, loss = 0.63 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:44.141009: step 126210, loss = 0.48 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:44.463721: step 126220, loss = 0.67 (7749.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:44.783982: step 126230, loss = 0.54 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:45.104581: step 126240, loss = 0.43 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:45.424177: step 126250, loss = 0.64 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:45.746047: step 126260, loss = 0.68 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:46.067969: step 126270, loss = 0.69 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:46.389393: step 126280, loss = 0.51 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:46.711712: step 126290, loss = 0.45 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:47.033512: step 126300, loss = 0.51 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:47.490111: step 126310, loss = 0.53 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:47.809821: step 126320, loss = 0.48 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:48.129948: step 126330, loss = 0.58 (7826.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:48.456444: step 126340, loss = 0.60 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:48.778252: step 126350, loss = 0.59 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:49.097727: step 126360, loss = 0.45 (8140.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:49.418948: step 126370, loss = 0.58 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:49.738375: step 126380, loss = 0.59 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:50.060527: step 126390, loss = 0.56 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:50.382117: step 126400, loss = 0.54 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:50.845861: step 126410, loss = 0.55 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:51.172264: step 126420, loss = 0.53 (7888.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:51.492255: step 126430, loss = 0.50 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:51.810633: step 126440, loss = 0.65 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:52.130119: step 126450, loss = 0.52 (7795.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:52.450285: step 126460, loss = 0.50 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:52.770464: step 126470, loss = 0.48 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:53.091372: step 126480, loss = 0.61 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:53.412996: step 126490, loss = 0.63 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:53.732578: step 126500, loss = 0.54 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:54.185162: step 126510, loss = 0.44 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:54.503229: step 126520, loss = 0.64 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:54.826892: step 126530, loss = 0.56 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:55.148309: step 126540, loss = 0.53 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:55.468114: step 126550, loss = 0.56 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:55.787567: step 126560, loss = 0.57 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:56.106031: step 126570, loss = 0.70 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:56.427676: step 126580, loss = 0.66 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:56.750000: step 126590, loss = 0.54 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:57.071846: step 126600, loss = 0.58 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:57.521218: step 126610, loss = 0.53 (8002.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:57.839905: step 126620, loss = 0.56 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:58.159281: step 126630, loss = 0.46 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:58.480620: step 126640, loss = 0.61 (7566.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:50:58.803126: step 126650, loss = 0.52 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:59.121998: step 126660, loss = 0.52 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:59.443252: step 126670, loss = 0.45 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:50:59.763359: step 126680, loss = 0.50 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:00.084837: step 126690, loss = 0.53 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:00.403974: step 126700, loss = 0.45 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:00.851161: step 126710, loss = 0.61 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:01.170391: step 126720, loss = 0.53 (8014.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:01.489560: step 126730, loss = 0.66 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:01.809535: step 126740, loss = 0.50 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:02.128450: step 126750, loss = 0.51 (7945.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:02.449207: step 126760, loss = 0.52 (7888.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:02.769423: step 126770, loss = 0.65 (8194.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:03.091294: step 126780, loss = 0.63 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:03.412963: step 126790, loss = 0.63 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:03.731555: step 126800, loss = 0.60 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:04.183230: step 126810, loss = 0.55 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:04.503945: step 126820, loss = 0.54 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:04.825600: step 126830, loss = 0.49 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:05.145697: step 126840, loss = 0.54 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:05.464824: step 126850, loss = 0.60 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:05.786621: step 126860, loss = 0.47 (7788.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:06.111677: step 126870, loss = 0.52 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:06.431421: step 126880, loss = 0.60 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:06.753964: step 126890, loss = 0.66 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:07.074873: step 126900, loss = 0.50 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:07.536997: step 126910, loss = 0.44 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:07.855848: step 126920, loss = 0.65 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:08.176633: step 126930, loss = 0.49 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:08.494826: step 126940, loss = 0.46 (8034.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:08.815996: step 126950, loss = 0.64 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:09.137033: step 126960, loss = 0.50 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:09.458863: step 126970, loss = 0.48 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:09.779157: step 126980, loss = 0.67 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:10.098576: step 126990, loss = 0.64 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:10.422839: step 127000, loss = 0.62 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:10.978712: step 127010, loss = 0.52 (7899.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:11.298015: step 127020, loss = 0.56 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:11.616585: step 127030, loss = 0.58 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:11.937987: step 127040, loss = 0.59 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:12.259884: step 127050, loss = 0.47 (7818.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:12.581664: step 127060, loss = 0.55 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:12.902455: step 127070, loss = 0.56 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:13.223748: step 127080, loss = 0.48 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:13.544419: step 127090, loss = 0.50 (8041.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:13.865635: step 127100, loss = 0.41 (7904.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:14.324355: step 127110, loss = 0.64 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:14.644613: step 127120, loss = 0.61 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:14.964346: step 127130, loss = 0.62 (7959.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:15.284667: step 127140, loss = 0.45 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:15.605394: step 127150, loss = 0.58 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:15.925210: step 127160, loss = 0.46 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:16.245614: step 127170, loss = 0.43 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:16.564864: step 127180, loss = 0.50 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:16.888342: step 127190, loss = 0.67 (7759.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:17.209874: step 127200, loss = 0.52 (7448.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:51:17.668195: step 127210, loss = 0.64 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:17.987365: step 127220, loss = 0.55 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:18.307224: step 127230, loss = 0.54 (7849.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:18.628928: step 127240, loss = 0.56 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:18.950092: step 127250, loss = 0.50 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:19.271379: step 127260, loss = 0.60 (7792.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:19.591767: step 127270, loss = 0.54 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:19.913115: step 127280, loss = 0.57 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:20.233786: step 127290, loss = 0.64 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:20.553507: step 127300, loss = 0.74 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:21.011267: step 127310, loss = 0.48 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:21.331265: step 127320, loss = 0.61 (8166.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:21.652652: step 127330, loss = 0.55 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:21.973347: step 127340, loss = 0.54 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:22.293153: step 127350, loss = 0.50 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:22.612931: step 127360, loss = 0.50 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:22.933313: step 127370, loss = 0.57 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:23.252567: step 127380, loss = 0.59 (7812.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:23.571728: step 127390, loss = 0.55 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:23.894261: step 127400, loss = 0.50 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:24.347497: step 127410, loss = 0.69 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:24.670723: step 127420, loss = 0.56 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:24.990062: step 127430, loss = 0.61 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:25.309139: step 127440, loss = 0.60 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:25.630000: step 127450, loss = 0.58 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:25.953971: step 127460, loss = 0.58 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:26.275074: step 127470, loss = 0.51 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:26.595858: step 127480, loss = 0.53 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:26.917222: step 127490, loss = 0.56 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:27.238881: step 127500, loss = 0.58 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:27.696740: step 127510, loss = 0.51 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:28.015414: step 127520, loss = 0.57 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:28.334836: step 127530, loss = 0.74 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:28.654123: step 127540, loss = 0.53 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:28.974406: step 127550, loss = 0.48 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:29.298928: step 127560, loss = 0.58 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:29.619660: step 127570, loss = 0.52 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:29.940415: step 127580, loss = 0.55 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:30.261057: step 127590, loss = 0.52 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:30.580385: step 127600, loss = 0.55 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:31.043979: step 127610, loss = 0.60 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:31.364549: step 127620, loss = 0.50 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:31.682845: step 127630, loss = 0.63 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:32.006506: step 127640, loss = 0.53 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:32.326437: step 127650, loss = 0.55 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:32.651634: step 127660, loss = 0.61 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:32.971118: step 127670, loss = 0.50 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:33.291419: step 127680, loss = 0.60 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:33.611470: step 127690, loss = 0.57 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:33.932291: step 127700, loss = 0.58 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:34.394218: step 127710, loss = 0.50 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:34.713660: step 127720, loss = 0.48 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:35.033308: step 127730, loss = 0.57 (8164.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:35.351883: step 127740, loss = 0.49 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:35.672529: step 127750, loss = 0.61 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:35.990529: step 127760, loss = 0.49 (8111.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:36.308719: step 127770, loss = 0.51 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:36.630382: step 127780, loss = 0.59 (7915.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:36.951240: step 127790, loss = 0.69 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:37.269123: step 127800, loss = 0.56 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:37.726060: step 127810, loss = 0.60 (7935.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:38.045456: step 127820, loss = 0.53 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:38.363928: step 127830, loss = 0.53 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:38.686494: step 127840, loss = 0.39 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:39.011880: step 127850, loss = 0.55 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:39.331404: step 127860, loss = 0.58 (8001.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:39.652713: step 127870, loss = 0.59 (7816.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:39.973333: step 127880, loss = 0.52 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:40.296850: step 127890, loss = 0.54 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:40.617809: step 127900, loss = 0.55 (7884.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:41.080882: step 127910, loss = 0.51 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:41.400610: step 127920, loss = 0.57 (8130.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:41.721077: step 127930, loss = 0.70 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:42.042219: step 127940, loss = 0.54 (7701.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:51:42.361122: step 127950, loss = 0.57 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:42.679245: step 127960, loss = 0.55 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:43.002086: step 127970, loss = 0.67 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:43.322470: step 127980, loss = 0.53 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:43.643350: step 127990, loss = 0.57 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:43.962011: step 128000, loss = 0.68 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:44.527053: step 128010, loss = 0.46 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:44.847640: step 128020, loss = 0.67 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:45.167559: step 128030, loss = 0.49 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:45.489425: step 128040, loss = 0.54 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:45.810569: step 128050, loss = 0.52 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:46.130253: step 128060, loss = 0.50 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:46.450947: step 128070, loss = 0.64 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:46.769647: step 128080, loss = 0.45 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:47.089477: step 128090, loss = 0.51 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:47.410577: step 128100, loss = 0.52 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:47.862146: step 128110, loss = 0.48 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:48.180737: step 128120, loss = 0.50 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:48.500727: step 128130, loss = 0.59 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:48.823018: step 128140, loss = 0.56 (7783.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:49.144723: step 128150, loss = 0.54 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:49.465502: step 128160, loss = 0.56 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:49.785205: step 128170, loss = 0.48 (8119.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:50.104169: step 128180, loss = 0.43 (7905.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:50.425134: step 128190, loss = 0.52 (7943.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:50.747248: step 128200, loss = 0.59 (7581.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:51:51.207597: step 128210, loss = 0.51 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:51.531487: step 128220, loss = 0.62 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:51.854289: step 128230, loss = 0.47 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:52.174014: step 128240, loss = 0.73 (8121.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:52.493523: step 128250, loss = 0.57 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:52.815622: step 128260, loss = 0.52 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:53.138333: step 128270, loss = 0.51 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:53.459747: step 128280, loss = 0.49 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:53.781261: step 128290, loss = 0.60 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:54.107455: step 128300, loss = 0.53 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:54.557694: step 128310, loss = 0.58 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:54.879000: step 128320, loss = 0.44 (8080.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:55.198253: step 128330, loss = 0.50 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:55.519224: step 128340, loss = 0.53 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:55.841649: step 128350, loss = 0.52 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:56.162087: step 128360, loss = 0.54 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:56.483077: step 128370, loss = 0.56 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:56.807808: step 128380, loss = 0.55 (8141.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:57.129958: step 128390, loss = 0.47 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:57.449956: step 128400, loss = 0.49 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:57.895425: step 128410, loss = 0.55 (7964.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:58.216843: step 128420, loss = 0.52 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:58.537143: step 128430, loss = 0.46 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:58.863957: step 128440, loss = 0.48 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:59.182306: step 128450, loss = 0.50 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:59.503697: step 128460, loss = 0.52 (8147.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:51:59.825472: step 128470, loss = 0.63 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:00.146270: step 128480, loss = 0.59 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:00.468661: step 128490, loss = 0.50 (7907.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:00.787824: step 128500, loss = 0.47 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:01.250580: step 128510, loss = 0.51 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:01.572628: step 128520, loss = 0.44 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:01.894911: step 128530, loss = 0.52 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:02.215853: step 128540, loss = 0.56 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:02.537870: step 128550, loss = 0.49 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:02.859786: step 128560, loss = 0.50 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:03.182722: step 128570, loss = 0.55 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:03.506669: step 128580, loss = 0.63 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:03.830349: step 128590, loss = 0.65 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:04.149856: step 128600, loss = 0.52 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:04.605828: step 128610, loss = 0.58 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:04.929641: step 128620, loss = 0.61 (7929.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:05.252836: step 128630, loss = 0.49 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:05.576520: step 128640, loss = 0.55 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:05.896693: step 128650, loss = 0.62 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:06.218909: step 128660, loss = 0.62 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:06.540232: step 128670, loss = 0.47 (7598.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:06.862314: step 128680, loss = 0.65 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:07.182023: step 128690, loss = 0.60 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:07.500680: step 128700, loss = 0.53 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:07.960771: step 128710, loss = 0.53 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:08.283996: step 128720, loss = 0.64 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:08.606509: step 128730, loss = 0.47 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:08.927733: step 128740, loss = 0.51 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:09.249164: step 128750, loss = 0.52 (7812.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:09.569859: step 128760, loss = 0.57 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:09.889326: step 128770, loss = 0.50 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:10.214328: step 128780, loss = 0.66 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:10.534170: step 128790, loss = 0.51 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:10.853560: step 128800, loss = 0.65 (7666.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:11.305736: step 128810, loss = 0.56 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:11.631702: step 128820, loss = 0.68 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:11.950093: step 128830, loss = 0.52 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:12.269362: step 128840, loss = 0.58 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:12.590161: step 128850, loss = 0.62 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:12.911580: step 128860, loss = 0.53 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:13.231243: step 128870, loss = 0.51 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:13.550611: step 128880, loss = 0.54 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:13.874225: step 128890, loss = 0.64 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:14.194112: step 128900, loss = 0.53 (8103.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:14.651617: step 128910, loss = 0.52 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:14.981890: step 128920, loss = 0.58 (7751.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:15.303209: step 128930, loss = 0.68 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:15.625601: step 128940, loss = 0.54 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:15.947412: step 128950, loss = 0.50 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:16.267294: step 128960, loss = 0.52 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:16.588413: step 128970, loss = 0.44 (7779.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:16.914791: step 128980, loss = 0.54 (7796.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:17.234810: step 128990, loss = 0.47 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:17.556414: step 129000, loss = 0.54 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:18.103605: step 129010, loss = 0.74 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:18.422323: step 129020, loss = 0.65 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:18.741625: step 129030, loss = 0.70 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:19.065106: step 129040, loss = 0.52 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:19.392952: step 129050, loss = 0.70 (7623.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:19.713581: step 129060, loss = 0.49 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:20.035199: step 129070, loss = 0.54 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:20.358005: step 129080, loss = 0.51 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:20.678599: step 129090, loss = 0.58 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:20.999864: step 129100, loss = 0.53 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:21.461874: step 129110, loss = 0.55 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:21.783409: step 129120, loss = 0.59 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:22.105310: step 129130, loss = 0.63 (8128.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:22.423970: step 129140, loss = 0.56 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:22.744405: step 129150, loss = 0.58 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:23.064597: step 129160, loss = 0.52 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:23.386779: step 129170, loss = 0.57 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:23.707340: step 129180, loss = 0.56 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:24.029436: step 129190, loss = 0.44 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:24.348492: step 129200, loss = 0.57 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:24.801295: step 129210, loss = 0.50 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:25.120321: step 129220, loss = 0.50 (7847.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:25.440685: step 129230, loss = 0.52 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:25.759959: step 129240, loss = 0.58 (8146.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:26.079285: step 129250, loss = 0.57 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:26.400080: step 129260, loss = 0.60 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:26.718313: step 129270, loss = 0.70 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:27.036315: step 129280, loss = 0.50 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:27.359517: step 129290, loss = 0.67 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:27.682069: step 129300, loss = 0.69 (7853.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:28.135783: step 129310, loss = 0.48 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:28.456573: step 129320, loss = 0.67 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:28.778080: step 129330, loss = 0.51 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:29.098796: step 129340, loss = 0.54 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:29.418867: step 129350, loss = 0.56 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:29.737619: step 129360, loss = 0.53 (8044.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:30.061307: step 129370, loss = 0.50 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:30.383604: step 129380, loss = 0.57 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:30.708403: step 129390, loss = 0.61 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:31.029621: step 129400, loss = 0.58 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:31.489820: step 129410, loss = 0.53 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:31.810641: step 129420, loss = 0.51 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:32.129900: step 129430, loss = 0.53 (7804.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:32.452443: step 129440, loss = 0.53 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:32.773455: step 129450, loss = 0.54 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:33.095710: step 129460, loss = 0.64 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:33.415037: step 129470, loss = 0.66 (7916.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:33.733851: step 129480, loss = 0.73 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:34.051834: step 129490, loss = 0.48 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:34.371244: step 129500, loss = 0.67 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:34.817116: step 129510, loss = 0.57 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:35.145424: step 129520, loss = 0.56 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:35.466935: step 129530, loss = 0.55 (7551.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:35.787727: step 129540, loss = 0.55 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:36.108375: step 129550, loss = 0.65 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:36.429159: step 129560, loss = 0.50 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:36.748169: step 129570, loss = 0.55 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:37.067946: step 129580, loss = 0.56 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:37.388115: step 129590, loss = 0.44 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:37.710268: step 129600, loss = 0.46 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:38.158416: step 129610, loss = 0.51 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:38.478614: step 129620, loss = 0.67 (7715.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:38.798071: step 129630, loss = 0.58 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:39.122709: step 129640, loss = 0.50 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:39.442461: step 129650, loss = 0.64 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:39.762581: step 129660, loss = 0.54 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:40.083957: step 129670, loss = 0.45 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:40.402109: step 129680, loss = 0.44 (8047.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:40.723883: step 129690, loss = 0.52 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:41.045437: step 129700, loss = 0.72 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:41.503097: step 129710, loss = 0.60 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:41.824955: step 129720, loss = 0.58 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:42.143969: step 129730, loss = 0.47 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:42.465043: step 129740, loss = 0.57 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:42.787384: step 129750, loss = 0.70 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:43.105330: step 129760, loss = 0.51 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:43.424832: step 129770, loss = 0.61 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:43.745484: step 129780, loss = 0.58 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:44.064499: step 129790, loss = 0.48 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:44.383041: step 129800, loss = 0.54 (8177.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:44.841713: step 129810, loss = 0.45 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:45.161235: step 129820, loss = 0.47 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:45.481248: step 129830, loss = 0.55 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:45.800229: step 129840, loss = 0.63 (8172.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:46.121725: step 129850, loss = 0.50 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:46.443302: step 129860, loss = 0.58 (7832.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:46.764499: step 129870, loss = 0.52 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:47.083867: step 129880, loss = 0.51 (7909.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:47.403615: step 129890, loss = 0.48 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:47.725061: step 129900, loss = 0.51 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:48.182151: step 129910, loss = 0.59 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:48.500976: step 129920, loss = 0.45 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:48.818280: step 129930, loss = 0.45 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:49.138063: step 129940, loss = 0.62 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:49.457113: step 129950, loss = 0.58 (8145.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:49.775902: step 129960, loss = 0.66 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:50.095247: step 129970, loss = 0.58 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:50.415334: step 129980, loss = 0.63 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:50.734672: step 129990, loss = 0.50 (8013.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:51.054336: step 130000, loss = 0.44 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:51.617496: step 130010, loss = 0.61 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:51.940072: step 130020, loss = 0.50 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:52.259299: step 130030, loss = 0.47 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:52.579118: step 130040, loss = 0.53 (7826.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:52.899889: step 130050, loss = 0.60 (8150.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:53.221703: step 130060, loss = 0.60 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:53.542857: step 130070, loss = 0.61 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:53.861816: step 130080, loss = 0.66 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:54.183374: step 130090, loss = 0.53 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:54.509968: step 130100, loss = 0.54 (7653.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:54.969370: step 130110, loss = 0.59 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:55.288260: step 130120, loss = 0.52 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:55.607177: step 130130, loss = 0.53 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:55.928555: step 130140, loss = 0.48 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:56.250360: step 130150, loss = 0.59 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:56.572777: step 130160, loss = 0.55 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:56.895503: step 130170, loss = 0.49 (7526.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:52:57.214096: step 130180, loss = 0.51 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:57.537122: step 130190, loss = 0.52 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:57.856557: step 130200, loss = 0.65 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:58.310366: step 130210, loss = 0.64 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:58.629829: step 130220, loss = 0.43 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:58.949375: step 130230, loss = 0.62 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:59.268498: step 130240, loss = 0.58 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:59.588409: step 130250, loss = 0.52 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:52:59.911646: step 130260, loss = 0.61 (7753.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:53:00.231307: step 130270, loss = 0.50 (7985.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:00.552153: step 130280, loss = 0.58 (7963.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:00.872640: step 130290, loss = 0.58 (8050.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:01.192604: step 130300, loss = 0.64 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:01.654103: step 130310, loss = 0.50 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:01.974122: step 130320, loss = 0.52 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:02.295071: step 130330, loss = 0.65 (8026.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:02.614231: step 130340, loss = 0.46 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:02.934013: step 130350, loss = 0.65 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:03.252707: step 130360, loss = 0.59 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:03.573656: step 130370, loss = 0.56 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:03.896572: step 130380, loss = 0.65 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:04.214897: step 130390, loss = 0.51 (7833.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:04.536108: step 130400, loss = 0.48 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:04.995088: step 130410, loss = 0.56 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:05.315831: step 130420, loss = 0.53 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:05.637134: step 130430, loss = 0.54 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:05.958548: step 130440, loss = 0.41 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:06.280496: step 130450, loss = 0.72 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:06.599290: step 130460, loss = 0.60 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:06.920989: step 130470, loss = 0.52 (8005.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:07.244176: step 130480, loss = 0.50 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:07.563151: step 130490, loss = 0.44 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:07.883225: step 130500, loss = 0.50 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:08.339569: step 130510, loss = 0.48 (7949.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:08.659597: step 130520, loss = 0.55 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:08.979493: step 130530, loss = 0.53 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:09.299528: step 130540, loss = 0.57 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:09.617739: step 130550, loss = 0.58 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:09.939271: step 130560, loss = 0.65 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:10.259206: step 130570, loss = 0.62 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:10.582051: step 130580, loss = 0.60 (7970.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:10.902639: step 130590, loss = 0.57 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:11.222729: step 130600, loss = 0.56 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:11.675384: step 130610, loss = 0.52 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:11.995934: step 130620, loss = 0.42 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:12.317347: step 130630, loss = 0.57 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:12.637628: step 130640, loss = 0.59 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:12.958292: step 130650, loss = 0.48 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:13.279143: step 130660, loss = 0.51 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:13.598477: step 130670, loss = 0.56 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:13.918546: step 130680, loss = 0.44 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:14.238849: step 130690, loss = 0.57 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:14.558057: step 130700, loss = 0.55 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:15.012146: step 130710, loss = 0.53 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:15.330839: step 130720, loss = 0.50 (8177.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:15.650185: step 130730, loss = 0.59 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:15.969632: step 130740, loss = 0.53 (8152.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:16.288651: step 130750, loss = 0.50 (7567.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:53:16.611965: step 130760, loss = 0.43 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:16.934233: step 130770, loss = 0.57 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:17.255635: step 130780, loss = 0.55 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:17.576224: step 130790, loss = 0.53 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:17.893981: step 130800, loss = 0.70 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:18.343607: step 130810, loss = 0.47 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:18.664650: step 130820, loss = 0.56 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:18.983402: step 130830, loss = 0.56 (8191.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:19.303751: step 130840, loss = 0.68 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:19.623420: step 130850, loss = 0.42 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:19.942565: step 130860, loss = 0.52 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:20.264796: step 130870, loss = 0.46 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:20.584763: step 130880, loss = 0.48 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:20.904702: step 130890, loss = 0.44 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:21.225736: step 130900, loss = 0.47 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:21.689323: step 130910, loss = 0.65 (7646.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:53:22.013827: step 130920, loss = 0.54 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:22.332932: step 130930, loss = 0.55 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:22.652762: step 130940, loss = 0.68 (7502.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:53:22.974027: step 130950, loss = 0.56 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:23.297885: step 130960, loss = 0.49 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:23.621400: step 130970, loss = 0.61 (7885.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:23.942230: step 130980, loss = 0.51 (8142.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:24.264967: step 130990, loss = 0.57 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:24.584646: step 131000, loss = 0.72 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:25.129068: step 131010, loss = 0.46 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:25.448736: step 131020, loss = 0.61 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:25.768553: step 131030, loss = 0.45 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:26.087964: step 131040, loss = 0.55 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:26.410888: step 131050, loss = 0.62 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:26.730428: step 131060, loss = 0.53 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:27.049949: step 131070, loss = 0.62 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:27.370365: step 131080, loss = 0.48 (8140.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:27.694199: step 131090, loss = 0.54 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:28.013047: step 131100, loss = 0.64 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:28.459774: step 131110, loss = 0.67 (7945.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:28.780575: step 131120, loss = 0.56 (7802.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:29.098394: step 131130, loss = 0.62 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:29.418857: step 131140, loss = 0.60 (7835.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:29.741978: step 131150, loss = 0.58 (8124.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:30.065766: step 131160, loss = 0.62 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:30.388561: step 131170, loss = 0.45 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:30.712443: step 131180, loss = 0.57 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:31.032070: step 131190, loss = 0.52 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:31.351823: step 131200, loss = 0.56 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:31.809712: step 131210, loss = 0.55 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:32.131196: step 131220, loss = 0.51 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:32.452495: step 131230, loss = 0.61 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:32.774084: step 131240, loss = 0.51 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:33.093226: step 131250, loss = 0.54 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:33.411441: step 131260, loss = 0.70 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:33.732499: step 131270, loss = 0.59 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:34.053554: step 131280, loss = 0.46 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:34.374528: step 131290, loss = 0.56 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:34.696370: step 131300, loss = 0.66 (8093.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:35.142721: step 131310, loss = 0.62 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:35.462490: step 131320, loss = 0.58 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:35.782103: step 131330, loss = 0.50 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:36.101553: step 131340, loss = 0.66 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:36.421299: step 131350, loss = 0.52 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:36.742843: step 131360, loss = 0.55 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:37.064333: step 131370, loss = 0.48 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:37.383073: step 131380, loss = 0.64 (8184.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:37.702074: step 131390, loss = 0.64 (7866.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:38.022359: step 131400, loss = 0.58 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:38.475202: step 131410, loss = 0.55 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:38.794498: step 131420, loss = 0.46 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:39.113945: step 131430, loss = 0.58 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:39.433787: step 131440, loss = 0.44 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:39.758277: step 131450, loss = 0.62 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:40.082508: step 131460, loss = 0.61 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:40.403200: step 131470, loss = 0.57 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:40.723997: step 131480, loss = 0.49 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:41.044653: step 131490, loss = 0.43 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:41.363779: step 131500, loss = 0.58 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:41.823967: step 131510, loss = 0.54 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:42.142876: step 131520, loss = 0.44 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:42.460478: step 131530, loss = 0.61 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:42.778437: step 131540, loss = 0.54 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:43.098215: step 131550, loss = 0.58 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:43.418214: step 131560, loss = 0.61 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:43.737949: step 131570, loss = 0.66 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:44.058555: step 131580, loss = 0.48 (8055.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:44.376944: step 131590, loss = 0.58 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:44.697161: step 131600, loss = 0.56 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:45.151402: step 131610, loss = 0.54 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:45.473449: step 131620, loss = 0.53 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:45.793042: step 131630, loss = 0.63 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:46.117797: step 131640, loss = 0.55 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:46.438658: step 131650, loss = 0.60 (7973.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:46.758373: step 131660, loss = 0.47 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:47.077166: step 131670, loss = 0.62 (8020.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:47.395889: step 131680, loss = 0.67 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:47.716965: step 131690, loss = 0.44 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:48.035221: step 131700, loss = 0.51 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:48.488449: step 131710, loss = 0.56 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:48.810398: step 131720, loss = 0.59 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:49.129990: step 131730, loss = 0.56 (7996.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:49.450046: step 131740, loss = 0.53 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:49.770577: step 131750, loss = 0.42 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:50.088477: step 131760, loss = 0.45 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:50.407624: step 131770, loss = 0.56 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:50.728414: step 131780, loss = 0.55 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:51.047903: step 131790, loss = 0.58 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:51.370162: step 131800, loss = 0.52 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:51.830412: step 131810, loss = 0.64 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:52.149557: step 131820, loss = 0.52 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:52.471893: step 131830, loss = 0.64 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:52.791112: step 131840, loss = 0.58 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:53.112449: step 131850, loss = 0.56 (7937.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:53.431257: step 131860, loss = 0.54 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:53.751868: step 131870, loss = 0.45 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:54.073454: step 131880, loss = 0.66 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:54.393542: step 131890, loss = 0.60 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:54.712562: step 131900, loss = 0.50 (8128.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:55.174236: step 131910, loss = 0.55 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:55.495023: step 131920, loss = 0.61 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:55.816514: step 131930, loss = 0.51 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:56.137535: step 131940, loss = 0.69 (7978.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:56.460396: step 131950, loss = 0.45 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:56.781808: step 131960, loss = 0.56 (7848.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:57.101132: step 131970, loss = 0.64 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:57.426240: step 131980, loss = 0.53 (7435.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:53:57.748624: step 131990, loss = 0.59 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:58.067612: step 132000, loss = 0.55 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:58.625048: step 132010, loss = 0.47 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:58.944273: step 132020, loss = 0.53 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:59.264163: step 132030, loss = 0.65 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:59.583442: step 132040, loss = 0.61 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:53:59.902703: step 132050, loss = 0.45 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:00.221157: step 132060, loss = 0.54 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:00.543246: step 132070, loss = 0.65 (7527.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:00.863690: step 132080, loss = 0.51 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:01.184109: step 132090, loss = 0.52 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:01.502305: step 132100, loss = 0.57 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:01.952466: step 132110, loss = 0.65 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:02.274211: step 132120, loss = 0.49 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:02.596627: step 132130, loss = 0.43 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:02.916686: step 132140, loss = 0.54 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:03.236785: step 132150, loss = 0.49 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:03.554143: step 132160, loss = 0.51 (7887.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:03.873310: step 132170, loss = 0.54 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:04.192974: step 132180, loss = 0.58 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:04.514261: step 132190, loss = 0.59 (7813.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:04.834275: step 132200, loss = 0.64 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:05.279670: step 132210, loss = 0.51 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:05.597600: step 132220, loss = 0.59 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:05.916131: step 132230, loss = 0.51 (8142.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:06.237392: step 132240, loss = 0.52 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:06.555951: step 132250, loss = 0.60 (8128.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:06.875855: step 132260, loss = 0.48 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:07.197931: step 132270, loss = 0.61 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:07.518450: step 132280, loss = 0.50 (7845.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:07.839885: step 132290, loss = 0.47 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:08.162030: step 132300, loss = 0.47 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:08.618341: step 132310, loss = 0.54 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:08.937981: step 132320, loss = 0.54 (7877.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:09.256898: step 132330, loss = 0.58 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:09.581372: step 132340, loss = 0.63 (7438.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:09.902144: step 132350, loss = 0.60 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:10.222273: step 132360, loss = 0.62 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:10.543984: step 132370, loss = 0.52 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:10.863724: step 132380, loss = 0.57 (8121.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:11.184113: step 132390, loss = 0.75 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:11.503231: step 132400, loss = 0.46 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:11.970205: step 132410, loss = 0.59 (7828.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:12.292089: step 132420, loss = 0.59 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:12.610578: step 132430, loss = 0.54 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:12.930057: step 132440, loss = 0.51 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:13.250338: step 132450, loss = 0.64 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:13.568881: step 132460, loss = 0.58 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:13.892868: step 132470, loss = 0.58 (7603.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:14.215027: step 132480, loss = 0.55 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:14.533150: step 132490, loss = 0.61 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:14.856698: step 132500, loss = 0.63 (7401.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:15.312933: step 132510, loss = 0.58 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:15.632098: step 132520, loss = 0.47 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:15.955730: step 132530, loss = 0.57 (7702.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:16.275514: step 132540, loss = 0.49 (8118.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:16.597681: step 132550, loss = 0.59 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:16.923087: step 132560, loss = 0.52 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:17.245863: step 132570, loss = 0.77 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:17.564196: step 132580, loss = 0.65 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:17.882777: step 132590, loss = 0.62 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:18.203007: step 132600, loss = 0.44 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:18.663241: step 132610, loss = 0.56 (7980.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:18.985430: step 132620, loss = 0.51 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:19.307912: step 132630, loss = 0.51 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:19.635802: step 132640, loss = 0.62 (7663.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:19.956585: step 132650, loss = 0.61 (7980.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:20.279664: step 132660, loss = 0.69 (7563.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:20.603288: step 132670, loss = 0.60 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:20.924819: step 132680, loss = 0.58 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:21.245817: step 132690, loss = 0.64 (7903.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:21.566867: step 132700, loss = 0.57 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:22.024045: step 132710, loss = 0.50 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:22.345760: step 132720, loss = 0.61 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:22.666170: step 132730, loss = 0.57 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:22.988649: step 132740, loss = 0.57 (7566.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:23.307525: step 132750, loss = 0.51 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:23.627415: step 132760, loss = 0.43 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:23.947751: step 132770, loss = 0.53 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:24.266915: step 132780, loss = 0.55 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:24.587000: step 132790, loss = 0.43 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:24.906606: step 132800, loss = 0.42 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:25.359933: step 132810, loss = 0.66 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:25.682326: step 132820, loss = 0.52 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:26.004067: step 132830, loss = 0.58 (7700.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:26.324335: step 132840, loss = 0.51 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:26.643288: step 132850, loss = 0.58 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:26.962420: step 132860, loss = 0.55 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:27.282871: step 132870, loss = 0.55 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:27.603530: step 132880, loss = 0.57 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:27.929583: step 132890, loss = 0.56 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:28.253762: step 132900, loss = 0.70 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:28.714286: step 132910, loss = 0.65 (7846.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:29.032780: step 132920, loss = 0.53 (8121.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:29.350447: step 132930, loss = 0.54 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:29.670825: step 132940, loss = 0.56 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:29.988784: step 132950, loss = 0.57 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:30.308052: step 132960, loss = 0.48 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:30.626416: step 132970, loss = 0.48 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:30.946824: step 132980, loss = 0.65 (7959.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:31.267385: step 132990, loss = 0.51 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:31.586471: step 133000, loss = 0.65 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:32.128999: step 133010, loss = 0.48 (7928.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:32.450219: step 133020, loss = 0.57 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:32.769147: step 133030, loss = 0.60 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:33.089438: step 133040, loss = 0.53 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:33.410305: step 133050, loss = 0.50 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:33.735388: step 133060, loss = 0.73 (7300.5 examples/sec; 0.018 sec/batch)
2017-09-16 16:54:34.056468: step 133070, loss = 0.45 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:34.377203: step 133080, loss = 0.52 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:34.698836: step 133090, loss = 0.56 (7846.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:35.020366: step 133100, loss = 0.55 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:35.479985: step 133110, loss = 0.55 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:35.801025: step 133120, loss = 0.48 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:36.122498: step 133130, loss = 0.44 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:36.444139: step 133140, loss = 0.58 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:36.763674: step 133150, loss = 0.46 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:37.083444: step 133160, loss = 0.55 (8030.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:37.402985: step 133170, loss = 0.54 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:37.724835: step 133180, loss = 0.61 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:38.044715: step 133190, loss = 0.58 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:38.365365: step 133200, loss = 0.43 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:38.828561: step 133210, loss = 0.79 (7716.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:39.148025: step 133220, loss = 0.49 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:39.468905: step 133230, loss = 0.61 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:39.790800: step 133240, loss = 0.50 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:40.111635: step 133250, loss = 0.55 (7892.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:40.431910: step 133260, loss = 0.45 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:40.752634: step 133270, loss = 0.60 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:41.070569: step 133280, loss = 0.54 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:41.390119: step 133290, loss = 0.55 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:41.710131: step 133300, loss = 0.58 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:42.167970: step 133310, loss = 0.55 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:42.489286: step 133320, loss = 0.61 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:42.811977: step 133330, loss = 0.49 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:43.134877: step 133340, loss = 0.66 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:43.453705: step 133350, loss = 0.47 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:43.773690: step 133360, loss = 0.52 (7897.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:44.095434: step 133370, loss = 0.65 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:44.415590: step 133380, loss = 0.56 (7797.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:44.737452: step 133390, loss = 0.56 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:45.058176: step 133400, loss = 0.60 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:45.513677: step 133410, loss = 0.61 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:45.832633: step 133420, loss = 0.53 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:46.153603: step 133430, loss = 0.52 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:46.473248: step 133440, loss = 0.58 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:46.792067: step 133450, loss = 0.63 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:47.113215: step 133460, loss = 0.63 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:47.433665: step 133470, loss = 0.51 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:47.751905: step 133480, loss = 0.57 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:48.071419: step 133490, loss = 0.52 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:48.392032: step 133500, loss = 0.54 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:48.861439: step 133510, loss = 0.48 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:49.179416: step 133520, loss = 0.58 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:49.498070: step 133530, loss = 0.53 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:49.817954: step 133540, loss = 0.54 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:50.136999: step 133550, loss = 0.57 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:50.456292: step 133560, loss = 0.63 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:50.774029: step 133570, loss = 0.55 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:51.092670: step 133580, loss = 0.54 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:51.413134: step 133590, loss = 0.55 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:51.732643: step 133600, loss = 0.50 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:52.200692: step 133610, loss = 0.49 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:52.520921: step 133620, loss = 0.58 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:52.840473: step 133630, loss = 0.59 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:53.160485: step 133640, loss = 0.45 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:53.480675: step 133650, loss = 0.59 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:53.802938: step 133660, loss = 0.55 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:54.124042: step 133670, loss = 0.51 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:54.444676: step 133680, loss = 0.64 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:54.765234: step 133690, loss = 0.59 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:55.086038: step 133700, loss = 0.63 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:55.539971: step 133710, loss = 0.68 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:55.860592: step 133720, loss = 0.48 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:56.182904: step 133730, loss = 0.65 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:56.504107: step 133740, loss = 0.58 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:56.828726: step 133750, loss = 0.63 (7420.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:57.152252: step 133760, loss = 0.61 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:57.473759: step 133770, loss = 0.55 (7728.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:54:57.793364: step 133780, loss = 0.58 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:58.113410: step 133790, loss = 0.52 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:58.432083: step 133800, loss = 0.59 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:58.894379: step 133810, loss = 0.50 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:59.215281: step 133820, loss = 0.55 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:59.534820: step 133830, loss = 0.53 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:54:59.855970: step 133840, loss = 0.43 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:00.175094: step 133850, loss = 0.52 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:00.497848: step 133860, loss = 0.51 (7769.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:00.818028: step 133870, loss = 0.62 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:01.138911: step 133880, loss = 0.59 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:01.460401: step 133890, loss = 0.60 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:01.779503: step 133900, loss = 0.56 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:02.232666: step 133910, loss = 0.56 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:02.550815: step 133920, loss = 0.62 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:02.872911: step 133930, loss = 0.50 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:03.193357: step 133940, loss = 0.50 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:03.513416: step 133950, loss = 0.47 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:03.832275: step 133960, loss = 0.46 (8018.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:04.152202: step 133970, loss = 0.52 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:04.471673: step 133980, loss = 0.61 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:04.790360: step 133990, loss = 0.45 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:05.107985: step 134000, loss = 0.61 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:05.699366: step 134010, loss = 0.44 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:06.020792: step 134020, loss = 0.55 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:06.344027: step 134030, loss = 0.62 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:06.663421: step 134040, loss = 0.50 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:06.981964: step 134050, loss = 0.52 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:07.301847: step 134060, loss = 0.49 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:07.621920: step 134070, loss = 0.61 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:07.942953: step 134080, loss = 0.61 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:08.263512: step 134090, loss = 0.65 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:08.583942: step 134100, loss = 0.49 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:09.044214: step 134110, loss = 0.52 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:09.365074: step 134120, loss = 0.62 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:09.684008: step 134130, loss = 0.57 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:10.002860: step 134140, loss = 0.51 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:10.321877: step 134150, loss = 0.46 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:10.642657: step 134160, loss = 0.57 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:10.963575: step 134170, loss = 0.70 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:11.282146: step 134180, loss = 0.63 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:11.601426: step 134190, loss = 0.58 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:11.920487: step 134200, loss = 0.51 (8078.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:12.372877: step 134210, loss = 0.58 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:12.691736: step 134220, loss = 0.62 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:13.012035: step 134230, loss = 0.46 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:13.333605: step 134240, loss = 0.48 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:13.653693: step 134250, loss = 0.67 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:13.973670: step 134260, loss = 0.65 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:14.292763: step 134270, loss = 0.54 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:14.612128: step 134280, loss = 0.61 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:14.936148: step 134290, loss = 0.47 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:15.257805: step 134300, loss = 0.49 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:15.711908: step 134310, loss = 0.64 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:16.030886: step 134320, loss = 0.51 (7941.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:16.353627: step 134330, loss = 0.48 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:16.672950: step 134340, loss = 0.45 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:16.992502: step 134350, loss = 0.63 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:17.310336: step 134360, loss = 0.48 (8115.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:17.630665: step 134370, loss = 0.53 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:17.949632: step 134380, loss = 0.52 (8130.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:18.269094: step 134390, loss = 0.46 (8147.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:18.588129: step 134400, loss = 0.58 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:19.050371: step 134410, loss = 0.62 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:19.370827: step 134420, loss = 0.55 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:19.692272: step 134430, loss = 0.45 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:20.014541: step 134440, loss = 0.62 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:20.338805: step 134450, loss = 0.50 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:20.659497: step 134460, loss = 0.63 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:20.978709: step 134470, loss = 0.59 (8166.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:21.299082: step 134480, loss = 0.61 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:21.619013: step 134490, loss = 0.50 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:21.936844: step 134500, loss = 0.51 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:22.389517: step 134510, loss = 0.55 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:22.711512: step 134520, loss = 0.57 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:23.030699: step 134530, loss = 0.39 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:23.351570: step 134540, loss = 0.50 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:23.671107: step 134550, loss = 0.59 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:23.991334: step 134560, loss = 0.59 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:24.310980: step 134570, loss = 0.52 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:24.633729: step 134580, loss = 0.47 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:24.953402: step 134590, loss = 0.49 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:25.277070: step 134600, loss = 0.47 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:25.733046: step 134610, loss = 0.50 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:26.053852: step 134620, loss = 0.44 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:26.374299: step 134630, loss = 0.52 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:26.695307: step 134640, loss = 0.56 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:27.014842: step 134650, loss = 0.69 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:27.335613: step 134660, loss = 0.56 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:27.656398: step 134670, loss = 0.47 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:27.980760: step 134680, loss = 0.56 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:28.299937: step 134690, loss = 0.47 (8128.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:28.620153: step 134700, loss = 0.61 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:29.065777: step 134710, loss = 0.51 (8176.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:29.386024: step 134720, loss = 0.50 (7962.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:29.706582: step 134730, loss = 0.51 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:30.028665: step 134740, loss = 0.64 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:30.355820: step 134750, loss = 0.46 (7324.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:55:30.676745: step 134760, loss = 0.54 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:30.996916: step 134770, loss = 0.58 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:31.316766: step 134780, loss = 0.54 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:31.637630: step 134790, loss = 0.60 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:31.960809: step 134800, loss = 0.51 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:32.419827: step 134810, loss = 0.57 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:32.740050: step 134820, loss = 0.59 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:33.060725: step 134830, loss = 0.64 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:33.380805: step 134840, loss = 0.51 (8149.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:33.701841: step 134850, loss = 0.47 (7828.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:34.023601: step 134860, loss = 0.60 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:34.347441: step 134870, loss = 0.61 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:34.667448: step 134880, loss = 0.49 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:34.989253: step 134890, loss = 0.42 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:35.310385: step 134900, loss = 0.61 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:35.755881: step 134910, loss = 0.53 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:36.076930: step 134920, loss = 0.49 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:36.398595: step 134930, loss = 0.65 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:36.717761: step 134940, loss = 0.81 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:37.040807: step 134950, loss = 0.46 (7964.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:37.359489: step 134960, loss = 0.59 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:37.678379: step 134970, loss = 0.60 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:37.999286: step 134980, loss = 0.45 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:38.318777: step 134990, loss = 0.53 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:38.638113: step 135000, loss = 0.60 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:39.183519: step 135010, loss = 0.49 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:39.505764: step 135020, loss = 0.60 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:39.824318: step 135030, loss = 0.64 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:40.143164: step 135040, loss = 0.53 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:40.463987: step 135050, loss = 0.57 (7656.6 examples/sec; 0.017 sec/batch)
2017-09-16 16:55:40.789052: step 135060, loss = 0.48 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:41.107488: step 135070, loss = 0.66 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:41.427390: step 135080, loss = 0.62 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:41.745745: step 135090, loss = 0.44 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:42.065376: step 135100, loss = 0.50 (8142.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:42.523720: step 135110, loss = 0.59 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:42.844851: step 135120, loss = 0.53 (8027.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:43.165830: step 135130, loss = 0.65 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:43.490717: step 135140, loss = 0.49 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:43.811575: step 135150, loss = 0.57 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:44.131720: step 135160, loss = 0.64 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:44.452614: step 135170, loss = 0.54 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:44.772814: step 135180, loss = 0.50 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:45.092049: step 135190, loss = 0.54 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:45.411239: step 135200, loss = 0.56 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:45.866404: step 135210, loss = 0.50 (7822.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:46.185621: step 135220, loss = 0.56 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:46.506057: step 135230, loss = 0.44 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:46.826892: step 135240, loss = 0.57 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:47.146644: step 135250, loss = 0.48 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:47.469022: step 135260, loss = 0.58 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:47.787803: step 135270, loss = 0.56 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:48.109912: step 135280, loss = 0.57 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:48.431343: step 135290, loss = 0.68 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:48.754322: step 135300, loss = 0.43 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:49.210713: step 135310, loss = 0.55 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:49.529749: step 135320, loss = 0.59 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:49.849837: step 135330, loss = 0.56 (7880.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:50.171530: step 135340, loss = 0.54 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:50.494172: step 135350, loss = 0.62 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:50.815926: step 135360, loss = 0.59 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:51.137657: step 135370, loss = 0.48 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:51.456897: step 135380, loss = 0.44 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:51.775761: step 135390, loss = 0.52 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:52.097192: step 135400, loss = 0.57 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:52.557649: step 135410, loss = 0.57 (7655.5 examples/sec; 0.017 sec/batch)
2017-09-16 16:55:52.877619: step 135420, loss = 0.61 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:53.196064: step 135430, loss = 0.58 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:53.515314: step 135440, loss = 0.57 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:53.835701: step 135450, loss = 0.45 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:54.155674: step 135460, loss = 0.65 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:54.477307: step 135470, loss = 0.61 (8127.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:54.798568: step 135480, loss = 0.46 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:55.120345: step 135490, loss = 0.56 (8077.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:55.439886: step 135500, loss = 0.62 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:55.897562: step 135510, loss = 0.63 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:56.217338: step 135520, loss = 0.69 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:56.539642: step 135530, loss = 0.61 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:56.860127: step 135540, loss = 0.64 (7767.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:57.180364: step 135550, loss = 0.65 (8040.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:57.501438: step 135560, loss = 0.64 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:57.823833: step 135570, loss = 0.48 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:58.143799: step 135580, loss = 0.54 (7806.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:58.462641: step 135590, loss = 0.55 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:58.781651: step 135600, loss = 0.51 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:59.244551: step 135610, loss = 0.53 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:59.563094: step 135620, loss = 0.57 (7885.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:55:59.882746: step 135630, loss = 0.65 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:00.201820: step 135640, loss = 0.52 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:00.524424: step 135650, loss = 0.56 (7828.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:00.846564: step 135660, loss = 0.41 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:01.166601: step 135670, loss = 0.43 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:01.485968: step 135680, loss = 0.66 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:01.807405: step 135690, loss = 0.46 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:02.127140: step 135700, loss = 0.50 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:02.593853: step 135710, loss = 0.51 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:02.912163: step 135720, loss = 0.50 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:03.231799: step 135730, loss = 0.50 (7881.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:03.553484: step 135740, loss = 0.58 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:03.874711: step 135750, loss = 0.52 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:04.193191: step 135760, loss = 0.58 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:04.518541: step 135770, loss = 0.61 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:04.840118: step 135780, loss = 0.65 (7939.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:05.158469: step 135790, loss = 0.65 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:05.479307: step 135800, loss = 0.74 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:05.933042: step 135810, loss = 0.55 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:06.251822: step 135820, loss = 0.54 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:06.571319: step 135830, loss = 0.55 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:06.891780: step 135840, loss = 0.52 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:07.211290: step 135850, loss = 0.61 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:07.533027: step 135860, loss = 0.41 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:07.852526: step 135870, loss = 0.60 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:08.173169: step 135880, loss = 0.63 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:08.491077: step 135890, loss = 0.52 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:08.813339: step 135900, loss = 0.63 (7524.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:56:09.280482: step 135910, loss = 0.60 (7768.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:09.600148: step 135920, loss = 0.65 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:09.920177: step 135930, loss = 0.55 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:10.241335: step 135940, loss = 0.66 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:10.562229: step 135950, loss = 0.66 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:10.884156: step 135960, loss = 0.60 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:11.204252: step 135970, loss = 0.64 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:11.525123: step 135980, loss = 0.68 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:11.844715: step 135990, loss = 0.61 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:12.165133: step 136000, loss = 0.52 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:12.723898: step 136010, loss = 0.59 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:13.042730: step 136020, loss = 0.53 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:13.362898: step 136030, loss = 0.67 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:13.681741: step 136040, loss = 0.74 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:14.000643: step 136050, loss = 0.59 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:14.319143: step 136060, loss = 0.62 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:14.641083: step 136070, loss = 0.53 (7879.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:14.960261: step 136080, loss = 0.54 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:15.282228: step 136090, loss = 0.44 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:15.601117: step 136100, loss = 0.56 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:16.062584: step 136110, loss = 0.60 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:16.382154: step 136120, loss = 0.54 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:16.702205: step 136130, loss = 0.54 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:17.021391: step 136140, loss = 0.54 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:17.340913: step 136150, loss = 0.48 (7920.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:17.661042: step 136160, loss = 0.42 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:17.983095: step 136170, loss = 0.57 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:18.304751: step 136180, loss = 0.74 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:18.627507: step 136190, loss = 0.52 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:18.947322: step 136200, loss = 0.48 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:19.404889: step 136210, loss = 0.56 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:19.724121: step 136220, loss = 0.52 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:20.044919: step 136230, loss = 0.50 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:20.367490: step 136240, loss = 0.53 (7815.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:20.687531: step 136250, loss = 0.59 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:21.010211: step 136260, loss = 0.40 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:21.329945: step 136270, loss = 0.48 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:21.649120: step 136280, loss = 0.56 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:21.971028: step 136290, loss = 0.49 (7848.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:22.291197: step 136300, loss = 0.46 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:22.752553: step 136310, loss = 0.43 (8119.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:23.072959: step 136320, loss = 0.52 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:23.392249: step 136330, loss = 0.49 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:23.712036: step 136340, loss = 0.47 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:24.029930: step 136350, loss = 0.55 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:24.350996: step 136360, loss = 0.59 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:24.671987: step 136370, loss = 0.58 (7811.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:24.991642: step 136380, loss = 0.58 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:25.310098: step 136390, loss = 0.60 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:25.629995: step 136400, loss = 0.50 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:26.091614: step 136410, loss = 0.64 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:26.412884: step 136420, loss = 0.57 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:26.732546: step 136430, loss = 0.58 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:27.053053: step 136440, loss = 0.42 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:27.374062: step 136450, loss = 0.44 (7935.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:27.693146: step 136460, loss = 0.52 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:28.013908: step 136470, loss = 0.57 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:28.335663: step 136480, loss = 0.55 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:28.657659: step 136490, loss = 0.54 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:28.977008: step 136500, loss = 0.55 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:29.423337: step 136510, loss = 0.65 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:29.743991: step 136520, loss = 0.50 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:30.064054: step 136530, loss = 0.49 (7988.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:30.385453: step 136540, loss = 0.48 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:30.707167: step 136550, loss = 0.63 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:31.028080: step 136560, loss = 0.66 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:31.346228: step 136570, loss = 0.58 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:31.665302: step 136580, loss = 0.65 (8121.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:31.983654: step 136590, loss = 0.55 (8174.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:32.304707: step 136600, loss = 0.51 (7811.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:32.770198: step 136610, loss = 0.56 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:33.088841: step 136620, loss = 0.51 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:33.406103: step 136630, loss = 0.56 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:33.724713: step 136640, loss = 0.67 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:34.044250: step 136650, loss = 0.57 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:34.363176: step 136660, loss = 0.58 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:34.683554: step 136670, loss = 0.59 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:35.011833: step 136680, loss = 0.59 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:35.332752: step 136690, loss = 0.58 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:35.651932: step 136700, loss = 0.65 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:36.109792: step 136710, loss = 0.51 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:36.431828: step 136720, loss = 0.45 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:36.751341: step 136730, loss = 0.40 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:37.071255: step 136740, loss = 0.48 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:37.390440: step 136750, loss = 0.53 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:37.714119: step 136760, loss = 0.50 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:38.033943: step 136770, loss = 0.54 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:38.357694: step 136780, loss = 0.46 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:38.681140: step 136790, loss = 0.46 (7789.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:39.007232: step 136800, loss = 0.40 (7685.8 examples/sec; 0.017 sec/batch)
2017-09-16 16:56:39.463609: step 136810, loss = 0.45 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:39.782952: step 136820, loss = 0.55 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:40.102762: step 136830, loss = 0.49 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:40.421745: step 136840, loss = 0.40 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:40.742710: step 136850, loss = 0.39 (8164.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:41.063080: step 136860, loss = 0.44 (7920.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:41.382732: step 136870, loss = 0.56 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:41.702794: step 136880, loss = 0.41 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:42.021328: step 136890, loss = 0.43 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:42.340541: step 136900, loss = 0.46 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:42.816603: step 136910, loss = 0.54 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:43.137180: step 136920, loss = 0.43 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:43.458520: step 136930, loss = 0.52 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:43.778566: step 136940, loss = 0.52 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:44.097753: step 136950, loss = 0.62 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:44.417322: step 136960, loss = 0.55 (7903.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:44.736502: step 136970, loss = 0.44 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:45.055923: step 136980, loss = 0.41 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:45.374447: step 136990, loss = 0.50 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:45.694528: step 137000, loss = 0.46 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:46.253238: step 137010, loss = 0.35 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:46.571803: step 137020, loss = 0.39 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:46.888387: step 137030, loss = 0.48 (8177.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:47.207002: step 137040, loss = 0.41 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:47.527904: step 137050, loss = 0.58 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:47.847264: step 137060, loss = 0.49 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:48.166572: step 137070, loss = 0.45 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:48.486340: step 137080, loss = 0.37 (7925.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:48.808901: step 137090, loss = 0.46 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:49.129699: step 137100, loss = 0.48 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:49.575552: step 137110, loss = 0.51 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:49.893932: step 137120, loss = 0.46 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:50.212332: step 137130, loss = 0.42 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:50.533003: step 137140, loss = 0.49 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:50.853449: step 137150, loss = 0.54 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:51.175667: step 137160, loss = 0.45 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:51.496065: step 137170, loss = 0.43 (7772.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:51.817939: step 137180, loss = 0.45 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:52.139161: step 137190, loss = 0.42 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:52.458695: step 137200, loss = 0.36 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:52.916630: step 137210, loss = 0.51 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:53.237240: step 137220, loss = 0.39 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:53.557855: step 137230, loss = 0.36 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:53.878401: step 137240, loss = 0.48 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:54.197995: step 137250, loss = 0.42 (7800.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:54.518857: step 137260, loss = 0.48 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:54.840873: step 137270, loss = 0.40 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:55.162477: step 137280, loss = 0.54 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:55.484678: step 137290, loss = 0.40 (7823.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:55.807821: step 137300, loss = 0.44 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:56.269640: step 137310, loss = 0.43 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:56.589129: step 137320, loss = 0.37 (7880.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:56.912874: step 137330, loss = 0.42 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:57.233049: step 137340, loss = 0.49 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:57.553415: step 137350, loss = 0.41 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:57.873968: step 137360, loss = 0.43 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:58.194382: step 137370, loss = 0.41 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:58.512926: step 137380, loss = 0.44 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:58.832959: step 137390, loss = 0.42 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:59.152146: step 137400, loss = 0.41 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:59.611263: step 137410, loss = 0.36 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:56:59.931888: step 137420, loss = 0.38 (7954.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:00.251424: step 137430, loss = 0.41 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:00.571195: step 137440, loss = 0.39 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:00.891068: step 137450, loss = 0.46 (8155.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:01.211550: step 137460, loss = 0.58 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:01.531147: step 137470, loss = 0.33 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:01.850337: step 137480, loss = 0.45 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:02.170349: step 137490, loss = 0.41 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:02.490611: step 137500, loss = 0.37 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:02.946641: step 137510, loss = 0.43 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:03.266107: step 137520, loss = 0.30 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:03.585004: step 137530, loss = 0.36 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:03.906751: step 137540, loss = 0.40 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:04.228046: step 137550, loss = 0.51 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:04.551389: step 137560, loss = 0.54 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:04.874711: step 137570, loss = 0.46 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:05.194249: step 137580, loss = 0.46 (7863.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:05.516457: step 137590, loss = 0.44 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:05.836071: step 137600, loss = 0.35 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:06.288858: step 137610, loss = 0.39 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:06.610722: step 137620, loss = 0.40 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:06.928459: step 137630, loss = 0.47 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:07.246357: step 137640, loss = 0.48 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:07.565764: step 137650, loss = 0.43 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:07.886591: step 137660, loss = 0.39 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:08.206999: step 137670, loss = 0.38 (8132.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:08.525838: step 137680, loss = 0.44 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:08.844644: step 137690, loss = 0.47 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:09.165875: step 137700, loss = 0.42 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:09.627460: step 137710, loss = 0.50 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:09.948479: step 137720, loss = 0.47 (7939.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:10.267154: step 137730, loss = 0.41 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:10.585736: step 137740, loss = 0.44 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:10.907441: step 137750, loss = 0.35 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:11.227979: step 137760, loss = 0.34 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:11.547066: step 137770, loss = 0.39 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:11.866888: step 137780, loss = 0.44 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:12.186396: step 137790, loss = 0.36 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:12.505426: step 137800, loss = 0.46 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:12.970900: step 137810, loss = 0.37 (7766.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:13.291661: step 137820, loss = 0.35 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:13.611870: step 137830, loss = 0.36 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:13.932149: step 137840, loss = 0.43 (8127.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:14.252599: step 137850, loss = 0.41 (7888.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:14.573071: step 137860, loss = 0.42 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:14.893809: step 137870, loss = 0.48 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:15.212988: step 137880, loss = 0.50 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:15.531941: step 137890, loss = 0.54 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:15.850814: step 137900, loss = 0.46 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:16.304117: step 137910, loss = 0.43 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:16.624164: step 137920, loss = 0.46 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:16.944509: step 137930, loss = 0.39 (8143.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:17.265530: step 137940, loss = 0.42 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:17.583574: step 137950, loss = 0.41 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:17.904307: step 137960, loss = 0.42 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:18.224465: step 137970, loss = 0.48 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:18.542520: step 137980, loss = 0.44 (8077.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:18.863741: step 137990, loss = 0.44 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:19.181666: step 138000, loss = 0.40 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:19.733336: step 138010, loss = 0.30 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:20.059729: step 138020, loss = 0.44 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:20.378444: step 138030, loss = 0.37 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:20.698575: step 138040, loss = 0.42 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:21.016368: step 138050, loss = 0.31 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:21.334693: step 138060, loss = 0.40 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:21.655222: step 138070, loss = 0.38 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:21.974730: step 138080, loss = 0.34 (7943.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:22.293596: step 138090, loss = 0.42 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:22.616552: step 138100, loss = 0.40 (7496.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:57:23.062245: step 138110, loss = 0.38 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:23.385302: step 138120, loss = 0.35 (7936.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:23.706216: step 138130, loss = 0.32 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:24.025909: step 138140, loss = 0.34 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:24.345998: step 138150, loss = 0.47 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:24.665658: step 138160, loss = 0.36 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:24.986821: step 138170, loss = 0.37 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:25.308976: step 138180, loss = 0.36 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:25.628148: step 138190, loss = 0.45 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:25.947513: step 138200, loss = 0.37 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:26.405046: step 138210, loss = 0.34 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:26.723868: step 138220, loss = 0.37 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:27.043966: step 138230, loss = 0.38 (7842.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:27.364048: step 138240, loss = 0.43 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:27.683408: step 138250, loss = 0.48 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:28.003476: step 138260, loss = 0.40 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:28.322934: step 138270, loss = 0.31 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:28.644067: step 138280, loss = 0.43 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:28.962347: step 138290, loss = 0.42 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:29.280910: step 138300, loss = 0.39 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:29.741186: step 138310, loss = 0.41 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:30.059967: step 138320, loss = 0.33 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:30.379702: step 138330, loss = 0.36 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:30.699138: step 138340, loss = 0.39 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:31.021361: step 138350, loss = 0.40 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:31.342367: step 138360, loss = 0.36 (7809.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:31.660834: step 138370, loss = 0.42 (8132.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:31.979901: step 138380, loss = 0.35 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:32.299222: step 138390, loss = 0.38 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:32.619231: step 138400, loss = 0.36 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:33.076962: step 138410, loss = 0.34 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:33.400159: step 138420, loss = 0.48 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:33.720370: step 138430, loss = 0.40 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:34.041666: step 138440, loss = 0.38 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:34.361775: step 138450, loss = 0.48 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:34.681943: step 138460, loss = 0.48 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:35.001019: step 138470, loss = 0.31 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:35.322372: step 138480, loss = 0.41 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:35.644230: step 138490, loss = 0.51 (7414.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:57:35.964479: step 138500, loss = 0.47 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:36.423949: step 138510, loss = 0.34 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:36.743727: step 138520, loss = 0.39 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:37.063240: step 138530, loss = 0.41 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:37.382237: step 138540, loss = 0.62 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:37.705221: step 138550, loss = 0.34 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:38.025393: step 138560, loss = 0.38 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:38.352036: step 138570, loss = 0.47 (7526.1 examples/sec; 0.017 sec/batch)
2017-09-16 16:57:38.672203: step 138580, loss = 0.37 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:38.990406: step 138590, loss = 0.39 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:39.310077: step 138600, loss = 0.39 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:39.768072: step 138610, loss = 0.46 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:40.088264: step 138620, loss = 0.39 (8138.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:40.406720: step 138630, loss = 0.40 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:40.725879: step 138640, loss = 0.40 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:41.047746: step 138650, loss = 0.39 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:41.368442: step 138660, loss = 0.46 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:41.687623: step 138670, loss = 0.39 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:42.008013: step 138680, loss = 0.44 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:42.327666: step 138690, loss = 0.48 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:42.647996: step 138700, loss = 0.34 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:43.108265: step 138710, loss = 0.43 (8103.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:43.427584: step 138720, loss = 0.40 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:43.748664: step 138730, loss = 0.33 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:44.071806: step 138740, loss = 0.38 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:44.393285: step 138750, loss = 0.39 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:44.712970: step 138760, loss = 0.41 (7955.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:45.034181: step 138770, loss = 0.32 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:45.353871: step 138780, loss = 0.39 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:45.675221: step 138790, loss = 0.40 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:45.997085: step 138800, loss = 0.32 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:46.458541: step 138810, loss = 0.45 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:46.779777: step 138820, loss = 0.38 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:47.099611: step 138830, loss = 0.44 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:47.418397: step 138840, loss = 0.43 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:47.738711: step 138850, loss = 0.44 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:48.059949: step 138860, loss = 0.37 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:48.379899: step 138870, loss = 0.48 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:48.698223: step 138880, loss = 0.49 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:49.017326: step 138890, loss = 0.38 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:49.339674: step 138900, loss = 0.43 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:49.798797: step 138910, loss = 0.40 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:50.120544: step 138920, loss = 0.40 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:50.441058: step 138930, loss = 0.33 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:50.764698: step 138940, loss = 0.36 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:51.085655: step 138950, loss = 0.41 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:51.403974: step 138960, loss = 0.35 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:51.725729: step 138970, loss = 0.39 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:52.043905: step 138980, loss = 0.41 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:52.363259: step 138990, loss = 0.39 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:52.681457: step 139000, loss = 0.33 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:53.235648: step 139010, loss = 0.39 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:53.558305: step 139020, loss = 0.44 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:53.880253: step 139030, loss = 0.44 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:54.201666: step 139040, loss = 0.32 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:54.522679: step 139050, loss = 0.37 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:54.846531: step 139060, loss = 0.45 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:55.166466: step 139070, loss = 0.46 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:55.486888: step 139080, loss = 0.49 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:55.810008: step 139090, loss = 0.33 (7861.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:56.129795: step 139100, loss = 0.45 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:56.594307: step 139110, loss = 0.34 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:56.916018: step 139120, loss = 0.33 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:57.237293: step 139130, loss = 0.34 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:57.556239: step 139140, loss = 0.35 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:57.877736: step 139150, loss = 0.39 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:58.197266: step 139160, loss = 0.37 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:58.521864: step 139170, loss = 0.47 (7839.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:58.841285: step 139180, loss = 0.39 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:59.161655: step 139190, loss = 0.35 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:59.481125: step 139200, loss = 0.34 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:57:59.939087: step 139210, loss = 0.39 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:00.259343: step 139220, loss = 0.47 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:00.578002: step 139230, loss = 0.31 (8120.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:00.898438: step 139240, loss = 0.33 (8135.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:01.216594: step 139250, loss = 0.32 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:01.537805: step 139260, loss = 0.38 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:01.860434: step 139270, loss = 0.35 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:02.182158: step 139280, loss = 0.36 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:02.504017: step 139290, loss = 0.37 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:02.824188: step 139300, loss = 0.32 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:03.280568: step 139310, loss = 0.30 (8012.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:03.602680: step 139320, loss = 0.46 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:03.929747: step 139330, loss = 0.48 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:04.253342: step 139340, loss = 0.43 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:04.573489: step 139350, loss = 0.30 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:04.893329: step 139360, loss = 0.35 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:05.212774: step 139370, loss = 0.34 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:05.533142: step 139380, loss = 0.35 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:05.854389: step 139390, loss = 0.36 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:06.173821: step 139400, loss = 0.41 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:06.625088: step 139410, loss = 0.31 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:06.944283: step 139420, loss = 0.37 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:07.265321: step 139430, loss = 0.34 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:07.586882: step 139440, loss = 0.38 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:07.907529: step 139450, loss = 0.29 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:08.228684: step 139460, loss = 0.38 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:08.548236: step 139470, loss = 0.38 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:08.867923: step 139480, loss = 0.36 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:09.190725: step 139490, loss = 0.32 (7469.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:09.516145: step 139500, loss = 0.37 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:09.974899: step 139510, loss = 0.39 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:10.296533: step 139520, loss = 0.37 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:10.617104: step 139530, loss = 0.31 (8059.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:10.937652: step 139540, loss = 0.38 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:11.256843: step 139550, loss = 0.46 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:11.576961: step 139560, loss = 0.42 (7971.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:11.895687: step 139570, loss = 0.43 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:12.215827: step 139580, loss = 0.36 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:12.534113: step 139590, loss = 0.38 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:12.855555: step 139600, loss = 0.35 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:13.316206: step 139610, loss = 0.33 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:13.635387: step 139620, loss = 0.41 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:13.954253: step 139630, loss = 0.40 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:14.275748: step 139640, loss = 0.30 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:14.594805: step 139650, loss = 0.27 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:14.913614: step 139660, loss = 0.36 (8177.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:15.232883: step 139670, loss = 0.40 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:15.551596: step 139680, loss = 0.39 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:15.871001: step 139690, loss = 0.44 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:16.191571: step 139700, loss = 0.53 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:16.643441: step 139710, loss = 0.33 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:16.962360: step 139720, loss = 0.31 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:17.283820: step 139730, loss = 0.33 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:17.603708: step 139740, loss = 0.36 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:17.923537: step 139750, loss = 0.42 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:18.246211: step 139760, loss = 0.33 (7422.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:18.566269: step 139770, loss = 0.40 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:18.885793: step 139780, loss = 0.31 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:19.207232: step 139790, loss = 0.34 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:19.526379: step 139800, loss = 0.31 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:19.983708: step 139810, loss = 0.39 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:20.302695: step 139820, loss = 0.31 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:20.621293: step 139830, loss = 0.36 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:20.938454: step 139840, loss = 0.31 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:21.262499: step 139850, loss = 0.35 (7933.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:21.586921: step 139860, loss = 0.34 (7589.7 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:21.904855: step 139870, loss = 0.39 (8140.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:22.224030: step 139880, loss = 0.31 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:22.543894: step 139890, loss = 0.31 (8152.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:22.861912: step 139900, loss = 0.31 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:23.317020: step 139910, loss = 0.36 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:23.638648: step 139920, loss = 0.35 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:23.961202: step 139930, loss = 0.34 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:24.280073: step 139940, loss = 0.42 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:24.601352: step 139950, loss = 0.43 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:24.922302: step 139960, loss = 0.35 (8128.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:25.241749: step 139970, loss = 0.41 (8146.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:25.562050: step 139980, loss = 0.39 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:25.883418: step 139990, loss = 0.32 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:26.203148: step 140000, loss = 0.39 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:26.752169: step 140010, loss = 0.33 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:27.075015: step 140020, loss = 0.37 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:27.392835: step 140030, loss = 0.32 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:27.712739: step 140040, loss = 0.34 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:28.034263: step 140050, loss = 0.46 (7531.2 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:28.351801: step 140060, loss = 0.31 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:28.672382: step 140070, loss = 0.31 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:28.991978: step 140080, loss = 0.47 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:29.311393: step 140090, loss = 0.40 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:29.630897: step 140100, loss = 0.30 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:30.075541: step 140110, loss = 0.31 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:30.395223: step 140120, loss = 0.43 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:30.714527: step 140130, loss = 0.34 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:31.033924: step 140140, loss = 0.32 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:31.354541: step 140150, loss = 0.28 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:31.672149: step 140160, loss = 0.36 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:31.993346: step 140170, loss = 0.29 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:32.314641: step 140180, loss = 0.36 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:32.634551: step 140190, loss = 0.36 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:32.954723: step 140200, loss = 0.35 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:33.413237: step 140210, loss = 0.37 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:33.734596: step 140220, loss = 0.32 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:34.055559: step 140230, loss = 0.38 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:34.376967: step 140240, loss = 0.40 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:34.695893: step 140250, loss = 0.34 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:35.014151: step 140260, loss = 0.35 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:35.334175: step 140270, loss = 0.39 (7910.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:35.655449: step 140280, loss = 0.37 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:35.975965: step 140290, loss = 0.39 (7859.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:36.296917: step 140300, loss = 0.39 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:36.751045: step 140310, loss = 0.31 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:37.073317: step 140320, loss = 0.36 (7855.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:37.395302: step 140330, loss = 0.39 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:37.716826: step 140340, loss = 0.38 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:38.040395: step 140350, loss = 0.39 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:38.362957: step 140360, loss = 0.29 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:38.683639: step 140370, loss = 0.30 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:39.001336: step 140380, loss = 0.29 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:39.322787: step 140390, loss = 0.32 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:39.644295: step 140400, loss = 0.36 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:40.099682: step 140410, loss = 0.33 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:40.419637: step 140420, loss = 0.36 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:40.739178: step 140430, loss = 0.39 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:41.060493: step 140440, loss = 0.47 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:41.385970: step 140450, loss = 0.25 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:41.707868: step 140460, loss = 0.32 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:42.030675: step 140470, loss = 0.37 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:42.348949: step 140480, loss = 0.32 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:42.668065: step 140490, loss = 0.32 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:42.986057: step 140500, loss = 0.32 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:43.444657: step 140510, loss = 0.34 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:43.767086: step 140520, loss = 0.33 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:44.090805: step 140530, loss = 0.32 (7942.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:44.413787: step 140540, loss = 0.27 (7549.9 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:44.734545: step 140550, loss = 0.36 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:45.056765: step 140560, loss = 0.37 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:45.378993: step 140570, loss = 0.42 (7898.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:45.698912: step 140580, loss = 0.37 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:46.020608: step 140590, loss = 0.37 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:46.343518: step 140600, loss = 0.37 (7968.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:46.792043: step 140610, loss = 0.35 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:47.113941: step 140620, loss = 0.51 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:47.433268: step 140630, loss = 0.31 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:47.752997: step 140640, loss = 0.33 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:48.071361: step 140650, loss = 0.28 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:48.390789: step 140660, loss = 0.34 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:48.710716: step 140670, loss = 0.37 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:49.029877: step 140680, loss = 0.33 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:49.348607: step 140690, loss = 0.29 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:49.668904: step 140700, loss = 0.28 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:50.134490: step 140710, loss = 0.29 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:50.454775: step 140720, loss = 0.36 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:50.774869: step 140730, loss = 0.40 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:51.093339: step 140740, loss = 0.45 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:51.411572: step 140750, loss = 0.36 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:51.730674: step 140760, loss = 0.36 (8140.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:52.049222: step 140770, loss = 0.33 (8149.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:52.370037: step 140780, loss = 0.25 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:52.689111: step 140790, loss = 0.36 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:53.008981: step 140800, loss = 0.32 (8152.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:53.467320: step 140810, loss = 0.40 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:53.786187: step 140820, loss = 0.32 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:54.107711: step 140830, loss = 0.38 (8067.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:54.429285: step 140840, loss = 0.33 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:54.746994: step 140850, loss = 0.38 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:55.064966: step 140860, loss = 0.26 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:55.387900: step 140870, loss = 0.36 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:55.709797: step 140880, loss = 0.38 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:56.029519: step 140890, loss = 0.51 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:56.349472: step 140900, loss = 0.36 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:56.806079: step 140910, loss = 0.39 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:57.124971: step 140920, loss = 0.34 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:57.443911: step 140930, loss = 0.27 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:57.764621: step 140940, loss = 0.30 (7636.3 examples/sec; 0.017 sec/batch)
2017-09-16 16:58:58.086748: step 140950, loss = 0.33 (7773.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:58.413666: step 140960, loss = 0.42 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:58.734240: step 140970, loss = 0.43 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:59.057256: step 140980, loss = 0.38 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:59.376642: step 140990, loss = 0.43 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:58:59.697354: step 141000, loss = 0.28 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:00.312526: step 141010, loss = 0.34 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:00.634272: step 141020, loss = 0.34 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:00.952451: step 141030, loss = 0.41 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:01.274942: step 141040, loss = 0.31 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:01.596067: step 141050, loss = 0.40 (8112.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:01.915706: step 141060, loss = 0.31 (7955.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:02.236605: step 141070, loss = 0.29 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:02.555855: step 141080, loss = 0.29 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:02.875938: step 141090, loss = 0.34 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:03.198830: step 141100, loss = 0.35 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:03.658073: step 141110, loss = 0.34 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:03.981662: step 141120, loss = 0.24 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:04.300990: step 141130, loss = 0.39 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:04.619418: step 141140, loss = 0.30 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:04.940435: step 141150, loss = 0.28 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:05.261155: step 141160, loss = 0.33 (7861.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:05.581488: step 141170, loss = 0.33 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:05.901338: step 141180, loss = 0.37 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:06.221749: step 141190, loss = 0.29 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:06.543019: step 141200, loss = 0.26 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:07.002647: step 141210, loss = 0.33 (7827.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:07.322990: step 141220, loss = 0.24 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:07.643596: step 141230, loss = 0.35 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:07.965049: step 141240, loss = 0.37 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:08.283740: step 141250, loss = 0.35 (8137.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:08.607579: step 141260, loss = 0.35 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:08.927363: step 141270, loss = 0.36 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:09.247639: step 141280, loss = 0.30 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:09.567840: step 141290, loss = 0.29 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:09.888344: step 141300, loss = 0.34 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:10.347212: step 141310, loss = 0.29 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:10.669095: step 141320, loss = 0.36 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:10.988913: step 141330, loss = 0.31 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:11.309344: step 141340, loss = 0.32 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:11.629401: step 141350, loss = 0.33 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:11.949443: step 141360, loss = 0.24 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:12.267155: step 141370, loss = 0.37 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:12.587283: step 141380, loss = 0.28 (7827.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:12.907324: step 141390, loss = 0.32 (8118.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:13.226713: step 141400, loss = 0.34 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:13.686265: step 141410, loss = 0.30 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:14.006874: step 141420, loss = 0.35 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:14.328109: step 141430, loss = 0.33 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:14.649231: step 141440, loss = 0.47 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:14.968552: step 141450, loss = 0.36 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:15.287944: step 141460, loss = 0.37 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:15.608201: step 141470, loss = 0.31 (7944.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:15.927539: step 141480, loss = 0.41 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:16.248325: step 141490, loss = 0.31 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:16.568851: step 141500, loss = 0.34 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:17.014914: step 141510, loss = 0.29 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:17.336848: step 141520, loss = 0.39 (8072.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:17.659218: step 141530, loss = 0.35 (8018.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:17.979139: step 141540, loss = 0.34 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:18.298698: step 141550, loss = 0.34 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:18.619312: step 141560, loss = 0.31 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:18.938867: step 141570, loss = 0.33 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:19.257723: step 141580, loss = 0.32 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:19.578426: step 141590, loss = 0.27 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:19.899346: step 141600, loss = 0.28 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:20.362008: step 141610, loss = 0.30 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:20.685611: step 141620, loss = 0.27 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:21.006388: step 141630, loss = 0.42 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:21.324851: step 141640, loss = 0.34 (8012.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:21.645664: step 141650, loss = 0.34 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:21.966009: step 141660, loss = 0.30 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:22.286104: step 141670, loss = 0.39 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:22.606038: step 141680, loss = 0.37 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:22.926191: step 141690, loss = 0.37 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:23.245188: step 141700, loss = 0.32 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:23.698393: step 141710, loss = 0.26 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:24.020514: step 141720, loss = 0.36 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:24.340253: step 141730, loss = 0.32 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:24.660280: step 141740, loss = 0.35 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:24.980838: step 141750, loss = 0.31 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:25.301643: step 141760, loss = 0.30 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:25.621386: step 141770, loss = 0.33 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:25.941891: step 141780, loss = 0.28 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:26.262109: step 141790, loss = 0.31 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:26.584892: step 141800, loss = 0.32 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:27.033936: step 141810, loss = 0.35 (7807.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:27.354469: step 141820, loss = 0.29 (7798.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:27.672611: step 141830, loss = 0.30 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:27.992602: step 141840, loss = 0.35 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:28.311175: step 141850, loss = 0.34 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:28.631220: step 141860, loss = 0.33 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:28.950540: step 141870, loss = 0.34 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:29.273195: step 141880, loss = 0.29 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:29.591973: step 141890, loss = 0.31 (8167.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:29.912566: step 141900, loss = 0.39 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:30.366370: step 141910, loss = 0.27 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:30.687332: step 141920, loss = 0.33 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:31.008736: step 141930, loss = 0.34 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:31.327108: step 141940, loss = 0.30 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:31.646104: step 141950, loss = 0.29 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:31.963944: step 141960, loss = 0.32 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:32.285456: step 141970, loss = 0.33 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:32.602770: step 141980, loss = 0.34 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:32.923824: step 141990, loss = 0.27 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:33.241653: step 142000, loss = 0.35 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:33.786002: step 142010, loss = 0.37 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:34.104020: step 142020, loss = 0.38 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:34.423595: step 142030, loss = 0.27 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:34.742740: step 142040, loss = 0.33 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:35.061774: step 142050, loss = 0.29 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:35.380688: step 142060, loss = 0.27 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:35.699200: step 142070, loss = 0.28 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:36.017064: step 142080, loss = 0.37 (8075.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:36.335940: step 142090, loss = 0.29 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:36.655854: step 142100, loss = 0.33 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:37.105826: step 142110, loss = 0.36 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:37.425773: step 142120, loss = 0.31 (7892.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:37.744504: step 142130, loss = 0.34 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:38.063896: step 142140, loss = 0.25 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:38.386055: step 142150, loss = 0.36 (7994.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:38.705733: step 142160, loss = 0.32 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:39.029954: step 142170, loss = 0.27 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:39.352704: step 142180, loss = 0.31 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:39.671656: step 142190, loss = 0.31 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:39.991268: step 142200, loss = 0.35 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:40.443108: step 142210, loss = 0.27 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:40.764407: step 142220, loss = 0.37 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:41.082755: step 142230, loss = 0.32 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:41.403256: step 142240, loss = 0.40 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:41.723401: step 142250, loss = 0.25 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:42.042521: step 142260, loss = 0.33 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:42.360759: step 142270, loss = 0.47 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:42.679652: step 142280, loss = 0.44 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:43.001179: step 142290, loss = 0.27 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:43.321165: step 142300, loss = 0.33 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:43.781761: step 142310, loss = 0.30 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:44.101357: step 142320, loss = 0.28 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:44.424206: step 142330, loss = 0.24 (7880.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:44.744164: step 142340, loss = 0.37 (7768.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:45.066160: step 142350, loss = 0.36 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:45.386382: step 142360, loss = 0.33 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:45.706206: step 142370, loss = 0.34 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:46.029168: step 142380, loss = 0.27 (7557.4 examples/sec; 0.017 sec/batch)
2017-09-16 16:59:46.352948: step 142390, loss = 0.28 (7736.0 examples/sec; 0.017 sec/batch)
2017-09-16 16:59:46.676612: step 142400, loss = 0.28 (7934.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:47.130757: step 142410, loss = 0.36 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:47.450117: step 142420, loss = 0.38 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:47.769186: step 142430, loss = 0.27 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:48.089049: step 142440, loss = 0.31 (8129.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:48.411108: step 142450, loss = 0.34 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:48.732641: step 142460, loss = 0.44 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:49.051572: step 142470, loss = 0.30 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:49.371436: step 142480, loss = 0.29 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:49.690896: step 142490, loss = 0.24 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:50.010858: step 142500, loss = 0.33 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:50.472307: step 142510, loss = 0.32 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:50.791735: step 142520, loss = 0.38 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:51.110735: step 142530, loss = 0.27 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:51.430730: step 142540, loss = 0.30 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:51.752781: step 142550, loss = 0.33 (8140.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:52.074182: step 142560, loss = 0.35 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:52.397532: step 142570, loss = 0.33 (7911.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:52.717317: step 142580, loss = 0.28 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:53.038159: step 142590, loss = 0.34 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:53.356667: step 142600, loss = 0.32 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:53.815402: step 142610, loss = 0.34 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:54.134237: step 142620, loss = 0.31 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:54.455216: step 142630, loss = 0.31 (7774.8 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:54.775152: step 142640, loss = 0.23 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:55.093763: step 142650, loss = 0.26 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:55.414165: step 142660, loss = 0.34 (8093.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:55.733969: step 142670, loss = 0.25 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:56.052232: step 142680, loss = 0.32 (8184.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:56.372891: step 142690, loss = 0.37 (7829.5 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:56.691925: step 142700, loss = 0.31 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:57.155168: step 142710, loss = 0.38 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:57.474555: step 142720, loss = 0.29 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:57.794473: step 142730, loss = 0.35 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:58.113584: step 142740, loss = 0.46 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:58.434004: step 142750, loss = 0.35 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:58.754219: step 142760, loss = 0.36 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:59.073255: step 142770, loss = 0.39 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:59.392365: step 142780, loss = 0.30 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 16:59:59.711781: step 142790, loss = 0.28 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:00.031577: step 142800, loss = 0.30 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:00.486667: step 142810, loss = 0.31 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:00.807456: step 142820, loss = 0.28 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:01.127423: step 142830, loss = 0.33 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:01.446943: step 142840, loss = 0.32 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:01.771094: step 142850, loss = 0.27 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:02.090368: step 142860, loss = 0.23 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:02.410846: step 142870, loss = 0.32 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:02.731109: step 142880, loss = 0.36 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:03.049608: step 142890, loss = 0.32 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:03.373491: step 142900, loss = 0.31 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:03.823583: step 142910, loss = 0.42 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:04.147197: step 142920, loss = 0.32 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:04.468251: step 142930, loss = 0.32 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:04.787694: step 142940, loss = 0.27 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:05.107438: step 142950, loss = 0.27 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:05.427583: step 142960, loss = 0.32 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:05.748711: step 142970, loss = 0.30 (7884.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:06.070163: step 142980, loss = 0.31 (7702.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:06.393907: step 142990, loss = 0.29 (7651.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:06.716152: step 143000, loss = 0.32 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:07.264962: step 143010, loss = 0.30 (7844.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:07.586219: step 143020, loss = 0.29 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:07.903516: step 143030, loss = 0.30 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:08.224432: step 143040, loss = 0.23 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:08.544031: step 143050, loss = 0.35 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:08.866433: step 143060, loss = 0.24 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:09.186596: step 143070, loss = 0.32 (8131.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:09.505667: step 143080, loss = 0.29 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:09.826220: step 143090, loss = 0.35 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:10.148627: step 143100, loss = 0.37 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:10.609592: step 143110, loss = 0.27 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:10.928078: step 143120, loss = 0.38 (7947.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:11.247442: step 143130, loss = 0.26 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:11.568136: step 143140, loss = 0.37 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:11.888408: step 143150, loss = 0.30 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:12.206670: step 143160, loss = 0.29 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:12.527214: step 143170, loss = 0.22 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:12.847071: step 143180, loss = 0.30 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:13.165822: step 143190, loss = 0.33 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:13.483427: step 143200, loss = 0.26 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:13.948064: step 143210, loss = 0.34 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:14.269791: step 143220, loss = 0.31 (7898.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:14.588355: step 143230, loss = 0.32 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:14.907936: step 143240, loss = 0.32 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:15.227866: step 143250, loss = 0.33 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:15.546051: step 143260, loss = 0.31 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:15.866884: step 143270, loss = 0.27 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:16.190214: step 143280, loss = 0.30 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:16.511052: step 143290, loss = 0.29 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:16.832370: step 143300, loss = 0.29 (7705.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:17.284885: step 143310, loss = 0.35 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:17.605164: step 143320, loss = 0.29 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:17.927643: step 143330, loss = 0.36 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:18.248119: step 143340, loss = 0.32 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:18.568622: step 143350, loss = 0.32 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:18.888220: step 143360, loss = 0.28 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:19.208916: step 143370, loss = 0.36 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:19.530392: step 143380, loss = 0.25 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:19.853210: step 143390, loss = 0.32 (7743.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:20.171900: step 143400, loss = 0.26 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:20.627003: step 143410, loss = 0.27 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:20.946345: step 143420, loss = 0.34 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:21.267627: step 143430, loss = 0.26 (7881.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:21.588082: step 143440, loss = 0.38 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:21.907767: step 143450, loss = 0.23 (7791.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:22.226896: step 143460, loss = 0.27 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:22.550549: step 143470, loss = 0.30 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:22.870092: step 143480, loss = 0.23 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:23.191329: step 143490, loss = 0.31 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:23.512794: step 143500, loss = 0.28 (7977.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:23.973999: step 143510, loss = 0.31 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:24.296381: step 143520, loss = 0.32 (7729.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:24.614151: step 143530, loss = 0.35 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:24.935543: step 143540, loss = 0.39 (7849.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:25.255829: step 143550, loss = 0.29 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:25.576925: step 143560, loss = 0.27 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:25.897111: step 143570, loss = 0.32 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:26.216810: step 143580, loss = 0.32 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:26.539766: step 143590, loss = 0.29 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:26.860265: step 143600, loss = 0.29 (8034.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:27.311856: step 143610, loss = 0.28 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:27.631342: step 143620, loss = 0.33 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:27.953515: step 143630, loss = 0.38 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:28.272810: step 143640, loss = 0.29 (8126.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:28.593906: step 143650, loss = 0.27 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:28.917841: step 143660, loss = 0.28 (7433.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:29.237619: step 143670, loss = 0.33 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:29.558852: step 143680, loss = 0.39 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:29.878782: step 143690, loss = 0.30 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:30.198709: step 143700, loss = 0.27 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:30.659050: step 143710, loss = 0.38 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:30.980710: step 143720, loss = 0.29 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:31.300450: step 143730, loss = 0.37 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:31.622999: step 143740, loss = 0.33 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:31.948071: step 143750, loss = 0.27 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:32.270281: step 143760, loss = 0.28 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:32.589453: step 143770, loss = 0.30 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:32.908834: step 143780, loss = 0.33 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:33.228828: step 143790, loss = 0.29 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:33.549086: step 143800, loss = 0.33 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:34.012382: step 143810, loss = 0.27 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:34.334074: step 143820, loss = 0.30 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:34.654346: step 143830, loss = 0.26 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:34.975882: step 143840, loss = 0.36 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:35.294462: step 143850, loss = 0.29 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:35.613343: step 143860, loss = 0.27 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:35.933314: step 143870, loss = 0.35 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:36.253698: step 143880, loss = 0.29 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:36.574933: step 143890, loss = 0.27 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:36.897389: step 143900, loss = 0.24 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:37.360386: step 143910, loss = 0.25 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:37.689079: step 143920, loss = 0.35 (7452.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:38.010219: step 143930, loss = 0.34 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:38.329872: step 143940, loss = 0.29 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:38.649784: step 143950, loss = 0.28 (7899.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:38.970029: step 143960, loss = 0.28 (7828.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:39.290998: step 143970, loss = 0.27 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:39.612668: step 143980, loss = 0.26 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:39.931464: step 143990, loss = 0.33 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:40.249988: step 144000, loss = 0.21 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:40.802231: step 144010, loss = 0.37 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:41.123413: step 144020, loss = 0.27 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:41.443648: step 144030, loss = 0.26 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:41.763613: step 144040, loss = 0.35 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:42.082107: step 144050, loss = 0.29 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:42.400744: step 144060, loss = 0.33 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:42.720804: step 144070, loss = 0.28 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:43.040223: step 144080, loss = 0.25 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:43.360008: step 144090, loss = 0.33 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:43.686832: step 144100, loss = 0.38 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:44.144006: step 144110, loss = 0.27 (7922.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:44.465008: step 144120, loss = 0.29 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:44.784753: step 144130, loss = 0.31 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:45.107611: step 144140, loss = 0.27 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:45.427831: step 144150, loss = 0.39 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:45.746546: step 144160, loss = 0.25 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:46.068190: step 144170, loss = 0.33 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:46.388743: step 144180, loss = 0.33 (7865.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:46.711361: step 144190, loss = 0.22 (7689.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:47.033504: step 144200, loss = 0.27 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:47.483229: step 144210, loss = 0.33 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:47.803658: step 144220, loss = 0.29 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:48.123462: step 144230, loss = 0.36 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:48.442574: step 144240, loss = 0.35 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:48.760773: step 144250, loss = 0.28 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:49.085751: step 144260, loss = 0.35 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:49.405407: step 144270, loss = 0.35 (7819.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:49.726854: step 144280, loss = 0.28 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:50.045633: step 144290, loss = 0.32 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:50.365543: step 144300, loss = 0.31 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:50.825592: step 144310, loss = 0.30 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:51.147130: step 144320, loss = 0.30 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:51.466126: step 144330, loss = 0.30 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:51.789636: step 144340, loss = 0.39 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:52.109421: step 144350, loss = 0.25 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:52.427575: step 144360, loss = 0.27 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:52.747944: step 144370, loss = 0.31 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:53.069056: step 144380, loss = 0.31 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:53.388907: step 144390, loss = 0.29 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:53.710376: step 144400, loss = 0.31 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:54.167271: step 144410, loss = 0.22 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:54.489286: step 144420, loss = 0.27 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:54.811470: step 144430, loss = 0.24 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:55.131075: step 144440, loss = 0.27 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:55.451657: step 144450, loss = 0.26 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:55.769752: step 144460, loss = 0.30 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:56.090075: step 144470, loss = 0.37 (8057.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:56.411431: step 144480, loss = 0.34 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:56.730404: step 144490, loss = 0.33 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:57.051116: step 144500, loss = 0.27 (7740.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:00:57.499093: step 144510, loss = 0.24 (8116.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:57.819357: step 144520, loss = 0.24 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:58.139471: step 144530, loss = 0.33 (8140.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:58.460017: step 144540, loss = 0.31 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:58.778403: step 144550, loss = 0.24 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:59.096924: step 144560, loss = 0.30 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:59.417326: step 144570, loss = 0.29 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:00:59.734806: step 144580, loss = 0.29 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:00.054548: step 144590, loss = 0.33 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:00.373488: step 144600, loss = 0.26 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:00.831716: step 144610, loss = 0.28 (8136.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:01.151472: step 144620, loss = 0.40 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:01.473877: step 144630, loss = 0.29 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:01.795352: step 144640, loss = 0.32 (7512.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:01:02.114536: step 144650, loss = 0.32 (8170.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:02.435598: step 144660, loss = 0.34 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:02.755900: step 144670, loss = 0.27 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:03.074915: step 144680, loss = 0.32 (8077.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:03.395235: step 144690, loss = 0.28 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:03.715993: step 144700, loss = 0.30 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:04.162243: step 144710, loss = 0.31 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:04.483865: step 144720, loss = 0.29 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:04.804070: step 144730, loss = 0.37 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:05.125116: step 144740, loss = 0.37 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:05.444339: step 144750, loss = 0.36 (7837.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:05.762318: step 144760, loss = 0.32 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:06.082358: step 144770, loss = 0.28 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:06.404496: step 144780, loss = 0.33 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:06.727330: step 144790, loss = 0.29 (7677.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:01:07.047159: step 144800, loss = 0.24 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:07.509677: step 144810, loss = 0.37 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:07.829501: step 144820, loss = 0.34 (7833.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:08.147080: step 144830, loss = 0.26 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:08.464770: step 144840, loss = 0.24 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:08.783319: step 144850, loss = 0.22 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:09.102471: step 144860, loss = 0.36 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:09.421751: step 144870, loss = 0.27 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:09.748022: step 144880, loss = 0.36 (7511.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:01:10.066686: step 144890, loss = 0.37 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:10.386712: step 144900, loss = 0.33 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:10.847923: step 144910, loss = 0.32 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:11.168148: step 144920, loss = 0.24 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:11.488777: step 144930, loss = 0.25 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:11.808904: step 144940, loss = 0.28 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:12.128618: step 144950, loss = 0.25 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:12.449415: step 144960, loss = 0.32 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:12.772040: step 144970, loss = 0.34 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:13.091990: step 144980, loss = 0.26 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:13.411695: step 144990, loss = 0.24 (8127.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:13.731394: step 145000, loss = 0.29 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:14.291319: step 145010, loss = 0.26 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:14.611355: step 145020, loss = 0.33 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:14.933455: step 145030, loss = 0.27 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:15.253074: step 145040, loss = 0.21 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:15.576304: step 145050, loss = 0.29 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:15.896536: step 145060, loss = 0.27 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:16.218145: step 145070, loss = 0.27 (7777.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:16.538230: step 145080, loss = 0.29 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:16.858840: step 145090, loss = 0.34 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:17.178271: step 145100, loss = 0.27 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:17.636400: step 145110, loss = 0.33 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:17.954667: step 145120, loss = 0.34 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:18.273793: step 145130, loss = 0.24 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:18.592585: step 145140, loss = 0.25 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:18.912104: step 145150, loss = 0.38 (8121.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:19.230871: step 145160, loss = 0.24 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:19.553854: step 145170, loss = 0.39 (7557.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:01:19.877434: step 145180, loss = 0.23 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:20.198360: step 145190, loss = 0.28 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:20.518880: step 145200, loss = 0.25 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:20.977447: step 145210, loss = 0.27 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:21.301341: step 145220, loss = 0.27 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:21.621903: step 145230, loss = 0.27 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:21.940345: step 145240, loss = 0.32 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:22.260517: step 145250, loss = 0.31 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:22.580548: step 145260, loss = 0.30 (8128.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:22.899294: step 145270, loss = 0.27 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:23.217940: step 145280, loss = 0.28 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:23.539921: step 145290, loss = 0.23 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:23.862640: step 145300, loss = 0.27 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:24.315870: step 145310, loss = 0.35 (7856.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:24.634689: step 145320, loss = 0.27 (8149.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:24.955400: step 145330, loss = 0.31 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:25.275089: step 145340, loss = 0.34 (8132.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:25.595779: step 145350, loss = 0.25 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:25.914988: step 145360, loss = 0.32 (7867.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:26.233957: step 145370, loss = 0.32 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:26.552257: step 145380, loss = 0.28 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:26.873191: step 145390, loss = 0.26 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:27.192677: step 145400, loss = 0.29 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:27.636886: step 145410, loss = 0.24 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:27.958019: step 145420, loss = 0.25 (7936.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:28.279537: step 145430, loss = 0.27 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:28.598095: step 145440, loss = 0.25 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:28.916603: step 145450, loss = 0.29 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:29.236191: step 145460, loss = 0.35 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:29.556181: step 145470, loss = 0.24 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:29.875190: step 145480, loss = 0.26 (7912.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:30.194781: step 145490, loss = 0.30 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:30.515434: step 145500, loss = 0.28 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:30.966601: step 145510, loss = 0.26 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:31.286152: step 145520, loss = 0.26 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:31.604585: step 145530, loss = 0.23 (8180.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:31.925273: step 145540, loss = 0.26 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:32.243516: step 145550, loss = 0.21 (8161.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:32.562643: step 145560, loss = 0.26 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:32.881304: step 145570, loss = 0.28 (8163.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:33.201361: step 145580, loss = 0.27 (8141.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:33.520703: step 145590, loss = 0.24 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:33.843121: step 145600, loss = 0.24 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:34.304312: step 145610, loss = 0.35 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:34.621379: step 145620, loss = 0.29 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:34.944042: step 145630, loss = 0.26 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:35.261986: step 145640, loss = 0.26 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:35.580242: step 145650, loss = 0.22 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:35.900244: step 145660, loss = 0.25 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:36.221585: step 145670, loss = 0.33 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:36.541280: step 145680, loss = 0.25 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:36.860212: step 145690, loss = 0.28 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:37.182502: step 145700, loss = 0.28 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:37.636761: step 145710, loss = 0.28 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:37.956917: step 145720, loss = 0.26 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:38.278038: step 145730, loss = 0.29 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:38.597412: step 145740, loss = 0.24 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:38.920682: step 145750, loss = 0.36 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:39.240457: step 145760, loss = 0.24 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:39.560965: step 145770, loss = 0.27 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:39.882315: step 145780, loss = 0.25 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:40.202781: step 145790, loss = 0.24 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:40.523976: step 145800, loss = 0.21 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:40.980449: step 145810, loss = 0.28 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:41.301516: step 145820, loss = 0.36 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:41.622973: step 145830, loss = 0.31 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:41.943249: step 145840, loss = 0.26 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:42.261776: step 145850, loss = 0.28 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:42.583784: step 145860, loss = 0.21 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:42.906785: step 145870, loss = 0.31 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:43.225244: step 145880, loss = 0.30 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:43.544103: step 145890, loss = 0.26 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:43.863478: step 145900, loss = 0.33 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:44.324121: step 145910, loss = 0.33 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:44.644077: step 145920, loss = 0.21 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:44.963388: step 145930, loss = 0.21 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:45.282674: step 145940, loss = 0.21 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:45.601816: step 145950, loss = 0.24 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:45.923401: step 145960, loss = 0.33 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:46.242009: step 145970, loss = 0.24 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:46.563485: step 145980, loss = 0.25 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:46.884049: step 145990, loss = 0.27 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:47.204989: step 146000, loss = 0.25 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:47.754270: step 146010, loss = 0.24 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:48.073359: step 146020, loss = 0.27 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:48.392759: step 146030, loss = 0.32 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:48.712377: step 146040, loss = 0.25 (7822.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:49.031881: step 146050, loss = 0.28 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:49.352677: step 146060, loss = 0.24 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:49.671691: step 146070, loss = 0.28 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:49.991516: step 146080, loss = 0.22 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:50.313697: step 146090, loss = 0.31 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:50.634585: step 146100, loss = 0.26 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:51.085730: step 146110, loss = 0.30 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:51.405732: step 146120, loss = 0.25 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:51.724641: step 146130, loss = 0.24 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:52.045757: step 146140, loss = 0.32 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:52.364835: step 146150, loss = 0.21 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:52.685840: step 146160, loss = 0.36 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:53.005006: step 146170, loss = 0.24 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:53.323700: step 146180, loss = 0.40 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:53.644358: step 146190, loss = 0.29 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:53.964969: step 146200, loss = 0.28 (8120.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:54.416564: step 146210, loss = 0.24 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:54.737216: step 146220, loss = 0.25 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:55.060172: step 146230, loss = 0.26 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:55.382120: step 146240, loss = 0.28 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:55.702441: step 146250, loss = 0.19 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:56.022725: step 146260, loss = 0.31 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:56.343330: step 146270, loss = 0.30 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:56.664315: step 146280, loss = 0.22 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:56.984066: step 146290, loss = 0.21 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:57.304458: step 146300, loss = 0.29 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:57.760089: step 146310, loss = 0.36 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:58.079365: step 146320, loss = 0.28 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:58.404425: step 146330, loss = 0.28 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:58.723261: step 146340, loss = 0.25 (7856.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:59.047927: step 146350, loss = 0.35 (7750.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:01:59.368238: step 146360, loss = 0.31 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:01:59.688461: step 146370, loss = 0.30 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:00.010756: step 146380, loss = 0.28 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:00.331716: step 146390, loss = 0.22 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:00.651466: step 146400, loss = 0.23 (7957.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:01.111166: step 146410, loss = 0.26 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:01.432023: step 146420, loss = 0.28 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:01.753600: step 146430, loss = 0.26 (7970.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:02.075113: step 146440, loss = 0.34 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:02.394564: step 146450, loss = 0.31 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:02.714129: step 146460, loss = 0.32 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:03.034493: step 146470, loss = 0.23 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:03.354492: step 146480, loss = 0.30 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:03.674181: step 146490, loss = 0.23 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:03.996327: step 146500, loss = 0.34 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:04.454865: step 146510, loss = 0.26 (7559.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:04.776583: step 146520, loss = 0.34 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:05.095134: step 146530, loss = 0.28 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:05.415616: step 146540, loss = 0.26 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:05.734677: step 146550, loss = 0.26 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:06.056031: step 146560, loss = 0.29 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:06.378961: step 146570, loss = 0.27 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:06.700342: step 146580, loss = 0.23 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:07.023275: step 146590, loss = 0.27 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:07.342857: step 146600, loss = 0.25 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:07.789918: step 146610, loss = 0.22 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:08.108962: step 146620, loss = 0.27 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:08.427937: step 146630, loss = 0.26 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:08.748639: step 146640, loss = 0.29 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:09.070056: step 146650, loss = 0.25 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:09.390408: step 146660, loss = 0.23 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:09.708701: step 146670, loss = 0.26 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:10.027499: step 146680, loss = 0.26 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:10.348638: step 146690, loss = 0.23 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:10.667192: step 146700, loss = 0.24 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:11.122405: step 146710, loss = 0.23 (7822.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:11.442699: step 146720, loss = 0.25 (8166.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:11.766090: step 146730, loss = 0.37 (7435.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:12.089382: step 146740, loss = 0.34 (7497.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:12.408601: step 146750, loss = 0.28 (8000.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:12.730851: step 146760, loss = 0.28 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:13.051196: step 146770, loss = 0.20 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:13.371792: step 146780, loss = 0.22 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:13.692860: step 146790, loss = 0.25 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:14.013084: step 146800, loss = 0.21 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:14.475345: step 146810, loss = 0.30 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:14.796929: step 146820, loss = 0.21 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:15.119249: step 146830, loss = 0.29 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:15.439691: step 146840, loss = 0.25 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:15.761227: step 146850, loss = 0.24 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:16.080040: step 146860, loss = 0.26 (7950.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:16.402210: step 146870, loss = 0.29 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:16.721934: step 146880, loss = 0.32 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:17.042882: step 146890, loss = 0.30 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:17.363005: step 146900, loss = 0.35 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:17.824766: step 146910, loss = 0.20 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:18.146305: step 146920, loss = 0.24 (7622.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:18.468166: step 146930, loss = 0.25 (7864.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:18.788247: step 146940, loss = 0.25 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:19.107397: step 146950, loss = 0.26 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:19.427249: step 146960, loss = 0.23 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:19.748333: step 146970, loss = 0.33 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:20.066527: step 146980, loss = 0.25 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:20.385890: step 146990, loss = 0.22 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:20.706216: step 147000, loss = 0.26 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:21.245821: step 147010, loss = 0.32 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:21.564173: step 147020, loss = 0.21 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:21.883758: step 147030, loss = 0.30 (7816.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:22.203280: step 147040, loss = 0.21 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:22.522494: step 147050, loss = 0.26 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:22.842000: step 147060, loss = 0.28 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:23.164046: step 147070, loss = 0.27 (7818.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:23.483352: step 147080, loss = 0.22 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:23.805847: step 147090, loss = 0.22 (7586.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:24.124156: step 147100, loss = 0.28 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:24.581991: step 147110, loss = 0.30 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:24.902576: step 147120, loss = 0.22 (8129.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:25.222483: step 147130, loss = 0.28 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:25.541421: step 147140, loss = 0.24 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:25.863719: step 147150, loss = 0.23 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:26.184183: step 147160, loss = 0.30 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:26.504019: step 147170, loss = 0.33 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:26.827038: step 147180, loss = 0.30 (7095.4 examples/sec; 0.018 sec/batch)
2017-09-16 17:02:27.146096: step 147190, loss = 0.21 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:27.466839: step 147200, loss = 0.30 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:27.927624: step 147210, loss = 0.34 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:28.246494: step 147220, loss = 0.31 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:28.568537: step 147230, loss = 0.25 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:28.886528: step 147240, loss = 0.28 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:29.205098: step 147250, loss = 0.25 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:29.524148: step 147260, loss = 0.27 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:29.843165: step 147270, loss = 0.29 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:30.164305: step 147280, loss = 0.23 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:30.484649: step 147290, loss = 0.30 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:30.802478: step 147300, loss = 0.32 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:31.250126: step 147310, loss = 0.28 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:31.568926: step 147320, loss = 0.30 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:31.889546: step 147330, loss = 0.19 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:32.210745: step 147340, loss = 0.30 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:32.528409: step 147350, loss = 0.26 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:32.848063: step 147360, loss = 0.26 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:33.168564: step 147370, loss = 0.28 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:33.489846: step 147380, loss = 0.33 (7495.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:33.809219: step 147390, loss = 0.33 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:34.128924: step 147400, loss = 0.32 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:34.588399: step 147410, loss = 0.33 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:34.905887: step 147420, loss = 0.24 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:35.226518: step 147430, loss = 0.31 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:35.547314: step 147440, loss = 0.24 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:35.865348: step 147450, loss = 0.26 (8108.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:36.185293: step 147460, loss = 0.30 (7970.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:36.504453: step 147470, loss = 0.26 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:36.822921: step 147480, loss = 0.34 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:37.142647: step 147490, loss = 0.28 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:37.466254: step 147500, loss = 0.38 (7561.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:37.928091: step 147510, loss = 0.25 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:38.248951: step 147520, loss = 0.21 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:38.570834: step 147530, loss = 0.24 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:38.889958: step 147540, loss = 0.24 (7830.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:39.210522: step 147550, loss = 0.26 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:39.530502: step 147560, loss = 0.26 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:39.849419: step 147570, loss = 0.20 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:40.176240: step 147580, loss = 0.21 (8034.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:40.494797: step 147590, loss = 0.21 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:40.813573: step 147600, loss = 0.18 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:41.273082: step 147610, loss = 0.24 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:41.591488: step 147620, loss = 0.22 (7814.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:41.909469: step 147630, loss = 0.24 (8165.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:42.227492: step 147640, loss = 0.28 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:42.547121: step 147650, loss = 0.25 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:42.869254: step 147660, loss = 0.25 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:43.188858: step 147670, loss = 0.25 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:43.509668: step 147680, loss = 0.31 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:43.828910: step 147690, loss = 0.27 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:44.148653: step 147700, loss = 0.23 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:44.612918: step 147710, loss = 0.25 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:44.933690: step 147720, loss = 0.22 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:45.252808: step 147730, loss = 0.23 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:45.575994: step 147740, loss = 0.27 (7688.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:45.896094: step 147750, loss = 0.21 (8193.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:46.217161: step 147760, loss = 0.26 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:46.536947: step 147770, loss = 0.26 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:46.857906: step 147780, loss = 0.31 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:47.177690: step 147790, loss = 0.45 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:47.496414: step 147800, loss = 0.31 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:47.958297: step 147810, loss = 0.21 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:48.280116: step 147820, loss = 0.32 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:48.599095: step 147830, loss = 0.21 (8138.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:48.916149: step 147840, loss = 0.27 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:49.236562: step 147850, loss = 0.27 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:49.559469: step 147860, loss = 0.33 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:49.879733: step 147870, loss = 0.23 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:50.200054: step 147880, loss = 0.27 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:50.520915: step 147890, loss = 0.21 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:50.842565: step 147900, loss = 0.32 (7908.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:51.291949: step 147910, loss = 0.27 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:51.612711: step 147920, loss = 0.36 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:51.930944: step 147930, loss = 0.24 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:52.250043: step 147940, loss = 0.28 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:52.567026: step 147950, loss = 0.32 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:52.887701: step 147960, loss = 0.23 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:53.207307: step 147970, loss = 0.33 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:53.527439: step 147980, loss = 0.25 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:53.848140: step 147990, loss = 0.33 (8147.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:54.167822: step 148000, loss = 0.22 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:54.798415: step 148010, loss = 0.25 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:55.116325: step 148020, loss = 0.26 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:55.434361: step 148030, loss = 0.23 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:55.753251: step 148040, loss = 0.26 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:56.071428: step 148050, loss = 0.26 (8159.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:56.391091: step 148060, loss = 0.26 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:56.708958: step 148070, loss = 0.28 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:57.028700: step 148080, loss = 0.26 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:57.350503: step 148090, loss = 0.32 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:57.672967: step 148100, loss = 0.25 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:58.137290: step 148110, loss = 0.29 (7546.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:02:58.458963: step 148120, loss = 0.31 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:58.778885: step 148130, loss = 0.32 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:59.101021: step 148140, loss = 0.29 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:59.420307: step 148150, loss = 0.23 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:02:59.740222: step 148160, loss = 0.20 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:00.059422: step 148170, loss = 0.29 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:00.382424: step 148180, loss = 0.29 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:00.701309: step 148190, loss = 0.21 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:01.021434: step 148200, loss = 0.35 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:01.481259: step 148210, loss = 0.29 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:01.802066: step 148220, loss = 0.30 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:02.122942: step 148230, loss = 0.36 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:02.442170: step 148240, loss = 0.26 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:02.760433: step 148250, loss = 0.21 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:03.079859: step 148260, loss = 0.22 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:03.401721: step 148270, loss = 0.24 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:03.721333: step 148280, loss = 0.35 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:04.041211: step 148290, loss = 0.27 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:04.360798: step 148300, loss = 0.26 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:04.826406: step 148310, loss = 0.31 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:05.150809: step 148320, loss = 0.24 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:05.468851: step 148330, loss = 0.23 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:05.790503: step 148340, loss = 0.29 (8007.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:06.109769: step 148350, loss = 0.23 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:06.428132: step 148360, loss = 0.20 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:06.748252: step 148370, loss = 0.23 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:07.069473: step 148380, loss = 0.28 (7639.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:07.389353: step 148390, loss = 0.27 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:07.712148: step 148400, loss = 0.25 (7785.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:08.172739: step 148410, loss = 0.21 (8090.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:08.492731: step 148420, loss = 0.33 (7833.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:08.814167: step 148430, loss = 0.20 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:09.134225: step 148440, loss = 0.29 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:09.454434: step 148450, loss = 0.26 (7930.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:09.772816: step 148460, loss = 0.36 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:10.093338: step 148470, loss = 0.25 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:10.417611: step 148480, loss = 0.25 (7834.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:10.739255: step 148490, loss = 0.22 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:11.058192: step 148500, loss = 0.24 (7884.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:11.515272: step 148510, loss = 0.19 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:11.835432: step 148520, loss = 0.25 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:12.154930: step 148530, loss = 0.28 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:12.475589: step 148540, loss = 0.30 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:12.794540: step 148550, loss = 0.26 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:13.113074: step 148560, loss = 0.27 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:13.432285: step 148570, loss = 0.22 (8143.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:13.751317: step 148580, loss = 0.23 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:14.074852: step 148590, loss = 0.26 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:14.395611: step 148600, loss = 0.33 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:14.856266: step 148610, loss = 0.22 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:15.175318: step 148620, loss = 0.26 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:15.495046: step 148630, loss = 0.28 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:15.812829: step 148640, loss = 0.24 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:16.134145: step 148650, loss = 0.31 (7825.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:16.453276: step 148660, loss = 0.21 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:16.773101: step 148670, loss = 0.30 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:17.095422: step 148680, loss = 0.28 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:17.415336: step 148690, loss = 0.22 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:17.736324: step 148700, loss = 0.31 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:18.194049: step 148710, loss = 0.26 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:18.514431: step 148720, loss = 0.28 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:18.833351: step 148730, loss = 0.23 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:19.156931: step 148740, loss = 0.29 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:19.477380: step 148750, loss = 0.20 (7642.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:19.796085: step 148760, loss = 0.23 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:20.116116: step 148770, loss = 0.31 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:20.435743: step 148780, loss = 0.30 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:20.754539: step 148790, loss = 0.26 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:21.072580: step 148800, loss = 0.24 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:21.520848: step 148810, loss = 0.23 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:21.842113: step 148820, loss = 0.26 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:22.164582: step 148830, loss = 0.17 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:22.484923: step 148840, loss = 0.23 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:22.806317: step 148850, loss = 0.26 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:23.127663: step 148860, loss = 0.21 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:23.449556: step 148870, loss = 0.37 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:23.769848: step 148880, loss = 0.26 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:24.088924: step 148890, loss = 0.30 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:24.409302: step 148900, loss = 0.29 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:24.863657: step 148910, loss = 0.31 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:25.187835: step 148920, loss = 0.23 (8164.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:25.508443: step 148930, loss = 0.30 (7914.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:25.826431: step 148940, loss = 0.24 (8067.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:26.148116: step 148950, loss = 0.29 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:26.469847: step 148960, loss = 0.20 (7595.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:26.789837: step 148970, loss = 0.21 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:27.112307: step 148980, loss = 0.22 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:27.432021: step 148990, loss = 0.25 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:27.753313: step 149000, loss = 0.30 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:28.308950: step 149010, loss = 0.24 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:28.629230: step 149020, loss = 0.30 (7755.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:28.949374: step 149030, loss = 0.29 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:29.274325: step 149040, loss = 0.30 (7686.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:29.594619: step 149050, loss = 0.20 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:29.915201: step 149060, loss = 0.30 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:30.236622: step 149070, loss = 0.23 (7789.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:30.559017: step 149080, loss = 0.20 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:30.877330: step 149090, loss = 0.25 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:31.198283: step 149100, loss = 0.28 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:31.659961: step 149110, loss = 0.24 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:31.980991: step 149120, loss = 0.24 (7835.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:32.303541: step 149130, loss = 0.34 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:32.622113: step 149140, loss = 0.31 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:32.943176: step 149150, loss = 0.23 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:33.261376: step 149160, loss = 0.21 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:33.582118: step 149170, loss = 0.27 (8036.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:33.899650: step 149180, loss = 0.31 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:34.218774: step 149190, loss = 0.26 (8142.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:34.540298: step 149200, loss = 0.23 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:34.999312: step 149210, loss = 0.24 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:35.323590: step 149220, loss = 0.35 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:35.645977: step 149230, loss = 0.31 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:35.964576: step 149240, loss = 0.27 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:36.282923: step 149250, loss = 0.26 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:36.601217: step 149260, loss = 0.23 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:36.919988: step 149270, loss = 0.33 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:37.240286: step 149280, loss = 0.21 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:37.557904: step 149290, loss = 0.32 (7995.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:37.879036: step 149300, loss = 0.23 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:38.339172: step 149310, loss = 0.22 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:38.660196: step 149320, loss = 0.36 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:38.977800: step 149330, loss = 0.24 (8035.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:39.295805: step 149340, loss = 0.27 (8217.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:39.616788: step 149350, loss = 0.21 (7716.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:39.937441: step 149360, loss = 0.37 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:40.258352: step 149370, loss = 0.28 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:40.577283: step 149380, loss = 0.31 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:40.895586: step 149390, loss = 0.24 (8106.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:41.216106: step 149400, loss = 0.20 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:41.670336: step 149410, loss = 0.21 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:41.990869: step 149420, loss = 0.20 (7852.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:42.308994: step 149430, loss = 0.28 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:42.627525: step 149440, loss = 0.25 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:42.947129: step 149450, loss = 0.22 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:43.269242: step 149460, loss = 0.25 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:43.591261: step 149470, loss = 0.27 (7795.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:43.911235: step 149480, loss = 0.21 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:44.233329: step 149490, loss = 0.30 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:44.555037: step 149500, loss = 0.26 (7916.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:45.005573: step 149510, loss = 0.23 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:45.326027: step 149520, loss = 0.26 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:45.646201: step 149530, loss = 0.23 (8130.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:45.967765: step 149540, loss = 0.26 (7493.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:03:46.288243: step 149550, loss = 0.26 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:46.607138: step 149560, loss = 0.28 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:46.925923: step 149570, loss = 0.22 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:47.243369: step 149580, loss = 0.21 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:47.562264: step 149590, loss = 0.29 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:47.882546: step 149600, loss = 0.26 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:48.330300: step 149610, loss = 0.31 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:48.652237: step 149620, loss = 0.24 (7809.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:48.972250: step 149630, loss = 0.21 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:49.291862: step 149640, loss = 0.30 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:49.612931: step 149650, loss = 0.23 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:49.934489: step 149660, loss = 0.22 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:50.252442: step 149670, loss = 0.23 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:50.571467: step 149680, loss = 0.31 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:50.891270: step 149690, loss = 0.27 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:51.210006: step 149700, loss = 0.24 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:51.672604: step 149710, loss = 0.29 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:51.994312: step 149720, loss = 0.25 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:52.312479: step 149730, loss = 0.30 (8090.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:52.633177: step 149740, loss = 0.29 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:52.952310: step 149750, loss = 0.23 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:53.272403: step 149760, loss = 0.23 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:53.591427: step 149770, loss = 0.31 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:53.911494: step 149780, loss = 0.24 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:54.230387: step 149790, loss = 0.23 (8066.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:54.548714: step 149800, loss = 0.19 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:55.012906: step 149810, loss = 0.17 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:55.332450: step 149820, loss = 0.26 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:55.650783: step 149830, loss = 0.26 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:55.972705: step 149840, loss = 0.26 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:56.291992: step 149850, loss = 0.26 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:56.612570: step 149860, loss = 0.25 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:56.932692: step 149870, loss = 0.37 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:57.252863: step 149880, loss = 0.20 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:57.571391: step 149890, loss = 0.24 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:57.890586: step 149900, loss = 0.22 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:58.352510: step 149910, loss = 0.29 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:58.672443: step 149920, loss = 0.29 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:58.990149: step 149930, loss = 0.21 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:59.309731: step 149940, loss = 0.21 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:59.629402: step 149950, loss = 0.20 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:03:59.950390: step 149960, loss = 0.33 (8075.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:00.272036: step 149970, loss = 0.33 (7530.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:04:00.592702: step 149980, loss = 0.22 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:00.913260: step 149990, loss = 0.31 (7534.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:04:01.234455: step 150000, loss = 0.35 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:01.790695: step 150010, loss = 0.27 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:02.111060: step 150020, loss = 0.23 (8002.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:02.432072: step 150030, loss = 0.24 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:02.754310: step 150040, loss = 0.27 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:03.074651: step 150050, loss = 0.31 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:03.394780: step 150060, loss = 0.23 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:03.713119: step 150070, loss = 0.23 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:04.031444: step 150080, loss = 0.30 (7971.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:04.353180: step 150090, loss = 0.25 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:04.672263: step 150100, loss = 0.21 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:05.137085: step 150110, loss = 0.22 (7951.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:05.458069: step 150120, loss = 0.21 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:05.780662: step 150130, loss = 0.27 (7823.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:06.100133: step 150140, loss = 0.24 (8157.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:06.419828: step 150150, loss = 0.29 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:06.739288: step 150160, loss = 0.24 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:07.060037: step 150170, loss = 0.26 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:07.378369: step 150180, loss = 0.29 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:07.697819: step 150190, loss = 0.24 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:08.017473: step 150200, loss = 0.32 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:08.477274: step 150210, loss = 0.21 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:08.797145: step 150220, loss = 0.29 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:09.116055: step 150230, loss = 0.27 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:09.438230: step 150240, loss = 0.22 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:09.758891: step 150250, loss = 0.27 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:10.078701: step 150260, loss = 0.23 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:10.398593: step 150270, loss = 0.24 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:10.716839: step 150280, loss = 0.25 (8140.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:11.039867: step 150290, loss = 0.30 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:11.360536: step 150300, loss = 0.24 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:11.821189: step 150310, loss = 0.25 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:12.145059: step 150320, loss = 0.20 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:12.462728: step 150330, loss = 0.30 (7985.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:12.782863: step 150340, loss = 0.25 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:13.102570: step 150350, loss = 0.22 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:13.421586: step 150360, loss = 0.22 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:13.739446: step 150370, loss = 0.25 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:14.063278: step 150380, loss = 0.25 (7934.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:14.388758: step 150390, loss = 0.29 (7500.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:04:14.708640: step 150400, loss = 0.29 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:15.168229: step 150410, loss = 0.26 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:15.488781: step 150420, loss = 0.23 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:15.811035: step 150430, loss = 0.20 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:16.131944: step 150440, loss = 0.22 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:16.452743: step 150450, loss = 0.29 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:16.772385: step 150460, loss = 0.23 (7985.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:17.093200: step 150470, loss = 0.17 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:17.412240: step 150480, loss = 0.23 (8097.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:17.732435: step 150490, loss = 0.21 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:18.053981: step 150500, loss = 0.31 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:18.515091: step 150510, loss = 0.21 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:18.837452: step 150520, loss = 0.30 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:19.156888: step 150530, loss = 0.32 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:19.479494: step 150540, loss = 0.24 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:19.799996: step 150550, loss = 0.29 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:20.120206: step 150560, loss = 0.21 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:20.439956: step 150570, loss = 0.26 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:20.760538: step 150580, loss = 0.17 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:21.083045: step 150590, loss = 0.26 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:21.402187: step 150600, loss = 0.23 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:21.863619: step 150610, loss = 0.18 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:22.183865: step 150620, loss = 0.18 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:22.504743: step 150630, loss = 0.31 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:22.823848: step 150640, loss = 0.28 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:23.144828: step 150650, loss = 0.23 (8105.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:23.465530: step 150660, loss = 0.22 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:23.787041: step 150670, loss = 0.22 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:24.108067: step 150680, loss = 0.24 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:24.425750: step 150690, loss = 0.24 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:24.745808: step 150700, loss = 0.24 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:25.203450: step 150710, loss = 0.20 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:25.521858: step 150720, loss = 0.22 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:25.843040: step 150730, loss = 0.27 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:26.167080: step 150740, loss = 0.31 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:26.487186: step 150750, loss = 0.29 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:26.806097: step 150760, loss = 0.21 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:27.126292: step 150770, loss = 0.22 (8005.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:27.447833: step 150780, loss = 0.25 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:27.766339: step 150790, loss = 0.22 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:28.085749: step 150800, loss = 0.29 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:28.532243: step 150810, loss = 0.34 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:28.852015: step 150820, loss = 0.25 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:29.170603: step 150830, loss = 0.19 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:29.491187: step 150840, loss = 0.27 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:29.812096: step 150850, loss = 0.22 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:30.131863: step 150860, loss = 0.19 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:30.452737: step 150870, loss = 0.29 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:30.770742: step 150880, loss = 0.24 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:31.093430: step 150890, loss = 0.24 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:31.413706: step 150900, loss = 0.26 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:31.869396: step 150910, loss = 0.22 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:32.189213: step 150920, loss = 0.23 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:32.508417: step 150930, loss = 0.29 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:32.831013: step 150940, loss = 0.28 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:33.150142: step 150950, loss = 0.24 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:33.474842: step 150960, loss = 0.23 (7879.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:33.795236: step 150970, loss = 0.21 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:34.116357: step 150980, loss = 0.29 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:34.437264: step 150990, loss = 0.21 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:34.757552: step 151000, loss = 0.22 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:35.317669: step 151010, loss = 0.26 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:35.636039: step 151020, loss = 0.23 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:35.957424: step 151030, loss = 0.32 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:36.277758: step 151040, loss = 0.28 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:36.597497: step 151050, loss = 0.22 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:36.922889: step 151060, loss = 0.23 (7562.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:04:37.244479: step 151070, loss = 0.21 (7821.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:37.563747: step 151080, loss = 0.21 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:37.882284: step 151090, loss = 0.23 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:38.202031: step 151100, loss = 0.31 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:38.658834: step 151110, loss = 0.25 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:38.979037: step 151120, loss = 0.32 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:39.297991: step 151130, loss = 0.29 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:39.616591: step 151140, loss = 0.27 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:39.937694: step 151150, loss = 0.29 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:40.257449: step 151160, loss = 0.29 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:40.577654: step 151170, loss = 0.28 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:40.895843: step 151180, loss = 0.31 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:41.216901: step 151190, loss = 0.29 (8130.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:41.536560: step 151200, loss = 0.27 (8174.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:42.000573: step 151210, loss = 0.28 (7873.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:42.319992: step 151220, loss = 0.24 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:42.639456: step 151230, loss = 0.27 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:42.958505: step 151240, loss = 0.27 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:43.277452: step 151250, loss = 0.24 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:43.595718: step 151260, loss = 0.29 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:43.914553: step 151270, loss = 0.24 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:44.236167: step 151280, loss = 0.23 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:44.557323: step 151290, loss = 0.21 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:44.875367: step 151300, loss = 0.21 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:45.341370: step 151310, loss = 0.25 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:45.662491: step 151320, loss = 0.27 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:45.982476: step 151330, loss = 0.21 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:46.302652: step 151340, loss = 0.27 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:46.628739: step 151350, loss = 0.26 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:46.947715: step 151360, loss = 0.26 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:47.265957: step 151370, loss = 0.21 (8151.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:47.588168: step 151380, loss = 0.27 (7947.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:47.906364: step 151390, loss = 0.24 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:48.227231: step 151400, loss = 0.22 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:48.686228: step 151410, loss = 0.24 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:49.004056: step 151420, loss = 0.30 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:49.325110: step 151430, loss = 0.24 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:49.644534: step 151440, loss = 0.24 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:49.965956: step 151450, loss = 0.19 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:50.289314: step 151460, loss = 0.24 (8089.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:50.609105: step 151470, loss = 0.19 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:50.931336: step 151480, loss = 0.22 (7192.4 examples/sec; 0.018 sec/batch)
2017-09-16 17:04:51.252740: step 151490, loss = 0.22 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:51.574680: step 151500, loss = 0.26 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:52.023079: step 151510, loss = 0.27 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:52.344349: step 151520, loss = 0.34 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:52.664803: step 151530, loss = 0.24 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:52.986131: step 151540, loss = 0.30 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:53.306512: step 151550, loss = 0.22 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:53.627700: step 151560, loss = 0.26 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:53.946295: step 151570, loss = 0.29 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:54.264163: step 151580, loss = 0.20 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:54.582492: step 151590, loss = 0.24 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:54.900910: step 151600, loss = 0.27 (7991.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:55.361107: step 151610, loss = 0.17 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:55.680450: step 151620, loss = 0.19 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:56.001271: step 151630, loss = 0.21 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:56.323255: step 151640, loss = 0.31 (7945.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:56.645891: step 151650, loss = 0.20 (7471.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:04:56.964478: step 151660, loss = 0.27 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:57.285519: step 151670, loss = 0.33 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:57.604082: step 151680, loss = 0.23 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:57.925316: step 151690, loss = 0.19 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:58.243240: step 151700, loss = 0.26 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:58.701854: step 151710, loss = 0.23 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:59.021374: step 151720, loss = 0.18 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:59.341013: step 151730, loss = 0.20 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:59.659849: step 151740, loss = 0.22 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:04:59.981833: step 151750, loss = 0.27 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:00.300595: step 151760, loss = 0.33 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:00.621082: step 151770, loss = 0.18 (8096.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:00.940309: step 151780, loss = 0.27 (7938.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:01.260797: step 151790, loss = 0.31 (7805.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:01.581541: step 151800, loss = 0.25 (7802.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:02.037863: step 151810, loss = 0.23 (8040.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:02.357536: step 151820, loss = 0.21 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:02.681176: step 151830, loss = 0.26 (7809.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:03.005481: step 151840, loss = 0.30 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:03.325654: step 151850, loss = 0.26 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:03.645220: step 151860, loss = 0.24 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:03.966508: step 151870, loss = 0.20 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:04.286714: step 151880, loss = 0.25 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:04.606719: step 151890, loss = 0.29 (8142.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:04.926348: step 151900, loss = 0.29 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:05.388344: step 151910, loss = 0.18 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:05.707548: step 151920, loss = 0.22 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:06.026313: step 151930, loss = 0.23 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:06.346804: step 151940, loss = 0.23 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:06.670079: step 151950, loss = 0.23 (8194.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:06.990989: step 151960, loss = 0.25 (7651.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:07.311134: step 151970, loss = 0.25 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:07.630684: step 151980, loss = 0.29 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:07.951010: step 151990, loss = 0.20 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:08.269102: step 152000, loss = 0.25 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:08.829160: step 152010, loss = 0.26 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:09.148670: step 152020, loss = 0.19 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:09.468723: step 152030, loss = 0.22 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:09.789679: step 152040, loss = 0.24 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:10.109478: step 152050, loss = 0.26 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:10.432063: step 152060, loss = 0.20 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:10.752996: step 152070, loss = 0.28 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:11.076146: step 152080, loss = 0.30 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:11.396295: step 152090, loss = 0.24 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:11.716886: step 152100, loss = 0.30 (7870.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:12.176710: step 152110, loss = 0.25 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:12.498463: step 152120, loss = 0.21 (7661.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:12.819782: step 152130, loss = 0.24 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:13.141309: step 152140, loss = 0.25 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:13.461845: step 152150, loss = 0.35 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:13.785168: step 152160, loss = 0.16 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:14.105654: step 152170, loss = 0.23 (7969.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:14.425912: step 152180, loss = 0.22 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:14.746098: step 152190, loss = 0.28 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:15.065220: step 152200, loss = 0.28 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:15.529678: step 152210, loss = 0.24 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:15.850993: step 152220, loss = 0.18 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:16.173693: step 152230, loss = 0.29 (7775.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:16.495495: step 152240, loss = 0.23 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:16.820276: step 152250, loss = 0.21 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:17.143202: step 152260, loss = 0.28 (7977.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:17.461708: step 152270, loss = 0.20 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:17.780231: step 152280, loss = 0.25 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:18.101278: step 152290, loss = 0.21 (7818.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:18.429594: step 152300, loss = 0.24 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:18.891248: step 152310, loss = 0.22 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:19.211891: step 152320, loss = 0.22 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:19.532683: step 152330, loss = 0.18 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:19.857167: step 152340, loss = 0.23 (7494.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:20.176365: step 152350, loss = 0.24 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:20.497319: step 152360, loss = 0.26 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:20.817384: step 152370, loss = 0.24 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:21.137524: step 152380, loss = 0.26 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:21.458314: step 152390, loss = 0.24 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:21.780247: step 152400, loss = 0.23 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:22.246341: step 152410, loss = 0.19 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:22.570612: step 152420, loss = 0.27 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:22.891053: step 152430, loss = 0.22 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:23.209110: step 152440, loss = 0.20 (8111.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:23.529481: step 152450, loss = 0.27 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:23.848693: step 152460, loss = 0.20 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:24.168786: step 152470, loss = 0.22 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:24.489664: step 152480, loss = 0.25 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:24.813565: step 152490, loss = 0.23 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:25.136273: step 152500, loss = 0.25 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:25.593854: step 152510, loss = 0.19 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:25.914679: step 152520, loss = 0.24 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:26.235423: step 152530, loss = 0.22 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:26.557055: step 152540, loss = 0.26 (7814.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:26.876770: step 152550, loss = 0.28 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:27.197083: step 152560, loss = 0.20 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:27.516275: step 152570, loss = 0.23 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:27.838332: step 152580, loss = 0.20 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:28.161755: step 152590, loss = 0.17 (8124.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:28.483648: step 152600, loss = 0.30 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:28.938466: step 152610, loss = 0.21 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:29.257376: step 152620, loss = 0.20 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:29.577140: step 152630, loss = 0.27 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:29.898894: step 152640, loss = 0.28 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:30.220115: step 152650, loss = 0.31 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:30.540905: step 152660, loss = 0.18 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:30.859732: step 152670, loss = 0.21 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:31.184036: step 152680, loss = 0.25 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:31.504578: step 152690, loss = 0.20 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:31.825024: step 152700, loss = 0.18 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:32.285343: step 152710, loss = 0.22 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:32.606017: step 152720, loss = 0.19 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:32.923927: step 152730, loss = 0.33 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:33.245090: step 152740, loss = 0.21 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:33.563400: step 152750, loss = 0.29 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:33.886123: step 152760, loss = 0.24 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:34.208608: step 152770, loss = 0.26 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:34.529339: step 152780, loss = 0.26 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:34.851078: step 152790, loss = 0.23 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:35.171660: step 152800, loss = 0.21 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:35.629395: step 152810, loss = 0.21 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:35.950718: step 152820, loss = 0.19 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:36.272261: step 152830, loss = 0.24 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:36.591554: step 152840, loss = 0.19 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:36.910428: step 152850, loss = 0.20 (8005.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:37.229910: step 152860, loss = 0.23 (8143.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:37.550373: step 152870, loss = 0.26 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:37.871109: step 152880, loss = 0.28 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:38.192264: step 152890, loss = 0.20 (8017.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:38.514171: step 152900, loss = 0.21 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:38.975501: step 152910, loss = 0.25 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:39.295068: step 152920, loss = 0.29 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:39.616812: step 152930, loss = 0.25 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:39.937107: step 152940, loss = 0.26 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:40.257169: step 152950, loss = 0.18 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:40.578523: step 152960, loss = 0.21 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:40.899479: step 152970, loss = 0.18 (7913.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:41.221364: step 152980, loss = 0.21 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:41.540333: step 152990, loss = 0.20 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:41.858786: step 153000, loss = 0.21 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:42.408687: step 153010, loss = 0.27 (7826.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:42.731514: step 153020, loss = 0.32 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:43.051716: step 153030, loss = 0.20 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:43.371861: step 153040, loss = 0.22 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:43.690609: step 153050, loss = 0.34 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:44.010763: step 153060, loss = 0.24 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:44.332426: step 153070, loss = 0.24 (7618.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:44.652293: step 153080, loss = 0.22 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:44.971781: step 153090, loss = 0.26 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:45.291758: step 153100, loss = 0.25 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:45.748343: step 153110, loss = 0.20 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:46.068316: step 153120, loss = 0.24 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:46.387654: step 153130, loss = 0.28 (8031.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:46.708549: step 153140, loss = 0.23 (7711.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:47.027046: step 153150, loss = 0.22 (8159.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:47.347346: step 153160, loss = 0.22 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:47.668298: step 153170, loss = 0.26 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:47.988644: step 153180, loss = 0.26 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:48.311742: step 153190, loss = 0.20 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:48.633797: step 153200, loss = 0.25 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:49.088649: step 153210, loss = 0.26 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:49.411381: step 153220, loss = 0.18 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:49.731598: step 153230, loss = 0.30 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:50.052990: step 153240, loss = 0.27 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:50.371795: step 153250, loss = 0.18 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:50.689150: step 153260, loss = 0.31 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:51.008754: step 153270, loss = 0.22 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:51.329869: step 153280, loss = 0.25 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:51.649059: step 153290, loss = 0.24 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:51.968639: step 153300, loss = 0.19 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:52.426863: step 153310, loss = 0.22 (8167.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:52.746489: step 153320, loss = 0.19 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:53.067544: step 153330, loss = 0.24 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:53.388370: step 153340, loss = 0.25 (7686.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:05:53.709550: step 153350, loss = 0.21 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:54.029180: step 153360, loss = 0.32 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:54.348190: step 153370, loss = 0.26 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:54.669429: step 153380, loss = 0.20 (7789.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:54.988897: step 153390, loss = 0.32 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:55.311138: step 153400, loss = 0.25 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:55.769731: step 153410, loss = 0.20 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:56.090012: step 153420, loss = 0.24 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:56.410705: step 153430, loss = 0.22 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:56.729023: step 153440, loss = 0.19 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:57.047155: step 153450, loss = 0.23 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:57.367074: step 153460, loss = 0.22 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:57.688862: step 153470, loss = 0.30 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:58.009910: step 153480, loss = 0.24 (7870.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:58.327865: step 153490, loss = 0.25 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:58.648801: step 153500, loss = 0.25 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:59.096991: step 153510, loss = 0.22 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:59.414902: step 153520, loss = 0.25 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:05:59.734166: step 153530, loss = 0.22 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:00.053848: step 153540, loss = 0.20 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:00.372890: step 153550, loss = 0.30 (8142.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:00.691789: step 153560, loss = 0.24 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:01.012896: step 153570, loss = 0.20 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:01.331581: step 153580, loss = 0.19 (8128.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:01.657962: step 153590, loss = 0.20 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:01.979249: step 153600, loss = 0.21 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:02.442733: step 153610, loss = 0.27 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:02.763125: step 153620, loss = 0.25 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:03.083346: step 153630, loss = 0.28 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:03.405249: step 153640, loss = 0.16 (7873.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:03.733077: step 153650, loss = 0.21 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:04.052771: step 153660, loss = 0.24 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:04.372163: step 153670, loss = 0.23 (8133.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:04.692184: step 153680, loss = 0.20 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:05.012638: step 153690, loss = 0.22 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:05.331137: step 153700, loss = 0.25 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:05.779472: step 153710, loss = 0.22 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:06.098996: step 153720, loss = 0.23 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:06.418681: step 153730, loss = 0.23 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:06.737951: step 153740, loss = 0.27 (7921.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:07.055891: step 153750, loss = 0.32 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:07.375832: step 153760, loss = 0.27 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:07.695689: step 153770, loss = 0.21 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:08.016279: step 153780, loss = 0.20 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:08.336153: step 153790, loss = 0.25 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:08.655221: step 153800, loss = 0.20 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:09.117222: step 153810, loss = 0.20 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:09.438562: step 153820, loss = 0.25 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:09.759289: step 153830, loss = 0.27 (7825.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:10.078551: step 153840, loss = 0.20 (8014.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:10.398556: step 153850, loss = 0.25 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:10.717239: step 153860, loss = 0.24 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:11.036173: step 153870, loss = 0.23 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:11.354744: step 153880, loss = 0.21 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:11.673173: step 153890, loss = 0.17 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:11.993510: step 153900, loss = 0.28 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:12.454974: step 153910, loss = 0.21 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:12.773713: step 153920, loss = 0.20 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:13.094630: step 153930, loss = 0.31 (7840.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:13.414895: step 153940, loss = 0.30 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:13.735072: step 153950, loss = 0.26 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:14.055191: step 153960, loss = 0.21 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:14.378658: step 153970, loss = 0.29 (7787.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:14.698129: step 153980, loss = 0.23 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:15.019208: step 153990, loss = 0.28 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:15.340317: step 154000, loss = 0.18 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:15.889001: step 154010, loss = 0.21 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:16.209112: step 154020, loss = 0.22 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:16.528978: step 154030, loss = 0.22 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:16.849774: step 154040, loss = 0.26 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:17.170611: step 154050, loss = 0.19 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:17.490783: step 154060, loss = 0.18 (7836.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:17.813790: step 154070, loss = 0.23 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:18.136404: step 154080, loss = 0.23 (7982.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:18.458567: step 154090, loss = 0.18 (7942.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:18.778440: step 154100, loss = 0.22 (8127.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:19.224623: step 154110, loss = 0.23 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:19.547752: step 154120, loss = 0.18 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:19.866854: step 154130, loss = 0.25 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:20.186517: step 154140, loss = 0.20 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:20.508350: step 154150, loss = 0.29 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:20.829457: step 154160, loss = 0.24 (7833.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:21.148804: step 154170, loss = 0.28 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:21.469025: step 154180, loss = 0.24 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:21.790590: step 154190, loss = 0.20 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:22.111286: step 154200, loss = 0.26 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:22.572190: step 154210, loss = 0.21 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:22.894913: step 154220, loss = 0.34 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:23.218707: step 154230, loss = 0.21 (7959.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:23.538122: step 154240, loss = 0.23 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:23.857858: step 154250, loss = 0.25 (7606.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:24.176967: step 154260, loss = 0.21 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:24.496753: step 154270, loss = 0.27 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:24.815513: step 154280, loss = 0.20 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:25.134289: step 154290, loss = 0.19 (8108.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:25.455729: step 154300, loss = 0.19 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:25.916931: step 154310, loss = 0.23 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:26.237358: step 154320, loss = 0.16 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:26.558856: step 154330, loss = 0.18 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:26.881462: step 154340, loss = 0.27 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:27.202823: step 154350, loss = 0.23 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:27.523008: step 154360, loss = 0.25 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:27.842389: step 154370, loss = 0.28 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:28.161486: step 154380, loss = 0.25 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:28.482960: step 154390, loss = 0.25 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:28.803437: step 154400, loss = 0.19 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:29.263297: step 154410, loss = 0.30 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:29.585177: step 154420, loss = 0.23 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:29.907055: step 154430, loss = 0.35 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:30.228109: step 154440, loss = 0.21 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:30.547849: step 154450, loss = 0.22 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:30.866702: step 154460, loss = 0.17 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:31.186911: step 154470, loss = 0.29 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:31.508202: step 154480, loss = 0.16 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:31.828966: step 154490, loss = 0.20 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:32.152176: step 154500, loss = 0.35 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:32.614143: step 154510, loss = 0.28 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:32.934609: step 154520, loss = 0.29 (7942.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:33.254614: step 154530, loss = 0.22 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:33.575365: step 154540, loss = 0.22 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:33.893979: step 154550, loss = 0.27 (8093.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:34.213165: step 154560, loss = 0.25 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:34.534298: step 154570, loss = 0.17 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:34.855486: step 154580, loss = 0.27 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:35.175943: step 154590, loss = 0.26 (7825.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:35.495476: step 154600, loss = 0.23 (8145.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:35.958217: step 154610, loss = 0.24 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:36.280625: step 154620, loss = 0.25 (7870.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:36.602821: step 154630, loss = 0.30 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:36.923376: step 154640, loss = 0.21 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:37.242285: step 154650, loss = 0.28 (7978.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:37.561477: step 154660, loss = 0.25 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:37.881661: step 154670, loss = 0.21 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:38.202310: step 154680, loss = 0.22 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:38.519660: step 154690, loss = 0.25 (8167.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:38.842208: step 154700, loss = 0.30 (7632.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:39.303762: step 154710, loss = 0.23 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:39.627368: step 154720, loss = 0.26 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:39.948455: step 154730, loss = 0.25 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:40.268222: step 154740, loss = 0.22 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:40.590628: step 154750, loss = 0.16 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:40.909843: step 154760, loss = 0.21 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:41.232191: step 154770, loss = 0.28 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:41.551994: step 154780, loss = 0.23 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:41.872677: step 154790, loss = 0.18 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:42.191908: step 154800, loss = 0.30 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:42.639680: step 154810, loss = 0.24 (7832.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:42.961930: step 154820, loss = 0.24 (7749.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:43.284710: step 154830, loss = 0.21 (7637.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:43.604443: step 154840, loss = 0.30 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:43.923432: step 154850, loss = 0.24 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:44.243361: step 154860, loss = 0.29 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:44.563919: step 154870, loss = 0.22 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:44.883393: step 154880, loss = 0.23 (7861.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:45.203761: step 154890, loss = 0.20 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:45.525617: step 154900, loss = 0.24 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:45.985187: step 154910, loss = 0.25 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:46.306325: step 154920, loss = 0.25 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:46.629358: step 154930, loss = 0.26 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:46.950692: step 154940, loss = 0.23 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:47.271313: step 154950, loss = 0.26 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:47.591666: step 154960, loss = 0.26 (8120.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:47.910602: step 154970, loss = 0.21 (7909.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:48.232213: step 154980, loss = 0.25 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:48.557153: step 154990, loss = 0.28 (7857.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:48.876420: step 155000, loss = 0.20 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:49.441958: step 155010, loss = 0.16 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:49.762334: step 155020, loss = 0.29 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:50.083028: step 155030, loss = 0.21 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:50.403280: step 155040, loss = 0.20 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:50.722889: step 155050, loss = 0.21 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:51.041655: step 155060, loss = 0.17 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:51.363002: step 155070, loss = 0.25 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:51.682069: step 155080, loss = 0.24 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:52.000795: step 155090, loss = 0.20 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:52.320065: step 155100, loss = 0.23 (7824.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:52.779516: step 155110, loss = 0.25 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:53.098702: step 155120, loss = 0.20 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:53.418033: step 155130, loss = 0.25 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:53.738890: step 155140, loss = 0.29 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:54.060625: step 155150, loss = 0.24 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:54.382307: step 155160, loss = 0.21 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:54.704277: step 155170, loss = 0.18 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:55.024551: step 155180, loss = 0.26 (8146.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:55.347061: step 155190, loss = 0.20 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:55.664547: step 155200, loss = 0.21 (8172.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:56.120286: step 155210, loss = 0.29 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:56.440381: step 155220, loss = 0.21 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:56.760681: step 155230, loss = 0.27 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:57.080545: step 155240, loss = 0.19 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:57.401915: step 155250, loss = 0.23 (7412.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:57.722136: step 155260, loss = 0.26 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:58.042595: step 155270, loss = 0.23 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:58.362415: step 155280, loss = 0.26 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:58.679583: step 155290, loss = 0.31 (8052.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:58.999259: step 155300, loss = 0.23 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:06:59.460290: step 155310, loss = 0.26 (7614.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:06:59.782589: step 155320, loss = 0.25 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:00.103800: step 155330, loss = 0.21 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:00.423432: step 155340, loss = 0.23 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:00.742819: step 155350, loss = 0.19 (8235.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:01.062648: step 155360, loss = 0.22 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:01.380814: step 155370, loss = 0.21 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:01.698804: step 155380, loss = 0.28 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:02.018309: step 155390, loss = 0.18 (7875.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:02.338161: step 155400, loss = 0.23 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:02.790372: step 155410, loss = 0.19 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:03.110395: step 155420, loss = 0.21 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:03.429618: step 155430, loss = 0.24 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:03.751383: step 155440, loss = 0.22 (7753.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:04.070851: step 155450, loss = 0.24 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:04.393690: step 155460, loss = 0.19 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:04.712967: step 155470, loss = 0.26 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:05.030551: step 155480, loss = 0.23 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:05.349276: step 155490, loss = 0.24 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:05.669172: step 155500, loss = 0.25 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:06.118033: step 155510, loss = 0.24 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:06.438054: step 155520, loss = 0.21 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:06.764186: step 155530, loss = 0.18 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:07.085124: step 155540, loss = 0.30 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:07.404929: step 155550, loss = 0.25 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:07.722988: step 155560, loss = 0.34 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:08.042534: step 155570, loss = 0.27 (7833.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:08.364126: step 155580, loss = 0.16 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:08.684582: step 155590, loss = 0.22 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:09.005431: step 155600, loss = 0.28 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:09.464393: step 155610, loss = 0.22 (8120.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:09.784984: step 155620, loss = 0.25 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:10.103717: step 155630, loss = 0.24 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:10.423227: step 155640, loss = 0.27 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:10.743580: step 155650, loss = 0.23 (7908.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:11.065257: step 155660, loss = 0.24 (7784.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:11.385378: step 155670, loss = 0.24 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:11.709516: step 155680, loss = 0.24 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:12.027953: step 155690, loss = 0.26 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:12.353549: step 155700, loss = 0.25 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:12.814436: step 155710, loss = 0.22 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:13.136492: step 155720, loss = 0.26 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:13.455477: step 155730, loss = 0.15 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:13.782172: step 155740, loss = 0.20 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:14.102631: step 155750, loss = 0.26 (7624.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:14.424744: step 155760, loss = 0.29 (7859.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:14.746431: step 155770, loss = 0.25 (7765.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:15.065739: step 155780, loss = 0.26 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:15.385354: step 155790, loss = 0.31 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:15.704132: step 155800, loss = 0.22 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:16.165302: step 155810, loss = 0.27 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:16.485623: step 155820, loss = 0.24 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:16.807546: step 155830, loss = 0.27 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:17.126609: step 155840, loss = 0.18 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:17.447593: step 155850, loss = 0.27 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:17.768658: step 155860, loss = 0.20 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:18.087237: step 155870, loss = 0.19 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:18.406201: step 155880, loss = 0.24 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:18.727064: step 155890, loss = 0.18 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:19.049025: step 155900, loss = 0.24 (7876.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:19.512288: step 155910, loss = 0.29 (7663.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:19.832379: step 155920, loss = 0.24 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:20.153719: step 155930, loss = 0.24 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:20.478501: step 155940, loss = 0.21 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:20.799697: step 155950, loss = 0.28 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:21.119593: step 155960, loss = 0.21 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:21.438077: step 155970, loss = 0.25 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:21.757938: step 155980, loss = 0.22 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:22.076790: step 155990, loss = 0.23 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:22.397263: step 156000, loss = 0.25 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:22.982957: step 156010, loss = 0.22 (7361.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:23.308341: step 156020, loss = 0.23 (7423.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:23.628664: step 156030, loss = 0.21 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:23.948801: step 156040, loss = 0.23 (7752.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:24.267553: step 156050, loss = 0.30 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:24.586339: step 156060, loss = 0.20 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:24.908932: step 156070, loss = 0.19 (7627.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:25.229791: step 156080, loss = 0.29 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:25.550570: step 156090, loss = 0.19 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:25.869394: step 156100, loss = 0.28 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:26.336211: step 156110, loss = 0.26 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:26.658198: step 156120, loss = 0.24 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:26.979822: step 156130, loss = 0.27 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:27.302249: step 156140, loss = 0.25 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:27.621239: step 156150, loss = 0.24 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:27.940281: step 156160, loss = 0.21 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:28.259294: step 156170, loss = 0.26 (8153.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:28.580790: step 156180, loss = 0.24 (8148.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:28.900112: step 156190, loss = 0.24 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:29.219903: step 156200, loss = 0.25 (7799.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:29.662835: step 156210, loss = 0.24 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:29.983821: step 156220, loss = 0.19 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:30.304072: step 156230, loss = 0.29 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:30.621429: step 156240, loss = 0.21 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:30.941753: step 156250, loss = 0.18 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:31.261051: step 156260, loss = 0.25 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:31.579518: step 156270, loss = 0.22 (8123.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:31.901679: step 156280, loss = 0.19 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:32.221794: step 156290, loss = 0.18 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:32.542922: step 156300, loss = 0.17 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:32.993614: step 156310, loss = 0.23 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:33.312331: step 156320, loss = 0.23 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:33.636935: step 156330, loss = 0.25 (7818.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:33.955394: step 156340, loss = 0.24 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:34.276071: step 156350, loss = 0.20 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:34.596688: step 156360, loss = 0.33 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:34.917541: step 156370, loss = 0.22 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:35.238134: step 156380, loss = 0.27 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:35.561760: step 156390, loss = 0.24 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:35.882015: step 156400, loss = 0.25 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:36.352876: step 156410, loss = 0.25 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:36.672969: step 156420, loss = 0.20 (8096.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:36.997194: step 156430, loss = 0.22 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:37.317459: step 156440, loss = 0.24 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:37.640368: step 156450, loss = 0.24 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:37.958999: step 156460, loss = 0.24 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:38.282932: step 156470, loss = 0.27 (7673.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:38.605366: step 156480, loss = 0.22 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:38.924419: step 156490, loss = 0.21 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:39.244746: step 156500, loss = 0.22 (7926.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:39.700758: step 156510, loss = 0.19 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:40.020848: step 156520, loss = 0.27 (8067.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:40.340784: step 156530, loss = 0.27 (8170.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:40.660259: step 156540, loss = 0.21 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:40.981599: step 156550, loss = 0.28 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:41.302193: step 156560, loss = 0.22 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:41.625899: step 156570, loss = 0.28 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:41.947189: step 156580, loss = 0.28 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:42.267199: step 156590, loss = 0.23 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:42.585803: step 156600, loss = 0.21 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:43.044640: step 156610, loss = 0.21 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:43.364900: step 156620, loss = 0.25 (7903.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:43.686554: step 156630, loss = 0.24 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:44.007946: step 156640, loss = 0.18 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:44.328020: step 156650, loss = 0.31 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:44.647833: step 156660, loss = 0.18 (7907.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:44.967161: step 156670, loss = 0.24 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:45.285042: step 156680, loss = 0.26 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:45.605054: step 156690, loss = 0.18 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:45.926019: step 156700, loss = 0.21 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:46.383319: step 156710, loss = 0.21 (8151.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:46.703926: step 156720, loss = 0.26 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:47.022892: step 156730, loss = 0.20 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:47.344104: step 156740, loss = 0.26 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:47.664452: step 156750, loss = 0.21 (7892.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:47.984099: step 156760, loss = 0.26 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:48.306692: step 156770, loss = 0.27 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:48.626672: step 156780, loss = 0.22 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:48.946735: step 156790, loss = 0.25 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:49.266492: step 156800, loss = 0.22 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:49.721571: step 156810, loss = 0.22 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:50.042348: step 156820, loss = 0.27 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:50.365021: step 156830, loss = 0.28 (7880.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:50.686284: step 156840, loss = 0.27 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:51.004897: step 156850, loss = 0.20 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:51.323119: step 156860, loss = 0.25 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:51.643739: step 156870, loss = 0.21 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:51.969954: step 156880, loss = 0.22 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:52.289699: step 156890, loss = 0.25 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:52.609992: step 156900, loss = 0.26 (7457.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:07:53.079492: step 156910, loss = 0.20 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:53.398656: step 156920, loss = 0.31 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:53.718989: step 156930, loss = 0.26 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:54.040455: step 156940, loss = 0.27 (7888.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:54.359326: step 156950, loss = 0.22 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:54.680273: step 156960, loss = 0.22 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:54.999333: step 156970, loss = 0.19 (8145.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:55.317314: step 156980, loss = 0.23 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:55.637040: step 156990, loss = 0.22 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:55.956441: step 157000, loss = 0.20 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:56.516644: step 157010, loss = 0.33 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:56.837510: step 157020, loss = 0.25 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:57.158617: step 157030, loss = 0.26 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:57.478875: step 157040, loss = 0.29 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:57.800051: step 157050, loss = 0.26 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:58.127856: step 157060, loss = 0.22 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:58.457260: step 157070, loss = 0.19 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:58.780443: step 157080, loss = 0.19 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:59.100907: step 157090, loss = 0.26 (7839.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:59.418844: step 157100, loss = 0.25 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:07:59.883872: step 157110, loss = 0.21 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:00.203255: step 157120, loss = 0.33 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:00.523783: step 157130, loss = 0.21 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:00.840672: step 157140, loss = 0.22 (8176.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:01.160080: step 157150, loss = 0.20 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:01.480673: step 157160, loss = 0.30 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:01.800666: step 157170, loss = 0.26 (7840.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:02.121329: step 157180, loss = 0.22 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:02.440171: step 157190, loss = 0.19 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:02.761428: step 157200, loss = 0.27 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:03.213048: step 157210, loss = 0.28 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:03.531393: step 157220, loss = 0.23 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:03.850895: step 157230, loss = 0.26 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:04.172880: step 157240, loss = 0.18 (7931.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:04.492548: step 157250, loss = 0.25 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:04.812504: step 157260, loss = 0.22 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:05.130499: step 157270, loss = 0.22 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:05.449750: step 157280, loss = 0.20 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:05.768238: step 157290, loss = 0.24 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:06.088275: step 157300, loss = 0.26 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:06.542361: step 157310, loss = 0.25 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:06.861881: step 157320, loss = 0.20 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:07.183564: step 157330, loss = 0.30 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:07.506487: step 157340, loss = 0.29 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:07.826332: step 157350, loss = 0.21 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:08.147320: step 157360, loss = 0.18 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:08.467332: step 157370, loss = 0.20 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:08.786993: step 157380, loss = 0.25 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:09.107577: step 157390, loss = 0.32 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:09.427586: step 157400, loss = 0.21 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:09.883991: step 157410, loss = 0.24 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:10.206023: step 157420, loss = 0.22 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:10.525838: step 157430, loss = 0.20 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:10.848808: step 157440, loss = 0.26 (7897.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:11.169996: step 157450, loss = 0.19 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:11.490043: step 157460, loss = 0.20 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:11.810148: step 157470, loss = 0.22 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:12.129870: step 157480, loss = 0.24 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:12.452825: step 157490, loss = 0.29 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:12.774441: step 157500, loss = 0.23 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:13.232792: step 157510, loss = 0.31 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:13.555210: step 157520, loss = 0.19 (7809.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:13.877284: step 157530, loss = 0.23 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:14.197442: step 157540, loss = 0.20 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:14.519323: step 157550, loss = 0.23 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:14.838347: step 157560, loss = 0.17 (7872.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:15.159661: step 157570, loss = 0.24 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:15.480360: step 157580, loss = 0.21 (7810.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:15.801401: step 157590, loss = 0.20 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:16.120479: step 157600, loss = 0.24 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:16.594307: step 157610, loss = 0.26 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:16.915646: step 157620, loss = 0.25 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:17.234467: step 157630, loss = 0.26 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:17.552222: step 157640, loss = 0.28 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:17.872353: step 157650, loss = 0.21 (8128.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:18.191279: step 157660, loss = 0.26 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:18.510564: step 157670, loss = 0.18 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:18.829677: step 157680, loss = 0.19 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:19.148804: step 157690, loss = 0.32 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:19.468583: step 157700, loss = 0.26 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:19.918021: step 157710, loss = 0.26 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:20.239933: step 157720, loss = 0.15 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:20.558632: step 157730, loss = 0.20 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:20.880045: step 157740, loss = 0.20 (7829.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:21.201851: step 157750, loss = 0.22 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:21.522387: step 157760, loss = 0.30 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:21.840685: step 157770, loss = 0.29 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:22.161423: step 157780, loss = 0.19 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:22.481527: step 157790, loss = 0.25 (8143.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:22.800226: step 157800, loss = 0.28 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:23.260223: step 157810, loss = 0.31 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:23.582652: step 157820, loss = 0.18 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:23.902405: step 157830, loss = 0.27 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:24.224121: step 157840, loss = 0.21 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:24.543910: step 157850, loss = 0.18 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:24.864549: step 157860, loss = 0.25 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:25.183856: step 157870, loss = 0.20 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:25.504368: step 157880, loss = 0.22 (7830.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:25.823319: step 157890, loss = 0.24 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:26.143181: step 157900, loss = 0.24 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:26.593953: step 157910, loss = 0.23 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:26.914515: step 157920, loss = 0.23 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:27.232668: step 157930, loss = 0.23 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:27.553795: step 157940, loss = 0.20 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:27.872593: step 157950, loss = 0.30 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:28.192789: step 157960, loss = 0.22 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:28.515631: step 157970, loss = 0.25 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:28.838912: step 157980, loss = 0.25 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:29.158842: step 157990, loss = 0.27 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:29.481954: step 158000, loss = 0.18 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:30.035899: step 158010, loss = 0.32 (7749.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:08:30.356704: step 158020, loss = 0.30 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:30.675793: step 158030, loss = 0.25 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:30.997684: step 158040, loss = 0.22 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:31.319005: step 158050, loss = 0.21 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:31.640655: step 158060, loss = 0.24 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:31.961472: step 158070, loss = 0.28 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:32.280417: step 158080, loss = 0.19 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:32.598835: step 158090, loss = 0.22 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:32.919620: step 158100, loss = 0.25 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:33.382028: step 158110, loss = 0.23 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:33.700812: step 158120, loss = 0.22 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:34.019615: step 158130, loss = 0.20 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:34.338026: step 158140, loss = 0.20 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:34.661351: step 158150, loss = 0.21 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:34.985997: step 158160, loss = 0.22 (7406.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:08:35.306201: step 158170, loss = 0.25 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:35.625502: step 158180, loss = 0.20 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:35.945993: step 158190, loss = 0.18 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:36.266049: step 158200, loss = 0.22 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:36.713052: step 158210, loss = 0.18 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:37.032667: step 158220, loss = 0.20 (7925.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:37.353974: step 158230, loss = 0.20 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:37.674585: step 158240, loss = 0.22 (8082.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:37.993608: step 158250, loss = 0.18 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:38.314356: step 158260, loss = 0.24 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:38.634408: step 158270, loss = 0.18 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:38.955904: step 158280, loss = 0.22 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:39.276396: step 158290, loss = 0.24 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:39.594902: step 158300, loss = 0.22 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:40.057536: step 158310, loss = 0.18 (7820.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:40.376507: step 158320, loss = 0.24 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:40.695223: step 158330, loss = 0.22 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:41.014549: step 158340, loss = 0.17 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:41.336842: step 158350, loss = 0.22 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:41.655969: step 158360, loss = 0.25 (8147.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:41.977014: step 158370, loss = 0.23 (7743.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:08:42.296637: step 158380, loss = 0.21 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:42.616099: step 158390, loss = 0.19 (7911.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:42.935574: step 158400, loss = 0.22 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:43.398121: step 158410, loss = 0.22 (8155.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:43.718432: step 158420, loss = 0.19 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:44.040896: step 158430, loss = 0.26 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:44.362264: step 158440, loss = 0.27 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:44.684504: step 158450, loss = 0.28 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:45.003045: step 158460, loss = 0.17 (8034.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:45.323307: step 158470, loss = 0.25 (7905.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:45.642896: step 158480, loss = 0.20 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:45.962736: step 158490, loss = 0.19 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:46.281289: step 158500, loss = 0.26 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:46.743415: step 158510, loss = 0.23 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:47.063119: step 158520, loss = 0.23 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:47.382286: step 158530, loss = 0.24 (8091.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:47.703819: step 158540, loss = 0.22 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:48.022223: step 158550, loss = 0.18 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:48.341145: step 158560, loss = 0.22 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:48.661595: step 158570, loss = 0.24 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:48.980788: step 158580, loss = 0.21 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:49.300821: step 158590, loss = 0.25 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:49.620100: step 158600, loss = 0.20 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:50.071283: step 158610, loss = 0.32 (7946.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:50.396835: step 158620, loss = 0.23 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:50.716831: step 158630, loss = 0.33 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:51.038856: step 158640, loss = 0.25 (8141.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:51.360175: step 158650, loss = 0.24 (7834.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:51.678581: step 158660, loss = 0.17 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:51.997777: step 158670, loss = 0.20 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:52.322135: step 158680, loss = 0.20 (7789.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:52.642227: step 158690, loss = 0.19 (7819.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:52.961340: step 158700, loss = 0.19 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:53.414656: step 158710, loss = 0.23 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:53.736899: step 158720, loss = 0.24 (7853.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:54.055660: step 158730, loss = 0.22 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:54.376878: step 158740, loss = 0.22 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:54.697207: step 158750, loss = 0.25 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:55.016667: step 158760, loss = 0.15 (7824.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:55.338943: step 158770, loss = 0.21 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:55.657626: step 158780, loss = 0.21 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:55.975875: step 158790, loss = 0.22 (8137.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:56.296685: step 158800, loss = 0.22 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:56.751686: step 158810, loss = 0.18 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:57.071873: step 158820, loss = 0.17 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:57.390925: step 158830, loss = 0.24 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:57.711541: step 158840, loss = 0.24 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:58.029711: step 158850, loss = 0.22 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:58.349427: step 158860, loss = 0.23 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:58.669549: step 158870, loss = 0.21 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:58.991026: step 158880, loss = 0.22 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:59.309385: step 158890, loss = 0.27 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:08:59.628330: step 158900, loss = 0.24 (8075.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:00.090504: step 158910, loss = 0.20 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:00.409271: step 158920, loss = 0.22 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:00.729323: step 158930, loss = 0.21 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:01.048595: step 158940, loss = 0.26 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:01.370998: step 158950, loss = 0.26 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:01.693303: step 158960, loss = 0.23 (7856.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:02.014050: step 158970, loss = 0.20 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:02.334160: step 158980, loss = 0.24 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:02.653685: step 158990, loss = 0.21 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:02.974457: step 159000, loss = 0.14 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:03.537439: step 159010, loss = 0.22 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:03.856460: step 159020, loss = 0.30 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:04.175546: step 159030, loss = 0.21 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:04.494207: step 159040, loss = 0.24 (7778.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:04.814025: step 159050, loss = 0.23 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:05.135599: step 159060, loss = 0.25 (7559.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:05.456949: step 159070, loss = 0.19 (7669.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:05.776082: step 159080, loss = 0.24 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:06.096753: step 159090, loss = 0.33 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:06.417076: step 159100, loss = 0.23 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:06.870689: step 159110, loss = 0.25 (7753.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:07.191274: step 159120, loss = 0.18 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:07.510353: step 159130, loss = 0.27 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:07.831023: step 159140, loss = 0.21 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:08.152047: step 159150, loss = 0.24 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:08.471041: step 159160, loss = 0.25 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:08.793421: step 159170, loss = 0.20 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:09.113484: step 159180, loss = 0.21 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:09.432434: step 159190, loss = 0.19 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:09.751623: step 159200, loss = 0.30 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:10.208329: step 159210, loss = 0.19 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:10.527991: step 159220, loss = 0.27 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:10.845684: step 159230, loss = 0.20 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:11.164787: step 159240, loss = 0.19 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:11.483930: step 159250, loss = 0.25 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:11.803190: step 159260, loss = 0.26 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:12.121665: step 159270, loss = 0.20 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:12.441332: step 159280, loss = 0.20 (8014.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:12.760717: step 159290, loss = 0.20 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:13.080295: step 159300, loss = 0.23 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:13.532257: step 159310, loss = 0.19 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:13.852712: step 159320, loss = 0.31 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:14.171253: step 159330, loss = 0.20 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:14.490108: step 159340, loss = 0.21 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:14.809686: step 159350, loss = 0.18 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:15.128325: step 159360, loss = 0.28 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:15.448236: step 159370, loss = 0.22 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:15.769976: step 159380, loss = 0.31 (7623.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:16.087565: step 159390, loss = 0.22 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:16.408116: step 159400, loss = 0.18 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:16.866590: step 159410, loss = 0.18 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:17.188530: step 159420, loss = 0.22 (7490.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:17.508947: step 159430, loss = 0.22 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:17.828598: step 159440, loss = 0.19 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:18.147675: step 159450, loss = 0.24 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:18.468799: step 159460, loss = 0.22 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:18.787916: step 159470, loss = 0.19 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:19.106215: step 159480, loss = 0.21 (8087.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:19.426639: step 159490, loss = 0.24 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:19.746922: step 159500, loss = 0.16 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:20.195356: step 159510, loss = 0.24 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:20.516242: step 159520, loss = 0.24 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:20.836722: step 159530, loss = 0.21 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:21.157870: step 159540, loss = 0.20 (8056.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:21.477039: step 159550, loss = 0.22 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:21.796245: step 159560, loss = 0.26 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:22.115938: step 159570, loss = 0.19 (7883.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:22.441809: step 159580, loss = 0.20 (7409.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:22.761761: step 159590, loss = 0.31 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:23.084177: step 159600, loss = 0.21 (7723.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:23.554508: step 159610, loss = 0.24 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:23.877147: step 159620, loss = 0.19 (7576.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:24.196125: step 159630, loss = 0.18 (8049.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:24.520911: step 159640, loss = 0.23 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:24.840917: step 159650, loss = 0.24 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:25.159489: step 159660, loss = 0.21 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:25.479966: step 159670, loss = 0.23 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:25.799952: step 159680, loss = 0.23 (8005.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:26.124368: step 159690, loss = 0.25 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:26.445608: step 159700, loss = 0.23 (7382.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:26.899601: step 159710, loss = 0.20 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:27.219984: step 159720, loss = 0.17 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:27.540086: step 159730, loss = 0.21 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:27.863245: step 159740, loss = 0.22 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:28.186838: step 159750, loss = 0.20 (7868.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:28.507113: step 159760, loss = 0.27 (7838.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:28.828516: step 159770, loss = 0.18 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:29.148714: step 159780, loss = 0.27 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:29.467450: step 159790, loss = 0.35 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:29.788340: step 159800, loss = 0.21 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:30.252522: step 159810, loss = 0.17 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:30.572753: step 159820, loss = 0.23 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:30.893137: step 159830, loss = 0.27 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:31.213000: step 159840, loss = 0.23 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:31.533397: step 159850, loss = 0.20 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:31.852279: step 159860, loss = 0.22 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:32.172184: step 159870, loss = 0.24 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:32.491769: step 159880, loss = 0.18 (8027.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:32.809252: step 159890, loss = 0.24 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:33.128982: step 159900, loss = 0.22 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:33.593484: step 159910, loss = 0.21 (7792.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:33.914885: step 159920, loss = 0.28 (7835.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:34.234269: step 159930, loss = 0.19 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:34.554991: step 159940, loss = 0.22 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:34.873012: step 159950, loss = 0.18 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:35.194539: step 159960, loss = 0.28 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:35.517915: step 159970, loss = 0.22 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:35.838983: step 159980, loss = 0.25 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:36.158652: step 159990, loss = 0.17 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:36.479756: step 160000, loss = 0.18 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:37.037349: step 160010, loss = 0.24 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:37.365991: step 160020, loss = 0.22 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:37.686874: step 160030, loss = 0.28 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:38.006913: step 160040, loss = 0.19 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:38.327295: step 160050, loss = 0.24 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:38.646906: step 160060, loss = 0.22 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:38.967276: step 160070, loss = 0.24 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:39.287013: step 160080, loss = 0.32 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:39.607974: step 160090, loss = 0.20 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:39.928158: step 160100, loss = 0.23 (7844.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:40.390532: step 160110, loss = 0.27 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:40.711191: step 160120, loss = 0.17 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:41.030877: step 160130, loss = 0.24 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:41.349307: step 160140, loss = 0.19 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:41.670070: step 160150, loss = 0.21 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:41.991538: step 160160, loss = 0.20 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:42.314451: step 160170, loss = 0.18 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:42.637272: step 160180, loss = 0.20 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:42.957671: step 160190, loss = 0.19 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:43.278064: step 160200, loss = 0.22 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:43.733220: step 160210, loss = 0.18 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:44.052546: step 160220, loss = 0.28 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:44.371999: step 160230, loss = 0.21 (8161.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:44.695145: step 160240, loss = 0.24 (7771.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:45.017150: step 160250, loss = 0.24 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:45.337601: step 160260, loss = 0.28 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:45.656171: step 160270, loss = 0.20 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:45.976071: step 160280, loss = 0.23 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:46.294078: step 160290, loss = 0.20 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:46.614159: step 160300, loss = 0.29 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:47.064424: step 160310, loss = 0.26 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:47.382967: step 160320, loss = 0.18 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:47.708556: step 160330, loss = 0.25 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:48.029385: step 160340, loss = 0.18 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:48.350877: step 160350, loss = 0.22 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:48.670865: step 160360, loss = 0.22 (7848.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:48.991867: step 160370, loss = 0.24 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:49.312969: step 160380, loss = 0.26 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:49.633680: step 160390, loss = 0.19 (7685.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:49.953668: step 160400, loss = 0.24 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:50.412385: step 160410, loss = 0.24 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:50.735913: step 160420, loss = 0.22 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:51.055177: step 160430, loss = 0.22 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:51.374478: step 160440, loss = 0.22 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:51.696600: step 160450, loss = 0.19 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:52.018625: step 160460, loss = 0.19 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:52.338656: step 160470, loss = 0.22 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:52.657005: step 160480, loss = 0.23 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:52.980016: step 160490, loss = 0.27 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:53.299081: step 160500, loss = 0.17 (8058.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:53.760807: step 160510, loss = 0.20 (8002.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:54.079704: step 160520, loss = 0.22 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:54.402068: step 160530, loss = 0.18 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:54.722491: step 160540, loss = 0.20 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:55.044380: step 160550, loss = 0.20 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:55.363542: step 160560, loss = 0.22 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:55.682496: step 160570, loss = 0.17 (8140.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:56.005707: step 160580, loss = 0.20 (7434.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:09:56.328437: step 160590, loss = 0.23 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:56.653037: step 160600, loss = 0.27 (7790.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:57.102977: step 160610, loss = 0.22 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:57.423693: step 160620, loss = 0.28 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:57.744436: step 160630, loss = 0.14 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:58.063891: step 160640, loss = 0.20 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:58.384786: step 160650, loss = 0.25 (7966.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:58.706813: step 160660, loss = 0.22 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:59.028371: step 160670, loss = 0.20 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:59.348316: step 160680, loss = 0.22 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:59.668119: step 160690, loss = 0.26 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:09:59.989636: step 160700, loss = 0.23 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:00.450913: step 160710, loss = 0.17 (8115.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:00.769329: step 160720, loss = 0.23 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:01.089816: step 160730, loss = 0.20 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:01.409330: step 160740, loss = 0.21 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:01.729990: step 160750, loss = 0.26 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:02.052284: step 160760, loss = 0.23 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:02.372680: step 160770, loss = 0.25 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:02.691154: step 160780, loss = 0.19 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:03.009711: step 160790, loss = 0.27 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:03.329664: step 160800, loss = 0.18 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:03.779729: step 160810, loss = 0.23 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:04.098629: step 160820, loss = 0.26 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:04.419883: step 160830, loss = 0.27 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:04.740635: step 160840, loss = 0.25 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:05.060815: step 160850, loss = 0.18 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:05.382346: step 160860, loss = 0.25 (7869.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:05.701318: step 160870, loss = 0.21 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:06.021088: step 160880, loss = 0.19 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:06.342045: step 160890, loss = 0.21 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:06.662536: step 160900, loss = 0.17 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:07.114671: step 160910, loss = 0.19 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:07.438978: step 160920, loss = 0.17 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:07.763423: step 160930, loss = 0.21 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:08.084627: step 160940, loss = 0.31 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:08.403855: step 160950, loss = 0.21 (7973.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:08.722508: step 160960, loss = 0.18 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:09.043708: step 160970, loss = 0.29 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:09.368402: step 160980, loss = 0.19 (7397.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:09.688831: step 160990, loss = 0.26 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:10.007188: step 161000, loss = 0.25 (8061.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:10.563537: step 161010, loss = 0.18 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:10.887143: step 161020, loss = 0.25 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:11.208813: step 161030, loss = 0.29 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:11.528925: step 161040, loss = 0.15 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:11.849995: step 161050, loss = 0.18 (7570.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:12.169244: step 161060, loss = 0.25 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:12.489897: step 161070, loss = 0.19 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:12.810329: step 161080, loss = 0.18 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:13.130617: step 161090, loss = 0.18 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:13.451848: step 161100, loss = 0.21 (7790.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:13.912863: step 161110, loss = 0.21 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:14.234726: step 161120, loss = 0.25 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:14.562972: step 161130, loss = 0.23 (7252.8 examples/sec; 0.018 sec/batch)
2017-09-16 17:10:14.883165: step 161140, loss = 0.21 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:15.202480: step 161150, loss = 0.22 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:15.523383: step 161160, loss = 0.25 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:15.844041: step 161170, loss = 0.29 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:16.164254: step 161180, loss = 0.16 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:16.485000: step 161190, loss = 0.21 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:16.803223: step 161200, loss = 0.20 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:17.271048: step 161210, loss = 0.20 (7996.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:17.589386: step 161220, loss = 0.18 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:17.909848: step 161230, loss = 0.21 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:18.230406: step 161240, loss = 0.22 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:18.553477: step 161250, loss = 0.19 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:18.873829: step 161260, loss = 0.17 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:19.193884: step 161270, loss = 0.19 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:19.512831: step 161280, loss = 0.20 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:19.833592: step 161290, loss = 0.26 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:20.156225: step 161300, loss = 0.21 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:20.621562: step 161310, loss = 0.23 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:20.940991: step 161320, loss = 0.18 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:21.263443: step 161330, loss = 0.19 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:21.582634: step 161340, loss = 0.27 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:21.905063: step 161350, loss = 0.17 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:22.228048: step 161360, loss = 0.23 (7475.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:22.549060: step 161370, loss = 0.21 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:22.870097: step 161380, loss = 0.22 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:23.192114: step 161390, loss = 0.30 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:23.511517: step 161400, loss = 0.21 (7992.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:23.977155: step 161410, loss = 0.25 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:24.299075: step 161420, loss = 0.17 (7827.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:24.624878: step 161430, loss = 0.24 (8136.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:24.945879: step 161440, loss = 0.27 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:25.269350: step 161450, loss = 0.20 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:25.595737: step 161460, loss = 0.37 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:25.916059: step 161470, loss = 0.20 (8148.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:26.236794: step 161480, loss = 0.22 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:26.557183: step 161490, loss = 0.22 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:26.876434: step 161500, loss = 0.27 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:27.342049: step 161510, loss = 0.19 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:27.662565: step 161520, loss = 0.20 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:27.985953: step 161530, loss = 0.21 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:28.307466: step 161540, loss = 0.17 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:28.627275: step 161550, loss = 0.17 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:28.950039: step 161560, loss = 0.27 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:29.269076: step 161570, loss = 0.20 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:29.588064: step 161580, loss = 0.21 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:29.907548: step 161590, loss = 0.21 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:30.229047: step 161600, loss = 0.22 (7878.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:30.684291: step 161610, loss = 0.24 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:31.005143: step 161620, loss = 0.21 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:31.327250: step 161630, loss = 0.19 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:31.651284: step 161640, loss = 0.28 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:31.972487: step 161650, loss = 0.26 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:32.292059: step 161660, loss = 0.23 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:32.616796: step 161670, loss = 0.24 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:32.938017: step 161680, loss = 0.26 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:33.257900: step 161690, loss = 0.17 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:33.578303: step 161700, loss = 0.27 (8127.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:34.036431: step 161710, loss = 0.23 (8018.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:34.355111: step 161720, loss = 0.22 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:34.680848: step 161730, loss = 0.23 (7370.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:35.000139: step 161740, loss = 0.24 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:35.318524: step 161750, loss = 0.18 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:35.637297: step 161760, loss = 0.19 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:35.955458: step 161770, loss = 0.19 (8161.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:36.274085: step 161780, loss = 0.27 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:36.593989: step 161790, loss = 0.22 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:36.913423: step 161800, loss = 0.22 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:37.361731: step 161810, loss = 0.32 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:37.680805: step 161820, loss = 0.28 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:38.001557: step 161830, loss = 0.22 (7871.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:38.322033: step 161840, loss = 0.25 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:38.642179: step 161850, loss = 0.22 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:38.960275: step 161860, loss = 0.22 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:39.279150: step 161870, loss = 0.18 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:39.598198: step 161880, loss = 0.18 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:39.918603: step 161890, loss = 0.26 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:40.241091: step 161900, loss = 0.25 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:40.693681: step 161910, loss = 0.26 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:41.013995: step 161920, loss = 0.19 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:41.333797: step 161930, loss = 0.20 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:41.655143: step 161940, loss = 0.26 (7913.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:41.974290: step 161950, loss = 0.17 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:42.295337: step 161960, loss = 0.25 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:42.615374: step 161970, loss = 0.28 (8108.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:42.934495: step 161980, loss = 0.21 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:43.254690: step 161990, loss = 0.21 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:43.575304: step 162000, loss = 0.23 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:44.135195: step 162010, loss = 0.21 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:44.454306: step 162020, loss = 0.28 (8152.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:44.773140: step 162030, loss = 0.20 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:45.093945: step 162040, loss = 0.16 (7837.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:45.417605: step 162050, loss = 0.18 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:45.738296: step 162060, loss = 0.19 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:46.058657: step 162070, loss = 0.16 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:46.380561: step 162080, loss = 0.25 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:46.699401: step 162090, loss = 0.23 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:47.020590: step 162100, loss = 0.19 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:47.472620: step 162110, loss = 0.19 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:47.794121: step 162120, loss = 0.22 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:48.114075: step 162130, loss = 0.22 (7565.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:48.433846: step 162140, loss = 0.17 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:48.753038: step 162150, loss = 0.27 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:49.073469: step 162160, loss = 0.26 (7803.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:49.395397: step 162170, loss = 0.16 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:49.715379: step 162180, loss = 0.20 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:50.034082: step 162190, loss = 0.28 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:50.353614: step 162200, loss = 0.21 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:50.814166: step 162210, loss = 0.24 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:51.133162: step 162220, loss = 0.20 (8160.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:51.452691: step 162230, loss = 0.23 (7809.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:51.771561: step 162240, loss = 0.22 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:52.097695: step 162250, loss = 0.21 (7372.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:10:52.418216: step 162260, loss = 0.20 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:52.739290: step 162270, loss = 0.28 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:53.061029: step 162280, loss = 0.21 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:53.379095: step 162290, loss = 0.19 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:53.699420: step 162300, loss = 0.24 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:54.155158: step 162310, loss = 0.20 (8069.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:54.474812: step 162320, loss = 0.21 (7961.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:54.797606: step 162330, loss = 0.22 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:55.120535: step 162340, loss = 0.27 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:55.444719: step 162350, loss = 0.19 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:55.766236: step 162360, loss = 0.26 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:56.086041: step 162370, loss = 0.17 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:56.408608: step 162380, loss = 0.23 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:56.729551: step 162390, loss = 0.19 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:57.049457: step 162400, loss = 0.17 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:57.499915: step 162410, loss = 0.22 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:57.820323: step 162420, loss = 0.28 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:58.140555: step 162430, loss = 0.25 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:58.465945: step 162440, loss = 0.19 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:58.792843: step 162450, loss = 0.23 (7816.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:59.111936: step 162460, loss = 0.22 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:59.431168: step 162470, loss = 0.23 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:10:59.749922: step 162480, loss = 0.15 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:00.070283: step 162490, loss = 0.21 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:00.390892: step 162500, loss = 0.23 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:00.852619: step 162510, loss = 0.25 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:01.176865: step 162520, loss = 0.29 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:01.497646: step 162530, loss = 0.27 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:01.817826: step 162540, loss = 0.23 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:02.139766: step 162550, loss = 0.24 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:02.460245: step 162560, loss = 0.20 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:02.781094: step 162570, loss = 0.27 (7858.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:03.102333: step 162580, loss = 0.29 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:03.422782: step 162590, loss = 0.19 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:03.741449: step 162600, loss = 0.19 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:04.205397: step 162610, loss = 0.22 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:04.526410: step 162620, loss = 0.26 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:04.846300: step 162630, loss = 0.29 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:05.166171: step 162640, loss = 0.19 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:05.487876: step 162650, loss = 0.26 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:05.807837: step 162660, loss = 0.24 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:06.130054: step 162670, loss = 0.28 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:06.450788: step 162680, loss = 0.27 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:06.770422: step 162690, loss = 0.24 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:07.090489: step 162700, loss = 0.19 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:07.549583: step 162710, loss = 0.21 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:07.871501: step 162720, loss = 0.24 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:08.193802: step 162730, loss = 0.23 (7531.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:08.514586: step 162740, loss = 0.23 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:08.835522: step 162750, loss = 0.25 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:09.157418: step 162760, loss = 0.14 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:09.476869: step 162770, loss = 0.29 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:09.797196: step 162780, loss = 0.18 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:10.118927: step 162790, loss = 0.20 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:10.440788: step 162800, loss = 0.23 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:10.906287: step 162810, loss = 0.21 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:11.229560: step 162820, loss = 0.23 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:11.550361: step 162830, loss = 0.19 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:11.871010: step 162840, loss = 0.27 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:12.192546: step 162850, loss = 0.25 (7876.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:12.514180: step 162860, loss = 0.23 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:12.837772: step 162870, loss = 0.27 (7760.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:13.156393: step 162880, loss = 0.19 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:13.475935: step 162890, loss = 0.25 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:13.795583: step 162900, loss = 0.14 (8103.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:14.246196: step 162910, loss = 0.18 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:14.565949: step 162920, loss = 0.26 (7833.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:14.887552: step 162930, loss = 0.25 (7830.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:15.212082: step 162940, loss = 0.17 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:15.533169: step 162950, loss = 0.16 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:15.857761: step 162960, loss = 0.23 (7176.3 examples/sec; 0.018 sec/batch)
2017-09-16 17:11:16.180352: step 162970, loss = 0.20 (7686.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:16.504586: step 162980, loss = 0.22 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:16.823799: step 162990, loss = 0.17 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:17.143964: step 163000, loss = 0.20 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:17.745192: step 163010, loss = 0.17 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:18.065281: step 163020, loss = 0.21 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:18.386159: step 163030, loss = 0.19 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:18.707456: step 163040, loss = 0.19 (7904.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:19.030689: step 163050, loss = 0.17 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:19.352718: step 163060, loss = 0.20 (7588.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:19.671970: step 163070, loss = 0.20 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:19.992515: step 163080, loss = 0.20 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:20.312558: step 163090, loss = 0.20 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:20.633815: step 163100, loss = 0.18 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:21.094755: step 163110, loss = 0.24 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:21.412461: step 163120, loss = 0.23 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:21.734151: step 163130, loss = 0.30 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:22.053496: step 163140, loss = 0.23 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:22.375141: step 163150, loss = 0.22 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:22.693683: step 163160, loss = 0.20 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:23.016362: step 163170, loss = 0.24 (7655.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:23.339664: step 163180, loss = 0.24 (7809.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:23.660188: step 163190, loss = 0.20 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:23.978690: step 163200, loss = 0.19 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:24.436186: step 163210, loss = 0.22 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:24.754861: step 163220, loss = 0.25 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:25.075547: step 163230, loss = 0.24 (8060.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:25.395376: step 163240, loss = 0.20 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:25.714756: step 163250, loss = 0.27 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:26.035364: step 163260, loss = 0.21 (7916.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:26.354445: step 163270, loss = 0.25 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:26.674728: step 163280, loss = 0.19 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:26.995780: step 163290, loss = 0.26 (8190.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:27.315204: step 163300, loss = 0.21 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:27.772216: step 163310, loss = 0.26 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:28.091699: step 163320, loss = 0.22 (8139.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:28.410834: step 163330, loss = 0.19 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:28.730429: step 163340, loss = 0.23 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:29.049809: step 163350, loss = 0.21 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:29.374949: step 163360, loss = 0.17 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:29.696484: step 163370, loss = 0.21 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:30.015001: step 163380, loss = 0.21 (8141.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:30.336855: step 163390, loss = 0.27 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:30.657494: step 163400, loss = 0.21 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:31.109480: step 163410, loss = 0.20 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:31.429714: step 163420, loss = 0.21 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:31.752002: step 163430, loss = 0.24 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:32.074970: step 163440, loss = 0.19 (7521.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:32.394400: step 163450, loss = 0.25 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:32.712251: step 163460, loss = 0.24 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:33.032478: step 163470, loss = 0.15 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:33.353273: step 163480, loss = 0.25 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:33.673551: step 163490, loss = 0.14 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:33.996290: step 163500, loss = 0.27 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:34.445180: step 163510, loss = 0.20 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:34.769730: step 163520, loss = 0.25 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:35.089740: step 163530, loss = 0.26 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:35.411031: step 163540, loss = 0.28 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:35.730208: step 163550, loss = 0.13 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:36.049498: step 163560, loss = 0.24 (7894.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:36.369930: step 163570, loss = 0.20 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:36.689531: step 163580, loss = 0.23 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:37.009345: step 163590, loss = 0.23 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:37.329710: step 163600, loss = 0.21 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:37.799020: step 163610, loss = 0.21 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:38.119179: step 163620, loss = 0.18 (7816.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:38.439964: step 163630, loss = 0.19 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:38.759888: step 163640, loss = 0.24 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:39.080935: step 163650, loss = 0.20 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:39.401955: step 163660, loss = 0.20 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:39.726190: step 163670, loss = 0.20 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:40.045108: step 163680, loss = 0.22 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:40.363163: step 163690, loss = 0.23 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:40.682330: step 163700, loss = 0.22 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:41.147702: step 163710, loss = 0.26 (7927.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:41.470537: step 163720, loss = 0.21 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:41.789611: step 163730, loss = 0.17 (8088.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:42.110294: step 163740, loss = 0.22 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:42.429744: step 163750, loss = 0.19 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:42.750426: step 163760, loss = 0.32 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:43.071384: step 163770, loss = 0.23 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:43.392052: step 163780, loss = 0.25 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:43.714693: step 163790, loss = 0.21 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:44.035457: step 163800, loss = 0.25 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:44.524091: step 163810, loss = 0.23 (7883.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:44.845348: step 163820, loss = 0.24 (8108.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:45.172169: step 163830, loss = 0.22 (7946.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:45.492182: step 163840, loss = 0.22 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:45.811739: step 163850, loss = 0.17 (8162.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:46.134047: step 163860, loss = 0.17 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:46.454635: step 163870, loss = 0.22 (7860.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:46.774097: step 163880, loss = 0.25 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:47.093945: step 163890, loss = 0.24 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:47.413965: step 163900, loss = 0.16 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:47.882222: step 163910, loss = 0.23 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:48.202720: step 163920, loss = 0.24 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:48.522947: step 163930, loss = 0.21 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:48.842439: step 163940, loss = 0.16 (8153.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:49.163885: step 163950, loss = 0.28 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:49.487035: step 163960, loss = 0.22 (7812.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:49.806043: step 163970, loss = 0.24 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:50.127559: step 163980, loss = 0.17 (7983.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:50.447194: step 163990, loss = 0.24 (7996.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:50.767018: step 164000, loss = 0.18 (8106.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:51.321151: step 164010, loss = 0.25 (8031.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:51.639917: step 164020, loss = 0.26 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:51.958695: step 164030, loss = 0.20 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:52.278252: step 164040, loss = 0.16 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:52.598338: step 164050, loss = 0.24 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:52.916860: step 164060, loss = 0.18 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:53.235731: step 164070, loss = 0.24 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:53.555084: step 164080, loss = 0.30 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:53.875712: step 164090, loss = 0.23 (8102.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:54.194450: step 164100, loss = 0.21 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:54.661764: step 164110, loss = 0.21 (7856.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:54.987386: step 164120, loss = 0.20 (7588.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:55.308553: step 164130, loss = 0.29 (7921.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:55.630228: step 164140, loss = 0.24 (7897.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:55.948583: step 164150, loss = 0.20 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:56.268135: step 164160, loss = 0.18 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:56.589587: step 164170, loss = 0.23 (7553.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:11:56.909300: step 164180, loss = 0.23 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:57.232529: step 164190, loss = 0.23 (8165.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:57.555133: step 164200, loss = 0.25 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:58.023595: step 164210, loss = 0.28 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:58.345507: step 164220, loss = 0.18 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:58.666019: step 164230, loss = 0.24 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:58.985926: step 164240, loss = 0.29 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:59.307067: step 164250, loss = 0.24 (7814.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:59.626239: step 164260, loss = 0.22 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:11:59.947867: step 164270, loss = 0.31 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:00.272976: step 164280, loss = 0.21 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:00.591730: step 164290, loss = 0.26 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:00.912161: step 164300, loss = 0.26 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:01.371692: step 164310, loss = 0.19 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:01.692446: step 164320, loss = 0.25 (7836.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:02.011029: step 164330, loss = 0.20 (7940.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:02.332155: step 164340, loss = 0.18 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:02.652584: step 164350, loss = 0.20 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:02.971672: step 164360, loss = 0.25 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:03.292526: step 164370, loss = 0.23 (7827.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:03.613413: step 164380, loss = 0.31 (7779.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:03.932958: step 164390, loss = 0.18 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:04.253645: step 164400, loss = 0.29 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:04.718684: step 164410, loss = 0.22 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:05.041238: step 164420, loss = 0.20 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:05.361246: step 164430, loss = 0.21 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:05.682420: step 164440, loss = 0.17 (8127.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:06.003125: step 164450, loss = 0.17 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:06.325644: step 164460, loss = 0.22 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:06.647512: step 164470, loss = 0.22 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:06.969323: step 164480, loss = 0.26 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:07.290529: step 164490, loss = 0.20 (7907.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:07.612883: step 164500, loss = 0.20 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:08.061547: step 164510, loss = 0.18 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:08.381015: step 164520, loss = 0.17 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:08.700309: step 164530, loss = 0.26 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:09.021236: step 164540, loss = 0.20 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:09.340487: step 164550, loss = 0.20 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:09.659311: step 164560, loss = 0.25 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:09.978244: step 164570, loss = 0.19 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:10.296634: step 164580, loss = 0.25 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:10.618021: step 164590, loss = 0.28 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:10.938594: step 164600, loss = 0.21 (8135.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:11.390653: step 164610, loss = 0.22 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:11.710103: step 164620, loss = 0.22 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:12.031374: step 164630, loss = 0.24 (7923.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:12.352016: step 164640, loss = 0.18 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:12.672195: step 164650, loss = 0.24 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:12.992516: step 164660, loss = 0.17 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:13.313661: step 164670, loss = 0.27 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:13.634293: step 164680, loss = 0.26 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:13.959445: step 164690, loss = 0.23 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:14.280533: step 164700, loss = 0.18 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:14.732387: step 164710, loss = 0.17 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:15.053567: step 164720, loss = 0.20 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:15.373136: step 164730, loss = 0.27 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:15.694652: step 164740, loss = 0.18 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:16.016218: step 164750, loss = 0.24 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:16.337154: step 164760, loss = 0.20 (7862.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:16.658544: step 164770, loss = 0.23 (7823.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:16.978217: step 164780, loss = 0.20 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:17.297857: step 164790, loss = 0.27 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:17.617841: step 164800, loss = 0.20 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:18.078197: step 164810, loss = 0.17 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:18.399584: step 164820, loss = 0.25 (7849.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:18.718878: step 164830, loss = 0.26 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:19.038460: step 164840, loss = 0.20 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:19.356959: step 164850, loss = 0.26 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:19.676151: step 164860, loss = 0.27 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:19.995509: step 164870, loss = 0.18 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:20.315998: step 164880, loss = 0.25 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:20.638028: step 164890, loss = 0.20 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:20.958106: step 164900, loss = 0.19 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:21.420470: step 164910, loss = 0.22 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:21.742930: step 164920, loss = 0.22 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:22.061507: step 164930, loss = 0.21 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:22.381562: step 164940, loss = 0.19 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:22.701765: step 164950, loss = 0.25 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:23.022911: step 164960, loss = 0.18 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:23.341480: step 164970, loss = 0.22 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:23.660824: step 164980, loss = 0.22 (8004.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:23.980147: step 164990, loss = 0.24 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:24.298383: step 165000, loss = 0.22 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:24.858727: step 165010, loss = 0.19 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:25.178007: step 165020, loss = 0.16 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:25.496401: step 165030, loss = 0.20 (8124.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:25.816434: step 165040, loss = 0.21 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:26.135911: step 165050, loss = 0.23 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:26.454522: step 165060, loss = 0.20 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:26.772614: step 165070, loss = 0.23 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:27.093791: step 165080, loss = 0.26 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:27.412484: step 165090, loss = 0.20 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:27.733395: step 165100, loss = 0.23 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:28.192597: step 165110, loss = 0.23 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:28.511513: step 165120, loss = 0.19 (8125.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:28.830394: step 165130, loss = 0.22 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:29.149594: step 165140, loss = 0.23 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:29.470716: step 165150, loss = 0.20 (7594.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:12:29.789971: step 165160, loss = 0.19 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:30.110221: step 165170, loss = 0.27 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:30.430214: step 165180, loss = 0.25 (7841.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:30.750151: step 165190, loss = 0.23 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:31.074746: step 165200, loss = 0.16 (7922.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:31.529653: step 165210, loss = 0.22 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:31.853794: step 165220, loss = 0.22 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:32.174214: step 165230, loss = 0.25 (7926.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:32.491718: step 165240, loss = 0.24 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:32.812094: step 165250, loss = 0.20 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:33.130358: step 165260, loss = 0.24 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:33.452235: step 165270, loss = 0.21 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:33.774375: step 165280, loss = 0.29 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:34.097808: step 165290, loss = 0.24 (7977.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:34.419125: step 165300, loss = 0.24 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:34.877143: step 165310, loss = 0.26 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:35.196128: step 165320, loss = 0.26 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:35.515543: step 165330, loss = 0.19 (8171.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:35.834838: step 165340, loss = 0.19 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:36.154745: step 165350, loss = 0.23 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:36.474043: step 165360, loss = 0.14 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:36.792795: step 165370, loss = 0.33 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:37.110664: step 165380, loss = 0.22 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:37.430061: step 165390, loss = 0.25 (7927.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:37.751628: step 165400, loss = 0.16 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:38.211506: step 165410, loss = 0.20 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:38.530902: step 165420, loss = 0.21 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:38.850174: step 165430, loss = 0.19 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:39.169565: step 165440, loss = 0.27 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:39.488590: step 165450, loss = 0.19 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:39.808220: step 165460, loss = 0.15 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:40.127143: step 165470, loss = 0.26 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:40.447577: step 165480, loss = 0.24 (8135.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:40.767213: step 165490, loss = 0.26 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:41.089858: step 165500, loss = 0.25 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:41.547913: step 165510, loss = 0.14 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:41.866822: step 165520, loss = 0.23 (8175.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:42.188696: step 165530, loss = 0.21 (7974.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:42.509569: step 165540, loss = 0.23 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:42.827562: step 165550, loss = 0.25 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:43.146253: step 165560, loss = 0.17 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:43.468283: step 165570, loss = 0.26 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:43.789159: step 165580, loss = 0.22 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:44.108976: step 165590, loss = 0.24 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:44.427542: step 165600, loss = 0.24 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:44.888749: step 165610, loss = 0.19 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:45.209454: step 165620, loss = 0.21 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:45.529095: step 165630, loss = 0.20 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:45.848873: step 165640, loss = 0.16 (8134.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:46.169818: step 165650, loss = 0.19 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:46.489322: step 165660, loss = 0.21 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:46.809609: step 165670, loss = 0.20 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:47.128963: step 165680, loss = 0.24 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:47.450715: step 165690, loss = 0.26 (7502.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:12:47.770318: step 165700, loss = 0.25 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:48.221424: step 165710, loss = 0.17 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:48.546165: step 165720, loss = 0.20 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:48.863651: step 165730, loss = 0.19 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:49.183791: step 165740, loss = 0.26 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:49.504414: step 165750, loss = 0.19 (7942.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:49.826840: step 165760, loss = 0.23 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:50.149099: step 165770, loss = 0.26 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:50.468970: step 165780, loss = 0.21 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:50.789784: step 165790, loss = 0.23 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:51.110294: step 165800, loss = 0.27 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:51.564107: step 165810, loss = 0.27 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:51.883514: step 165820, loss = 0.25 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:52.203762: step 165830, loss = 0.24 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:52.526335: step 165840, loss = 0.30 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:52.845471: step 165850, loss = 0.23 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:53.165324: step 165860, loss = 0.21 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:53.485701: step 165870, loss = 0.25 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:53.805336: step 165880, loss = 0.21 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:54.124202: step 165890, loss = 0.20 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:54.445544: step 165900, loss = 0.23 (7840.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:54.907712: step 165910, loss = 0.24 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:55.226660: step 165920, loss = 0.19 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:55.546491: step 165930, loss = 0.23 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:55.867110: step 165940, loss = 0.17 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:56.187733: step 165950, loss = 0.22 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:56.512096: step 165960, loss = 0.20 (8148.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:56.836172: step 165970, loss = 0.17 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:57.157433: step 165980, loss = 0.30 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:57.479963: step 165990, loss = 0.25 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:57.800256: step 166000, loss = 0.24 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:58.362290: step 166010, loss = 0.18 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:58.682086: step 166020, loss = 0.22 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:59.004364: step 166030, loss = 0.19 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:59.322834: step 166040, loss = 0.23 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:59.644621: step 166050, loss = 0.31 (7940.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:12:59.965273: step 166060, loss = 0.24 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:00.285793: step 166070, loss = 0.25 (7992.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:00.605347: step 166080, loss = 0.24 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:00.925117: step 166090, loss = 0.24 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:01.245951: step 166100, loss = 0.26 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:01.713433: step 166110, loss = 0.20 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:02.036548: step 166120, loss = 0.22 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:02.356555: step 166130, loss = 0.23 (7875.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:02.678150: step 166140, loss = 0.20 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:02.998095: step 166150, loss = 0.22 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:03.315913: step 166160, loss = 0.28 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:03.637891: step 166170, loss = 0.30 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:03.958080: step 166180, loss = 0.23 (7907.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:04.282956: step 166190, loss = 0.24 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:04.603744: step 166200, loss = 0.25 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:05.063217: step 166210, loss = 0.31 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:05.383221: step 166220, loss = 0.20 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:05.703104: step 166230, loss = 0.24 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:06.023014: step 166240, loss = 0.20 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:06.345333: step 166250, loss = 0.21 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:06.664932: step 166260, loss = 0.15 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:06.985388: step 166270, loss = 0.20 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:07.307911: step 166280, loss = 0.24 (7959.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:07.627579: step 166290, loss = 0.26 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:07.948083: step 166300, loss = 0.19 (7884.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:08.403448: step 166310, loss = 0.26 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:08.723926: step 166320, loss = 0.21 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:09.042397: step 166330, loss = 0.18 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:09.363103: step 166340, loss = 0.23 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:09.683374: step 166350, loss = 0.23 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:10.002927: step 166360, loss = 0.26 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:10.323557: step 166370, loss = 0.24 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:10.647819: step 166380, loss = 0.22 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:10.970904: step 166390, loss = 0.23 (7687.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:11.289791: step 166400, loss = 0.19 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:11.751530: step 166410, loss = 0.23 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:12.071787: step 166420, loss = 0.24 (7935.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:12.392510: step 166430, loss = 0.28 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:12.715706: step 166440, loss = 0.21 (7635.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:13.037035: step 166450, loss = 0.20 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:13.361393: step 166460, loss = 0.23 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:13.683353: step 166470, loss = 0.25 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:14.005735: step 166480, loss = 0.19 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:14.326845: step 166490, loss = 0.21 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:14.648244: step 166500, loss = 0.22 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:15.097921: step 166510, loss = 0.20 (7665.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:15.420732: step 166520, loss = 0.18 (8099.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:15.740299: step 166530, loss = 0.26 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:16.059284: step 166540, loss = 0.27 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:16.380437: step 166550, loss = 0.15 (7954.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:16.700314: step 166560, loss = 0.17 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:17.019150: step 166570, loss = 0.24 (8143.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:17.338467: step 166580, loss = 0.20 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:17.659913: step 166590, loss = 0.19 (8152.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:17.978533: step 166600, loss = 0.22 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:18.426900: step 166610, loss = 0.20 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:18.751422: step 166620, loss = 0.23 (7445.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:19.071693: step 166630, loss = 0.21 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:19.389459: step 166640, loss = 0.20 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:19.711219: step 166650, loss = 0.19 (7821.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:20.035675: step 166660, loss = 0.28 (7018.5 examples/sec; 0.018 sec/batch)
2017-09-16 17:13:20.356310: step 166670, loss = 0.23 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:20.677046: step 166680, loss = 0.19 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:20.997143: step 166690, loss = 0.21 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:21.318859: step 166700, loss = 0.21 (7506.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:21.770985: step 166710, loss = 0.23 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:22.089976: step 166720, loss = 0.28 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:22.410613: step 166730, loss = 0.17 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:22.732729: step 166740, loss = 0.24 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:23.056916: step 166750, loss = 0.20 (7577.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:23.377798: step 166760, loss = 0.24 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:23.698533: step 166770, loss = 0.18 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:24.017035: step 166780, loss = 0.20 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:24.335301: step 166790, loss = 0.30 (7896.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:24.654898: step 166800, loss = 0.22 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:25.119224: step 166810, loss = 0.31 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:25.438293: step 166820, loss = 0.24 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:25.759048: step 166830, loss = 0.23 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:26.079504: step 166840, loss = 0.20 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:26.400054: step 166850, loss = 0.20 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:26.721030: step 166860, loss = 0.26 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:27.042306: step 166870, loss = 0.23 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:27.367394: step 166880, loss = 0.22 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:27.688725: step 166890, loss = 0.18 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:28.007504: step 166900, loss = 0.22 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:28.480267: step 166910, loss = 0.31 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:28.800166: step 166920, loss = 0.21 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:29.120409: step 166930, loss = 0.33 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:29.441808: step 166940, loss = 0.20 (7891.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:29.762736: step 166950, loss = 0.22 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:30.082159: step 166960, loss = 0.16 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:30.405854: step 166970, loss = 0.25 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:30.724163: step 166980, loss = 0.20 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:31.044067: step 166990, loss = 0.23 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:31.363624: step 167000, loss = 0.28 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:31.912638: step 167010, loss = 0.17 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:32.231678: step 167020, loss = 0.21 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:32.552220: step 167030, loss = 0.21 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:32.873521: step 167040, loss = 0.20 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:33.196958: step 167050, loss = 0.29 (7672.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:13:33.517320: step 167060, loss = 0.20 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:33.836708: step 167070, loss = 0.22 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:34.157642: step 167080, loss = 0.19 (8036.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:34.476743: step 167090, loss = 0.22 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:34.798178: step 167100, loss = 0.19 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:35.254019: step 167110, loss = 0.21 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:35.574825: step 167120, loss = 0.20 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:35.896014: step 167130, loss = 0.18 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:36.216869: step 167140, loss = 0.23 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:36.536453: step 167150, loss = 0.17 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:36.857689: step 167160, loss = 0.20 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:37.178726: step 167170, loss = 0.21 (7826.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:37.500724: step 167180, loss = 0.26 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:37.821274: step 167190, loss = 0.16 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:38.143133: step 167200, loss = 0.22 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:38.617972: step 167210, loss = 0.21 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:38.938683: step 167220, loss = 0.23 (7810.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:39.258268: step 167230, loss = 0.19 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:39.579180: step 167240, loss = 0.25 (7830.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:39.897935: step 167250, loss = 0.23 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:40.217968: step 167260, loss = 0.18 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:40.539116: step 167270, loss = 0.24 (7921.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:40.858511: step 167280, loss = 0.23 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:41.185238: step 167290, loss = 0.19 (7929.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:41.506136: step 167300, loss = 0.24 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:41.968807: step 167310, loss = 0.21 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:42.289388: step 167320, loss = 0.23 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:42.611059: step 167330, loss = 0.27 (7934.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:42.930876: step 167340, loss = 0.26 (7935.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:43.251559: step 167350, loss = 0.18 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:43.571042: step 167360, loss = 0.18 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:43.896310: step 167370, loss = 0.20 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:44.217517: step 167380, loss = 0.19 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:44.537051: step 167390, loss = 0.28 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:44.859054: step 167400, loss = 0.24 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:45.321178: step 167410, loss = 0.23 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:45.643671: step 167420, loss = 0.19 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:45.962119: step 167430, loss = 0.26 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:46.283007: step 167440, loss = 0.25 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:46.606202: step 167450, loss = 0.24 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:46.927270: step 167460, loss = 0.18 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:47.245881: step 167470, loss = 0.27 (8106.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:47.568272: step 167480, loss = 0.22 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:47.888452: step 167490, loss = 0.21 (7989.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:48.208541: step 167500, loss = 0.26 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:48.664396: step 167510, loss = 0.17 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:48.984173: step 167520, loss = 0.25 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:49.306780: step 167530, loss = 0.19 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:49.627443: step 167540, loss = 0.21 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:49.947708: step 167550, loss = 0.24 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:50.267978: step 167560, loss = 0.19 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:50.588151: step 167570, loss = 0.26 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:50.906848: step 167580, loss = 0.21 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:51.233558: step 167590, loss = 0.23 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:51.555157: step 167600, loss = 0.27 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:52.018022: step 167610, loss = 0.29 (7828.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:52.338931: step 167620, loss = 0.22 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:52.660268: step 167630, loss = 0.30 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:52.980038: step 167640, loss = 0.19 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:53.301146: step 167650, loss = 0.19 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:53.621762: step 167660, loss = 0.21 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:53.942259: step 167670, loss = 0.23 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:54.261522: step 167680, loss = 0.21 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:54.582846: step 167690, loss = 0.31 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:54.904558: step 167700, loss = 0.22 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:55.365353: step 167710, loss = 0.19 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:55.687018: step 167720, loss = 0.36 (7877.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:56.007909: step 167730, loss = 0.20 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:56.328395: step 167740, loss = 0.32 (8000.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:56.647643: step 167750, loss = 0.30 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:56.971917: step 167760, loss = 0.16 (7778.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:57.293035: step 167770, loss = 0.19 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:57.613445: step 167780, loss = 0.19 (7824.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:57.933179: step 167790, loss = 0.22 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:58.255714: step 167800, loss = 0.23 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:58.728985: step 167810, loss = 0.20 (8138.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:59.048407: step 167820, loss = 0.21 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:59.368032: step 167830, loss = 0.18 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:13:59.689352: step 167840, loss = 0.28 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:00.007257: step 167850, loss = 0.20 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:00.327646: step 167860, loss = 0.20 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:00.648335: step 167870, loss = 0.21 (7894.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:00.968818: step 167880, loss = 0.30 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:01.290342: step 167890, loss = 0.32 (8040.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:01.611229: step 167900, loss = 0.25 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:02.076343: step 167910, loss = 0.28 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:02.396430: step 167920, loss = 0.19 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:02.716656: step 167930, loss = 0.23 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:03.036538: step 167940, loss = 0.20 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:03.356924: step 167950, loss = 0.16 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:03.677645: step 167960, loss = 0.22 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:03.998735: step 167970, loss = 0.21 (7952.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:04.319507: step 167980, loss = 0.25 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:04.639508: step 167990, loss = 0.21 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:04.958077: step 168000, loss = 0.29 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:05.519433: step 168010, loss = 0.28 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:05.844367: step 168020, loss = 0.19 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:06.165628: step 168030, loss = 0.28 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:06.486948: step 168040, loss = 0.26 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:06.807714: step 168050, loss = 0.31 (7787.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:07.130281: step 168060, loss = 0.21 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:07.450691: step 168070, loss = 0.25 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:07.771672: step 168080, loss = 0.18 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:08.094615: step 168090, loss = 0.18 (7841.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:08.416126: step 168100, loss = 0.26 (7659.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:14:08.881174: step 168110, loss = 0.28 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:09.201968: step 168120, loss = 0.23 (7916.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:09.522082: step 168130, loss = 0.21 (8178.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:09.846368: step 168140, loss = 0.20 (7513.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:14:10.164871: step 168150, loss = 0.22 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:10.488225: step 168160, loss = 0.19 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:10.807471: step 168170, loss = 0.25 (8134.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:11.126383: step 168180, loss = 0.21 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:11.448381: step 168190, loss = 0.18 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:11.769267: step 168200, loss = 0.22 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:12.227015: step 168210, loss = 0.19 (7930.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:12.549228: step 168220, loss = 0.23 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:12.870853: step 168230, loss = 0.22 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:13.192496: step 168240, loss = 0.26 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:13.517142: step 168250, loss = 0.23 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:13.839097: step 168260, loss = 0.24 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:14.159615: step 168270, loss = 0.23 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:14.481181: step 168280, loss = 0.17 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:14.802152: step 168290, loss = 0.22 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:15.123018: step 168300, loss = 0.15 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:15.585830: step 168310, loss = 0.21 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:15.907220: step 168320, loss = 0.26 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:16.228239: step 168330, loss = 0.18 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:16.550177: step 168340, loss = 0.26 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:16.868108: step 168350, loss = 0.28 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:17.186887: step 168360, loss = 0.19 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:17.506954: step 168370, loss = 0.27 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:17.826328: step 168380, loss = 0.20 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:18.145515: step 168390, loss = 0.19 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:18.466033: step 168400, loss = 0.18 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:18.930336: step 168410, loss = 0.20 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:19.249884: step 168420, loss = 0.23 (8111.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:19.569798: step 168430, loss = 0.23 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:19.890298: step 168440, loss = 0.21 (8009.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:20.216290: step 168450, loss = 0.22 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:20.537712: step 168460, loss = 0.25 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:20.858582: step 168470, loss = 0.19 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:21.179870: step 168480, loss = 0.24 (7861.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:21.502867: step 168490, loss = 0.19 (7795.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:21.824407: step 168500, loss = 0.23 (7839.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:22.278596: step 168510, loss = 0.22 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:22.599961: step 168520, loss = 0.17 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:22.920948: step 168530, loss = 0.23 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:23.242439: step 168540, loss = 0.22 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:23.562193: step 168550, loss = 0.18 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:23.882502: step 168560, loss = 0.20 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:24.201567: step 168570, loss = 0.19 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:24.520988: step 168580, loss = 0.17 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:24.840199: step 168590, loss = 0.26 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:25.161048: step 168600, loss = 0.23 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:25.621271: step 168610, loss = 0.17 (7809.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:25.942620: step 168620, loss = 0.23 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:26.263251: step 168630, loss = 0.16 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:26.583745: step 168640, loss = 0.27 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:26.902208: step 168650, loss = 0.18 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:27.221359: step 168660, loss = 0.15 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:27.540339: step 168670, loss = 0.16 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:27.860001: step 168680, loss = 0.18 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:28.182769: step 168690, loss = 0.20 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:28.501704: step 168700, loss = 0.23 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:28.959173: step 168710, loss = 0.21 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:29.281315: step 168720, loss = 0.19 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:29.603656: step 168730, loss = 0.23 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:29.924940: step 168740, loss = 0.20 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:30.245284: step 168750, loss = 0.17 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:30.566722: step 168760, loss = 0.22 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:30.888224: step 168770, loss = 0.23 (8097.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:31.208438: step 168780, loss = 0.22 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:31.531740: step 168790, loss = 0.19 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:31.851867: step 168800, loss = 0.17 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:32.313087: step 168810, loss = 0.23 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:32.633862: step 168820, loss = 0.21 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:32.958919: step 168830, loss = 0.29 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:33.277455: step 168840, loss = 0.22 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:33.597730: step 168850, loss = 0.31 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:33.919163: step 168860, loss = 0.21 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:34.238143: step 168870, loss = 0.19 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:34.557769: step 168880, loss = 0.21 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:34.875490: step 168890, loss = 0.21 (8145.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:35.196878: step 168900, loss = 0.18 (7813.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:35.659793: step 168910, loss = 0.19 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:35.981138: step 168920, loss = 0.23 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:36.301108: step 168930, loss = 0.22 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:36.620605: step 168940, loss = 0.25 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:36.943586: step 168950, loss = 0.23 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:37.266398: step 168960, loss = 0.16 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:37.586004: step 168970, loss = 0.14 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:37.909690: step 168980, loss = 0.25 (8099.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:38.230346: step 168990, loss = 0.19 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:38.549446: step 169000, loss = 0.24 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:39.094698: step 169010, loss = 0.19 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:39.414531: step 169020, loss = 0.20 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:39.733806: step 169030, loss = 0.24 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:40.053877: step 169040, loss = 0.20 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:40.377477: step 169050, loss = 0.23 (7719.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:14:40.696952: step 169060, loss = 0.16 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:41.018836: step 169070, loss = 0.25 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:41.340308: step 169080, loss = 0.21 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:41.659126: step 169090, loss = 0.15 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:41.978572: step 169100, loss = 0.23 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:42.442444: step 169110, loss = 0.33 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:42.762796: step 169120, loss = 0.22 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:43.085632: step 169130, loss = 0.23 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:43.406855: step 169140, loss = 0.19 (7722.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:14:43.727779: step 169150, loss = 0.21 (8007.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:44.049855: step 169160, loss = 0.17 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:44.370681: step 169170, loss = 0.25 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:44.696254: step 169180, loss = 0.26 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:45.016838: step 169190, loss = 0.22 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:45.337842: step 169200, loss = 0.26 (7810.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:45.801404: step 169210, loss = 0.19 (7814.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:46.121729: step 169220, loss = 0.22 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:46.446094: step 169230, loss = 0.20 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:46.765874: step 169240, loss = 0.23 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:47.085914: step 169250, loss = 0.20 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:47.408650: step 169260, loss = 0.18 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:47.727726: step 169270, loss = 0.21 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:48.051266: step 169280, loss = 0.19 (7887.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:48.375933: step 169290, loss = 0.18 (7640.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:14:48.693872: step 169300, loss = 0.19 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:49.152903: step 169310, loss = 0.15 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:49.472432: step 169320, loss = 0.19 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:49.791487: step 169330, loss = 0.24 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:50.112575: step 169340, loss = 0.23 (7916.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:50.432777: step 169350, loss = 0.18 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:50.756856: step 169360, loss = 0.16 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:51.076692: step 169370, loss = 0.20 (7927.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:51.396953: step 169380, loss = 0.19 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:51.717803: step 169390, loss = 0.21 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:52.039005: step 169400, loss = 0.24 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:52.500152: step 169410, loss = 0.21 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:52.821386: step 169420, loss = 0.21 (7923.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:53.144431: step 169430, loss = 0.19 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:53.466097: step 169440, loss = 0.27 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:53.788728: step 169450, loss = 0.14 (8141.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:54.109368: step 169460, loss = 0.22 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:54.433001: step 169470, loss = 0.25 (7791.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:54.754443: step 169480, loss = 0.21 (7879.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:55.075150: step 169490, loss = 0.21 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:55.394916: step 169500, loss = 0.20 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:55.860639: step 169510, loss = 0.20 (7948.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:56.180833: step 169520, loss = 0.23 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:56.500591: step 169530, loss = 0.19 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:56.822925: step 169540, loss = 0.23 (7957.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:57.142033: step 169550, loss = 0.24 (8139.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:57.462956: step 169560, loss = 0.24 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:57.784622: step 169570, loss = 0.25 (8118.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:58.102993: step 169580, loss = 0.21 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:58.425615: step 169590, loss = 0.21 (7758.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:58.745244: step 169600, loss = 0.21 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:59.199818: step 169610, loss = 0.19 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:59.520239: step 169620, loss = 0.20 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:14:59.842167: step 169630, loss = 0.22 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:00.162220: step 169640, loss = 0.15 (7942.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:00.481136: step 169650, loss = 0.21 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:00.801494: step 169660, loss = 0.19 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:01.121389: step 169670, loss = 0.25 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:01.441751: step 169680, loss = 0.22 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:01.765171: step 169690, loss = 0.20 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:02.086403: step 169700, loss = 0.21 (8104.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:02.552186: step 169710, loss = 0.29 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:02.871419: step 169720, loss = 0.30 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:03.191710: step 169730, loss = 0.25 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:03.512723: step 169740, loss = 0.22 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:03.835354: step 169750, loss = 0.22 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:04.157559: step 169760, loss = 0.23 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:04.480257: step 169770, loss = 0.18 (7802.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:04.802415: step 169780, loss = 0.21 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:05.123199: step 169790, loss = 0.28 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:05.443068: step 169800, loss = 0.16 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:05.902047: step 169810, loss = 0.18 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:06.223045: step 169820, loss = 0.22 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:06.543516: step 169830, loss = 0.15 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:06.864797: step 169840, loss = 0.20 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:07.185472: step 169850, loss = 0.27 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:07.506468: step 169860, loss = 0.25 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:07.829557: step 169870, loss = 0.25 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:08.150069: step 169880, loss = 0.23 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:08.473608: step 169890, loss = 0.23 (7706.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:15:08.796784: step 169900, loss = 0.33 (7837.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:09.255402: step 169910, loss = 0.21 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:09.576952: step 169920, loss = 0.23 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:09.896950: step 169930, loss = 0.24 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:10.217808: step 169940, loss = 0.20 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:10.539219: step 169950, loss = 0.21 (7806.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:10.859630: step 169960, loss = 0.18 (7852.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:11.181863: step 169970, loss = 0.17 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:11.503666: step 169980, loss = 0.23 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:11.821586: step 169990, loss = 0.24 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:12.141122: step 170000, loss = 0.21 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:12.712360: step 170010, loss = 0.18 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:13.037832: step 170020, loss = 0.24 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:13.363581: step 170030, loss = 0.21 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:13.685358: step 170040, loss = 0.35 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:14.006683: step 170050, loss = 0.20 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:14.327244: step 170060, loss = 0.24 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:14.649711: step 170070, loss = 0.15 (8020.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:14.969539: step 170080, loss = 0.23 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:15.294175: step 170090, loss = 0.26 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:15.615203: step 170100, loss = 0.27 (7845.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:16.076673: step 170110, loss = 0.19 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:16.395078: step 170120, loss = 0.27 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:16.719775: step 170130, loss = 0.25 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:17.040028: step 170140, loss = 0.23 (7934.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:17.360273: step 170150, loss = 0.19 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:17.680070: step 170160, loss = 0.20 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:18.001017: step 170170, loss = 0.25 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:18.320169: step 170180, loss = 0.25 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:18.640090: step 170190, loss = 0.25 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:18.960814: step 170200, loss = 0.21 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:19.424894: step 170210, loss = 0.20 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:19.745394: step 170220, loss = 0.21 (7901.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:20.065523: step 170230, loss = 0.22 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:20.385559: step 170240, loss = 0.21 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:20.703753: step 170250, loss = 0.23 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:21.023217: step 170260, loss = 0.26 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:21.344224: step 170270, loss = 0.28 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:21.665340: step 170280, loss = 0.24 (7785.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:21.987084: step 170290, loss = 0.18 (8033.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:22.308643: step 170300, loss = 0.20 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:22.771625: step 170310, loss = 0.30 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:23.094884: step 170320, loss = 0.20 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:23.416283: step 170330, loss = 0.21 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:23.738214: step 170340, loss = 0.18 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:24.061944: step 170350, loss = 0.24 (7902.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:24.384979: step 170360, loss = 0.17 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:24.707709: step 170370, loss = 0.18 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:25.028282: step 170380, loss = 0.28 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:25.347601: step 170390, loss = 0.17 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:25.667556: step 170400, loss = 0.22 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:26.129217: step 170410, loss = 0.25 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:26.449910: step 170420, loss = 0.22 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:26.770947: step 170430, loss = 0.20 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:27.091879: step 170440, loss = 0.22 (7897.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:27.412427: step 170450, loss = 0.21 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:27.733019: step 170460, loss = 0.21 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:28.054496: step 170470, loss = 0.29 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:28.378359: step 170480, loss = 0.19 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:28.699658: step 170490, loss = 0.24 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:29.023209: step 170500, loss = 0.14 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:29.505127: step 170510, loss = 0.23 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:29.823144: step 170520, loss = 0.21 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:30.144633: step 170530, loss = 0.24 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:30.465773: step 170540, loss = 0.17 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:30.789068: step 170550, loss = 0.20 (7976.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:31.111778: step 170560, loss = 0.20 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:31.432087: step 170570, loss = 0.23 (8006.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:31.754034: step 170580, loss = 0.20 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:32.074312: step 170590, loss = 0.19 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:32.397159: step 170600, loss = 0.27 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:32.855035: step 170610, loss = 0.27 (7891.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:33.175347: step 170620, loss = 0.20 (7959.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:33.496450: step 170630, loss = 0.18 (8017.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:33.816466: step 170640, loss = 0.28 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:34.138879: step 170650, loss = 0.18 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:34.462585: step 170660, loss = 0.27 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:34.785708: step 170670, loss = 0.28 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:35.105227: step 170680, loss = 0.24 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:35.426866: step 170690, loss = 0.21 (7962.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:35.748792: step 170700, loss = 0.26 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:36.220703: step 170710, loss = 0.23 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:36.541652: step 170720, loss = 0.26 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:36.865793: step 170730, loss = 0.25 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:37.185407: step 170740, loss = 0.19 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:37.506099: step 170750, loss = 0.20 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:37.825152: step 170760, loss = 0.21 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:38.144129: step 170770, loss = 0.24 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:38.467320: step 170780, loss = 0.21 (7557.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:15:38.787830: step 170790, loss = 0.21 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:39.111702: step 170800, loss = 0.22 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:39.566574: step 170810, loss = 0.23 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:39.886933: step 170820, loss = 0.15 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:40.207889: step 170830, loss = 0.23 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:40.529754: step 170840, loss = 0.29 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:40.849991: step 170850, loss = 0.20 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:41.171012: step 170860, loss = 0.22 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:41.491936: step 170870, loss = 0.15 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:41.815978: step 170880, loss = 0.25 (7886.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:42.136722: step 170890, loss = 0.22 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:42.456392: step 170900, loss = 0.18 (7798.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:42.911347: step 170910, loss = 0.22 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:43.231953: step 170920, loss = 0.16 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:43.552972: step 170930, loss = 0.30 (7899.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:43.873359: step 170940, loss = 0.17 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:44.197306: step 170950, loss = 0.23 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:44.517579: step 170960, loss = 0.21 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:44.838937: step 170970, loss = 0.18 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:45.158892: step 170980, loss = 0.17 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:45.480754: step 170990, loss = 0.26 (8001.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:45.801062: step 171000, loss = 0.20 (8133.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:46.418171: step 171010, loss = 0.24 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:46.739002: step 171020, loss = 0.21 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:47.061693: step 171030, loss = 0.22 (7944.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:47.382980: step 171040, loss = 0.25 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:47.701154: step 171050, loss = 0.27 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:48.020538: step 171060, loss = 0.25 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:48.343223: step 171070, loss = 0.16 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:48.662922: step 171080, loss = 0.20 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:48.985562: step 171090, loss = 0.29 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:49.307559: step 171100, loss = 0.23 (7623.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:15:49.767168: step 171110, loss = 0.17 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:50.087650: step 171120, loss = 0.21 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:50.407666: step 171130, loss = 0.21 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:50.729101: step 171140, loss = 0.17 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:51.048806: step 171150, loss = 0.23 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:51.367240: step 171160, loss = 0.25 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:51.686736: step 171170, loss = 0.20 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:52.007761: step 171180, loss = 0.19 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:52.326872: step 171190, loss = 0.14 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:52.647292: step 171200, loss = 0.19 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:53.098935: step 171210, loss = 0.22 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:53.418452: step 171220, loss = 0.27 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:53.738609: step 171230, loss = 0.19 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:54.059455: step 171240, loss = 0.19 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:54.380224: step 171250, loss = 0.20 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:54.699878: step 171260, loss = 0.19 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:55.020186: step 171270, loss = 0.27 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:55.340863: step 171280, loss = 0.24 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:55.660744: step 171290, loss = 0.28 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:55.982896: step 171300, loss = 0.19 (7959.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:56.446430: step 171310, loss = 0.17 (7872.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:56.767555: step 171320, loss = 0.19 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:57.088371: step 171330, loss = 0.22 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:57.412162: step 171340, loss = 0.16 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:57.732269: step 171350, loss = 0.19 (7802.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:58.052101: step 171360, loss = 0.19 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:58.372291: step 171370, loss = 0.24 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:58.693904: step 171380, loss = 0.19 (7970.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:59.013459: step 171390, loss = 0.18 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:59.336114: step 171400, loss = 0.25 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:15:59.801592: step 171410, loss = 0.22 (8058.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:00.122611: step 171420, loss = 0.21 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:00.444613: step 171430, loss = 0.15 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:00.764423: step 171440, loss = 0.21 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:01.086193: step 171450, loss = 0.20 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:01.406781: step 171460, loss = 0.19 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:01.728194: step 171470, loss = 0.20 (7773.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:02.046973: step 171480, loss = 0.25 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:02.365593: step 171490, loss = 0.19 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:02.686205: step 171500, loss = 0.21 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:03.147318: step 171510, loss = 0.37 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:03.468843: step 171520, loss = 0.28 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:03.790585: step 171530, loss = 0.22 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:04.111029: step 171540, loss = 0.28 (7916.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:04.429407: step 171550, loss = 0.19 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:04.749702: step 171560, loss = 0.22 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:05.071519: step 171570, loss = 0.28 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:05.394277: step 171580, loss = 0.28 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:05.715126: step 171590, loss = 0.22 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:06.035082: step 171600, loss = 0.23 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:06.497218: step 171610, loss = 0.25 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:06.815251: step 171620, loss = 0.21 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:07.136148: step 171630, loss = 0.25 (8136.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:07.459102: step 171640, loss = 0.18 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:07.779582: step 171650, loss = 0.22 (7853.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:08.104051: step 171660, loss = 0.18 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:08.429361: step 171670, loss = 0.23 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:08.749603: step 171680, loss = 0.26 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:09.068408: step 171690, loss = 0.18 (7814.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:09.393281: step 171700, loss = 0.22 (7811.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:09.847669: step 171710, loss = 0.23 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:10.170352: step 171720, loss = 0.24 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:10.490893: step 171730, loss = 0.19 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:10.812082: step 171740, loss = 0.27 (7955.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:11.133682: step 171750, loss = 0.19 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:11.452091: step 171760, loss = 0.22 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:11.773917: step 171770, loss = 0.17 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:12.093491: step 171780, loss = 0.24 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:12.415087: step 171790, loss = 0.19 (7903.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:12.739671: step 171800, loss = 0.15 (7556.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:16:13.209942: step 171810, loss = 0.17 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:13.532605: step 171820, loss = 0.25 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:13.854820: step 171830, loss = 0.19 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:14.174585: step 171840, loss = 0.25 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:14.495865: step 171850, loss = 0.21 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:14.817595: step 171860, loss = 0.16 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:15.139064: step 171870, loss = 0.20 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:15.465399: step 171880, loss = 0.22 (7764.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:15.786114: step 171890, loss = 0.28 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:16.107691: step 171900, loss = 0.16 (7863.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:16.569928: step 171910, loss = 0.28 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:16.890465: step 171920, loss = 0.20 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:17.209982: step 171930, loss = 0.19 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:17.530178: step 171940, loss = 0.25 (8019.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:17.849856: step 171950, loss = 0.23 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:18.171907: step 171960, loss = 0.25 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:18.492955: step 171970, loss = 0.22 (7842.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:18.812317: step 171980, loss = 0.27 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:19.133963: step 171990, loss = 0.20 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:19.461950: step 172000, loss = 0.23 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:20.026710: step 172010, loss = 0.18 (7889.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:20.348337: step 172020, loss = 0.19 (7965.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:20.669259: step 172030, loss = 0.25 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:20.989645: step 172040, loss = 0.21 (8130.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:21.314188: step 172050, loss = 0.17 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:21.636814: step 172060, loss = 0.21 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:21.956697: step 172070, loss = 0.19 (7954.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:22.279254: step 172080, loss = 0.24 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:22.607250: step 172090, loss = 0.21 (7865.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:22.928347: step 172100, loss = 0.32 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:23.390454: step 172110, loss = 0.24 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:23.710667: step 172120, loss = 0.20 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:24.031447: step 172130, loss = 0.17 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:24.354043: step 172140, loss = 0.19 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:24.677423: step 172150, loss = 0.34 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:24.997618: step 172160, loss = 0.19 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:25.317888: step 172170, loss = 0.23 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:25.637747: step 172180, loss = 0.25 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:25.957003: step 172190, loss = 0.22 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:26.277743: step 172200, loss = 0.17 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:26.744109: step 172210, loss = 0.17 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:27.063911: step 172220, loss = 0.18 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:27.382964: step 172230, loss = 0.24 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:27.702687: step 172240, loss = 0.23 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:28.025857: step 172250, loss = 0.22 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:28.347589: step 172260, loss = 0.20 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:28.667599: step 172270, loss = 0.24 (7965.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:28.993050: step 172280, loss = 0.23 (6929.0 examples/sec; 0.018 sec/batch)
2017-09-16 17:16:29.314846: step 172290, loss = 0.18 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:29.640885: step 172300, loss = 0.22 (7527.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:16:30.103385: step 172310, loss = 0.18 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:30.431120: step 172320, loss = 0.18 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:30.752553: step 172330, loss = 0.21 (7869.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:31.072351: step 172340, loss = 0.26 (8027.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:31.392460: step 172350, loss = 0.19 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:31.713299: step 172360, loss = 0.22 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:32.039516: step 172370, loss = 0.27 (7786.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:32.358255: step 172380, loss = 0.28 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:32.677748: step 172390, loss = 0.24 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:33.000380: step 172400, loss = 0.20 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:33.464583: step 172410, loss = 0.19 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:33.784684: step 172420, loss = 0.23 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:34.104591: step 172430, loss = 0.24 (7848.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:34.427147: step 172440, loss = 0.17 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:34.747435: step 172450, loss = 0.27 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:35.066349: step 172460, loss = 0.22 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:35.388658: step 172470, loss = 0.23 (8080.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:35.710510: step 172480, loss = 0.19 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:36.035375: step 172490, loss = 0.22 (7891.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:36.358863: step 172500, loss = 0.22 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:36.819174: step 172510, loss = 0.22 (7413.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:16:37.137863: step 172520, loss = 0.25 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:37.458714: step 172530, loss = 0.18 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:37.778526: step 172540, loss = 0.21 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:38.099575: step 172550, loss = 0.24 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:38.419570: step 172560, loss = 0.15 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:38.741691: step 172570, loss = 0.21 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:39.061720: step 172580, loss = 0.20 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:39.386511: step 172590, loss = 0.15 (7450.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:16:39.706847: step 172600, loss = 0.24 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:40.153808: step 172610, loss = 0.16 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:40.472606: step 172620, loss = 0.22 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:40.792886: step 172630, loss = 0.18 (7981.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:41.112447: step 172640, loss = 0.26 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:41.437521: step 172650, loss = 0.24 (7876.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:41.757531: step 172660, loss = 0.28 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:42.079233: step 172670, loss = 0.22 (7834.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:42.397519: step 172680, loss = 0.17 (8158.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:42.716355: step 172690, loss = 0.21 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:43.036973: step 172700, loss = 0.20 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:43.499234: step 172710, loss = 0.19 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:43.820633: step 172720, loss = 0.21 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:44.141214: step 172730, loss = 0.20 (7856.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:44.460205: step 172740, loss = 0.16 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:44.784537: step 172750, loss = 0.21 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:45.106123: step 172760, loss = 0.25 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:45.425840: step 172770, loss = 0.20 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:45.745680: step 172780, loss = 0.24 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:46.067115: step 172790, loss = 0.14 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:46.388682: step 172800, loss = 0.20 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:46.858878: step 172810, loss = 0.19 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:47.182510: step 172820, loss = 0.17 (7600.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:16:47.507423: step 172830, loss = 0.30 (7810.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:47.828341: step 172840, loss = 0.24 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:48.149778: step 172850, loss = 0.23 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:48.469300: step 172860, loss = 0.21 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:48.789630: step 172870, loss = 0.13 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:49.110504: step 172880, loss = 0.20 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:49.430661: step 172890, loss = 0.18 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:49.752648: step 172900, loss = 0.18 (7894.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:50.215980: step 172910, loss = 0.17 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:50.538351: step 172920, loss = 0.20 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:50.861923: step 172930, loss = 0.19 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:51.183335: step 172940, loss = 0.15 (7783.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:51.507311: step 172950, loss = 0.20 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:51.832085: step 172960, loss = 0.16 (7891.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:52.154511: step 172970, loss = 0.22 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:52.476422: step 172980, loss = 0.22 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:52.797253: step 172990, loss = 0.20 (7958.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:53.118468: step 173000, loss = 0.33 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:53.670285: step 173010, loss = 0.18 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:53.988702: step 173020, loss = 0.17 (7980.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:54.310005: step 173030, loss = 0.17 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:54.630752: step 173040, loss = 0.20 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:54.950088: step 173050, loss = 0.22 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:55.269582: step 173060, loss = 0.15 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:55.589791: step 173070, loss = 0.27 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:55.911176: step 173080, loss = 0.24 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:56.232402: step 173090, loss = 0.26 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:56.551734: step 173100, loss = 0.21 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:57.013456: step 173110, loss = 0.18 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:57.333025: step 173120, loss = 0.30 (7929.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:57.656034: step 173130, loss = 0.22 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:57.979115: step 173140, loss = 0.15 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:58.299388: step 173150, loss = 0.21 (7766.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:58.620901: step 173160, loss = 0.21 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:58.941037: step 173170, loss = 0.21 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:59.262194: step 173180, loss = 0.24 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:59.581619: step 173190, loss = 0.15 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:16:59.899571: step 173200, loss = 0.22 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:00.349016: step 173210, loss = 0.17 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:00.669002: step 173220, loss = 0.20 (8178.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:00.988402: step 173230, loss = 0.18 (8057.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:01.308195: step 173240, loss = 0.26 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:01.630021: step 173250, loss = 0.21 (7821.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:01.950466: step 173260, loss = 0.18 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:02.271082: step 173270, loss = 0.17 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:02.591965: step 173280, loss = 0.19 (7838.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:02.912514: step 173290, loss = 0.21 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:03.229951: step 173300, loss = 0.26 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:03.692197: step 173310, loss = 0.20 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:04.017045: step 173320, loss = 0.31 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:04.338090: step 173330, loss = 0.26 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:04.658986: step 173340, loss = 0.20 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:04.978838: step 173350, loss = 0.22 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:05.300322: step 173360, loss = 0.28 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:05.619158: step 173370, loss = 0.18 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:05.938790: step 173380, loss = 0.24 (7804.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:06.259182: step 173390, loss = 0.17 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:06.581243: step 173400, loss = 0.24 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:07.046002: step 173410, loss = 0.20 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:07.367226: step 173420, loss = 0.21 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:07.687730: step 173430, loss = 0.18 (7910.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:08.008444: step 173440, loss = 0.20 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:08.329658: step 173450, loss = 0.23 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:08.651800: step 173460, loss = 0.23 (7897.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:08.971232: step 173470, loss = 0.24 (7901.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:09.291845: step 173480, loss = 0.18 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:09.612840: step 173490, loss = 0.21 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:09.930687: step 173500, loss = 0.17 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:10.396152: step 173510, loss = 0.27 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:10.717946: step 173520, loss = 0.18 (7871.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:11.040174: step 173530, loss = 0.21 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:11.359865: step 173540, loss = 0.17 (7780.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:11.679734: step 173550, loss = 0.22 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:11.999949: step 173560, loss = 0.22 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:12.321548: step 173570, loss = 0.23 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:12.641967: step 173580, loss = 0.29 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:12.963913: step 173590, loss = 0.18 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:13.286573: step 173600, loss = 0.19 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:13.749417: step 173610, loss = 0.22 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:14.068482: step 173620, loss = 0.27 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:14.389809: step 173630, loss = 0.15 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:14.711119: step 173640, loss = 0.27 (8145.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:15.032111: step 173650, loss = 0.26 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:15.354445: step 173660, loss = 0.18 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:15.677395: step 173670, loss = 0.27 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:16.000830: step 173680, loss = 0.24 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:16.321137: step 173690, loss = 0.28 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:16.640621: step 173700, loss = 0.21 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:17.104657: step 173710, loss = 0.20 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:17.424170: step 173720, loss = 0.20 (8036.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:17.746458: step 173730, loss = 0.21 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:18.068094: step 173740, loss = 0.26 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:18.388419: step 173750, loss = 0.18 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:18.708492: step 173760, loss = 0.30 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:19.028918: step 173770, loss = 0.31 (7813.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:19.351417: step 173780, loss = 0.18 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:19.672615: step 173790, loss = 0.20 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:19.997269: step 173800, loss = 0.24 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:20.454409: step 173810, loss = 0.21 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:20.773812: step 173820, loss = 0.20 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:21.095004: step 173830, loss = 0.23 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:21.416322: step 173840, loss = 0.21 (7611.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:17:21.739680: step 173850, loss = 0.21 (7973.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:22.061859: step 173860, loss = 0.22 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:22.381786: step 173870, loss = 0.19 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:22.700402: step 173880, loss = 0.24 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:23.019452: step 173890, loss = 0.19 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:23.339275: step 173900, loss = 0.23 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:23.803373: step 173910, loss = 0.21 (7913.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:24.122615: step 173920, loss = 0.20 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:24.442942: step 173930, loss = 0.19 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:24.763636: step 173940, loss = 0.16 (7977.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:25.083391: step 173950, loss = 0.21 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:25.404877: step 173960, loss = 0.18 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:25.725491: step 173970, loss = 0.25 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:26.048791: step 173980, loss = 0.21 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:26.369778: step 173990, loss = 0.19 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:26.691628: step 174000, loss = 0.18 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:27.248968: step 174010, loss = 0.17 (7978.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:27.569037: step 174020, loss = 0.26 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:27.887790: step 174030, loss = 0.19 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:28.210319: step 174040, loss = 0.25 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:28.531167: step 174050, loss = 0.18 (8014.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:28.851861: step 174060, loss = 0.22 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:29.172340: step 174070, loss = 0.25 (7775.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:29.492761: step 174080, loss = 0.18 (8068.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:29.812028: step 174090, loss = 0.19 (8066.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:30.134326: step 174100, loss = 0.19 (8003.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:30.591661: step 174110, loss = 0.22 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:30.910039: step 174120, loss = 0.18 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:31.231855: step 174130, loss = 0.15 (7811.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:31.551119: step 174140, loss = 0.22 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:31.872980: step 174150, loss = 0.24 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:32.195839: step 174160, loss = 0.18 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:32.516760: step 174170, loss = 0.21 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:32.837507: step 174180, loss = 0.23 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:33.158109: step 174190, loss = 0.27 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:33.479402: step 174200, loss = 0.21 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:33.943442: step 174210, loss = 0.18 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:34.263111: step 174220, loss = 0.18 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:34.584843: step 174230, loss = 0.25 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:34.903797: step 174240, loss = 0.21 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:35.223171: step 174250, loss = 0.32 (8134.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:35.543012: step 174260, loss = 0.19 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:35.863556: step 174270, loss = 0.21 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:36.181622: step 174280, loss = 0.24 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:36.501519: step 174290, loss = 0.28 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:36.820077: step 174300, loss = 0.20 (7859.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:37.286269: step 174310, loss = 0.25 (7881.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:37.606212: step 174320, loss = 0.18 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:37.925137: step 174330, loss = 0.25 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:38.247773: step 174340, loss = 0.22 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:38.566612: step 174350, loss = 0.22 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:38.886304: step 174360, loss = 0.17 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:39.208482: step 174370, loss = 0.21 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:39.530417: step 174380, loss = 0.19 (8072.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:39.850087: step 174390, loss = 0.20 (7766.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:40.172334: step 174400, loss = 0.17 (7946.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:40.633620: step 174410, loss = 0.25 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:40.954575: step 174420, loss = 0.17 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:41.276265: step 174430, loss = 0.23 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:41.595171: step 174440, loss = 0.19 (7902.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:41.914850: step 174450, loss = 0.15 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:42.234477: step 174460, loss = 0.18 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:42.557189: step 174470, loss = 0.23 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:42.876077: step 174480, loss = 0.23 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:43.195407: step 174490, loss = 0.23 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:43.517267: step 174500, loss = 0.32 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:43.990647: step 174510, loss = 0.17 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:44.310184: step 174520, loss = 0.24 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:44.630708: step 174530, loss = 0.27 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:44.949771: step 174540, loss = 0.30 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:45.268978: step 174550, loss = 0.23 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:45.588605: step 174560, loss = 0.24 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:45.908736: step 174570, loss = 0.24 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:46.227423: step 174580, loss = 0.22 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:46.548747: step 174590, loss = 0.18 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:46.867740: step 174600, loss = 0.21 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:47.330309: step 174610, loss = 0.29 (8169.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:47.649862: step 174620, loss = 0.18 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:47.970276: step 174630, loss = 0.20 (8013.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:48.292031: step 174640, loss = 0.16 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:48.614141: step 174650, loss = 0.23 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:48.937639: step 174660, loss = 0.25 (7609.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:17:49.257870: step 174670, loss = 0.29 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:49.579261: step 174680, loss = 0.21 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:49.898365: step 174690, loss = 0.25 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:50.219775: step 174700, loss = 0.23 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:50.681116: step 174710, loss = 0.26 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:50.999847: step 174720, loss = 0.18 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:51.319588: step 174730, loss = 0.19 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:51.639149: step 174740, loss = 0.17 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:51.960044: step 174750, loss = 0.17 (8140.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:52.281794: step 174760, loss = 0.24 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:52.602251: step 174770, loss = 0.23 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:52.920412: step 174780, loss = 0.18 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:53.244056: step 174790, loss = 0.20 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:53.562869: step 174800, loss = 0.17 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:54.027354: step 174810, loss = 0.21 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:54.348072: step 174820, loss = 0.24 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:54.668647: step 174830, loss = 0.22 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:54.987688: step 174840, loss = 0.19 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:55.307610: step 174850, loss = 0.24 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:55.631700: step 174860, loss = 0.25 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:55.954905: step 174870, loss = 0.18 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:56.276436: step 174880, loss = 0.18 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:56.597800: step 174890, loss = 0.21 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:56.918708: step 174900, loss = 0.27 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:57.388080: step 174910, loss = 0.19 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:57.709704: step 174920, loss = 0.29 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:58.030287: step 174930, loss = 0.26 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:58.352739: step 174940, loss = 0.22 (7817.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:58.674692: step 174950, loss = 0.25 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:58.993590: step 174960, loss = 0.18 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:59.312264: step 174970, loss = 0.23 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:59.632801: step 174980, loss = 0.21 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:17:59.952759: step 174990, loss = 0.25 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:00.274938: step 175000, loss = 0.19 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:00.837343: step 175010, loss = 0.16 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:01.155872: step 175020, loss = 0.19 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:01.476420: step 175030, loss = 0.17 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:01.796314: step 175040, loss = 0.24 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:02.118248: step 175050, loss = 0.19 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:02.439685: step 175060, loss = 0.20 (7786.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:02.762602: step 175070, loss = 0.17 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:03.081538: step 175080, loss = 0.25 (8139.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:03.402316: step 175090, loss = 0.15 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:03.723368: step 175100, loss = 0.22 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:04.184265: step 175110, loss = 0.15 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:04.507903: step 175120, loss = 0.19 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:04.831477: step 175130, loss = 0.21 (7147.2 examples/sec; 0.018 sec/batch)
2017-09-16 17:18:05.151006: step 175140, loss = 0.21 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:05.472031: step 175150, loss = 0.19 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:05.791921: step 175160, loss = 0.29 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:06.111153: step 175170, loss = 0.15 (7903.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:06.430035: step 175180, loss = 0.23 (8083.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:06.750722: step 175190, loss = 0.18 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:07.070959: step 175200, loss = 0.22 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:07.531492: step 175210, loss = 0.24 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:07.853287: step 175220, loss = 0.22 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:08.173472: step 175230, loss = 0.23 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:08.492174: step 175240, loss = 0.24 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:08.814035: step 175250, loss = 0.19 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:09.137710: step 175260, loss = 0.22 (7905.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:09.461852: step 175270, loss = 0.26 (8117.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:09.780781: step 175280, loss = 0.18 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:10.100509: step 175290, loss = 0.19 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:10.423004: step 175300, loss = 0.20 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:10.872182: step 175310, loss = 0.22 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:11.192281: step 175320, loss = 0.14 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:11.510338: step 175330, loss = 0.26 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:11.832479: step 175340, loss = 0.20 (7961.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:12.153930: step 175350, loss = 0.23 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:12.473812: step 175360, loss = 0.19 (7925.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:12.794866: step 175370, loss = 0.27 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:13.116563: step 175380, loss = 0.23 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:13.434389: step 175390, loss = 0.20 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:13.755759: step 175400, loss = 0.22 (7937.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:14.216665: step 175410, loss = 0.22 (7878.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:14.536452: step 175420, loss = 0.23 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:14.856620: step 175430, loss = 0.25 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:15.177104: step 175440, loss = 0.21 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:15.500621: step 175450, loss = 0.30 (7496.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:15.820168: step 175460, loss = 0.16 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:16.143342: step 175470, loss = 0.23 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:16.467017: step 175480, loss = 0.24 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:16.786316: step 175490, loss = 0.23 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:17.106770: step 175500, loss = 0.23 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:17.567501: step 175510, loss = 0.24 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:17.887977: step 175520, loss = 0.20 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:18.212298: step 175530, loss = 0.26 (7424.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:18.531271: step 175540, loss = 0.21 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:18.852502: step 175550, loss = 0.26 (7954.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:19.173284: step 175560, loss = 0.26 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:19.492694: step 175570, loss = 0.19 (7859.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:19.815494: step 175580, loss = 0.17 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:20.135285: step 175590, loss = 0.24 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:20.454707: step 175600, loss = 0.17 (8150.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:20.913109: step 175610, loss = 0.15 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:21.232378: step 175620, loss = 0.18 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:21.550760: step 175630, loss = 0.17 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:21.869302: step 175640, loss = 0.16 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:22.188946: step 175650, loss = 0.24 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:22.507163: step 175660, loss = 0.23 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:22.829786: step 175670, loss = 0.23 (7720.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:23.148583: step 175680, loss = 0.22 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:23.469514: step 175690, loss = 0.21 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:23.789044: step 175700, loss = 0.21 (7795.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:24.256121: step 175710, loss = 0.19 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:24.575362: step 175720, loss = 0.22 (8078.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:24.898239: step 175730, loss = 0.23 (7510.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:25.220352: step 175740, loss = 0.27 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:25.542059: step 175750, loss = 0.28 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:25.865070: step 175760, loss = 0.23 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:26.186933: step 175770, loss = 0.25 (8018.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:26.509169: step 175780, loss = 0.25 (7797.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:26.831134: step 175790, loss = 0.23 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:27.158990: step 175800, loss = 0.19 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:27.631017: step 175810, loss = 0.20 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:27.950652: step 175820, loss = 0.22 (8145.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:28.271578: step 175830, loss = 0.19 (7803.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:28.591409: step 175840, loss = 0.24 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:28.912720: step 175850, loss = 0.17 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:29.232424: step 175860, loss = 0.21 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:29.552371: step 175870, loss = 0.25 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:29.876169: step 175880, loss = 0.25 (7450.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:30.198891: step 175890, loss = 0.17 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:30.520865: step 175900, loss = 0.21 (7808.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:30.984234: step 175910, loss = 0.23 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:31.302716: step 175920, loss = 0.25 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:31.624777: step 175930, loss = 0.17 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:31.945638: step 175940, loss = 0.25 (7941.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:32.264524: step 175950, loss = 0.15 (8109.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:32.586143: step 175960, loss = 0.16 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:32.906087: step 175970, loss = 0.21 (7924.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:33.225510: step 175980, loss = 0.16 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:33.546850: step 175990, loss = 0.22 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:33.866290: step 176000, loss = 0.26 (8106.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:34.427344: step 176010, loss = 0.19 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:34.746042: step 176020, loss = 0.22 (8044.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:35.066574: step 176030, loss = 0.25 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:35.388163: step 176040, loss = 0.26 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:35.707389: step 176050, loss = 0.19 (8107.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:36.029214: step 176060, loss = 0.18 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:36.347762: step 176070, loss = 0.26 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:36.667563: step 176080, loss = 0.16 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:36.990269: step 176090, loss = 0.22 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:37.310343: step 176100, loss = 0.21 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:37.778063: step 176110, loss = 0.21 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:38.098292: step 176120, loss = 0.18 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:38.416887: step 176130, loss = 0.18 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:38.735300: step 176140, loss = 0.24 (8198.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:39.054406: step 176150, loss = 0.15 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:39.373933: step 176160, loss = 0.16 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:39.695891: step 176170, loss = 0.18 (7785.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:40.014514: step 176180, loss = 0.25 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:40.335580: step 176190, loss = 0.19 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:40.653511: step 176200, loss = 0.24 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:41.106316: step 176210, loss = 0.17 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:41.427362: step 176220, loss = 0.21 (7813.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:41.746265: step 176230, loss = 0.20 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:42.066220: step 176240, loss = 0.29 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:42.387518: step 176250, loss = 0.22 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:42.708370: step 176260, loss = 0.20 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:43.029121: step 176270, loss = 0.22 (7953.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:43.347892: step 176280, loss = 0.16 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:43.666438: step 176290, loss = 0.18 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:43.985638: step 176300, loss = 0.22 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:44.457179: step 176310, loss = 0.19 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:44.782815: step 176320, loss = 0.33 (7538.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:45.102959: step 176330, loss = 0.17 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:45.423188: step 176340, loss = 0.25 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:45.743174: step 176350, loss = 0.22 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:46.061630: step 176360, loss = 0.27 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:46.379847: step 176370, loss = 0.22 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:46.699284: step 176380, loss = 0.19 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:47.022905: step 176390, loss = 0.26 (7469.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:47.342943: step 176400, loss = 0.23 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:47.794446: step 176410, loss = 0.19 (8128.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:48.113022: step 176420, loss = 0.22 (8105.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:48.432666: step 176430, loss = 0.22 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:48.752503: step 176440, loss = 0.23 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:49.071446: step 176450, loss = 0.24 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:49.392467: step 176460, loss = 0.20 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:49.715349: step 176470, loss = 0.17 (7500.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:18:50.037506: step 176480, loss = 0.22 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:50.356778: step 176490, loss = 0.17 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:50.676520: step 176500, loss = 0.21 (7913.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:51.125515: step 176510, loss = 0.17 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:51.445836: step 176520, loss = 0.15 (8031.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:51.765494: step 176530, loss = 0.16 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:52.084286: step 176540, loss = 0.29 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:52.405817: step 176550, loss = 0.25 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:52.725031: step 176560, loss = 0.19 (7887.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:53.044360: step 176570, loss = 0.24 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:53.364908: step 176580, loss = 0.21 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:53.685232: step 176590, loss = 0.25 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:54.005877: step 176600, loss = 0.18 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:54.470753: step 176610, loss = 0.23 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:54.790092: step 176620, loss = 0.21 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:55.110282: step 176630, loss = 0.16 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:55.429240: step 176640, loss = 0.21 (8111.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:55.750200: step 176650, loss = 0.28 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:56.070757: step 176660, loss = 0.22 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:56.391524: step 176670, loss = 0.25 (8122.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:56.709166: step 176680, loss = 0.14 (8121.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:57.027488: step 176690, loss = 0.19 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:57.349080: step 176700, loss = 0.16 (7855.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:57.811506: step 176710, loss = 0.20 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:58.132173: step 176720, loss = 0.24 (7937.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:58.451764: step 176730, loss = 0.22 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:58.769992: step 176740, loss = 0.16 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:59.090824: step 176750, loss = 0.22 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:59.407692: step 176760, loss = 0.21 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:18:59.727086: step 176770, loss = 0.21 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:00.049583: step 176780, loss = 0.18 (7920.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:00.370233: step 176790, loss = 0.21 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:00.690570: step 176800, loss = 0.28 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:01.144010: step 176810, loss = 0.29 (8118.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:01.462473: step 176820, loss = 0.25 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:01.783030: step 176830, loss = 0.29 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:02.101671: step 176840, loss = 0.19 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:02.420968: step 176850, loss = 0.21 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:02.739845: step 176860, loss = 0.16 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:03.060194: step 176870, loss = 0.26 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:03.379693: step 176880, loss = 0.20 (8054.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:03.702207: step 176890, loss = 0.26 (7858.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:04.022029: step 176900, loss = 0.23 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:04.488099: step 176910, loss = 0.23 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:04.807117: step 176920, loss = 0.21 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:05.125658: step 176930, loss = 0.18 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:05.443426: step 176940, loss = 0.27 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:05.766120: step 176950, loss = 0.24 (7898.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:06.089581: step 176960, loss = 0.22 (8022.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:06.408471: step 176970, loss = 0.20 (8079.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:06.730293: step 176980, loss = 0.18 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:07.049012: step 176990, loss = 0.22 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:07.369898: step 177000, loss = 0.18 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:07.930679: step 177010, loss = 0.21 (7942.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:08.249220: step 177020, loss = 0.26 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:08.568311: step 177030, loss = 0.18 (8110.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:08.886436: step 177040, loss = 0.19 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:09.206834: step 177050, loss = 0.23 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:09.526840: step 177060, loss = 0.26 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:09.851236: step 177070, loss = 0.27 (8003.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:10.171379: step 177080, loss = 0.22 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:10.494478: step 177090, loss = 0.20 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:10.815487: step 177100, loss = 0.20 (7805.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:11.273575: step 177110, loss = 0.18 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:11.591997: step 177120, loss = 0.24 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:11.911305: step 177130, loss = 0.23 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:12.233203: step 177140, loss = 0.21 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:12.554163: step 177150, loss = 0.21 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:12.875866: step 177160, loss = 0.19 (7613.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:19:13.195488: step 177170, loss = 0.23 (8115.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:13.516441: step 177180, loss = 0.25 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:13.835180: step 177190, loss = 0.20 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:14.156117: step 177200, loss = 0.17 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:14.609527: step 177210, loss = 0.25 (8064.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:14.934590: step 177220, loss = 0.20 (7882.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:15.254416: step 177230, loss = 0.41 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:15.575080: step 177240, loss = 0.24 (7883.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:15.897012: step 177250, loss = 0.21 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:16.215573: step 177260, loss = 0.20 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:16.535296: step 177270, loss = 0.22 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:16.856444: step 177280, loss = 0.17 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:17.177130: step 177290, loss = 0.21 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:17.497162: step 177300, loss = 0.18 (8010.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:17.959167: step 177310, loss = 0.18 (7405.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:19:18.284126: step 177320, loss = 0.20 (7946.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:18.604535: step 177330, loss = 0.21 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:18.923663: step 177340, loss = 0.20 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:19.243535: step 177350, loss = 0.17 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:19.568171: step 177360, loss = 0.25 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:19.887247: step 177370, loss = 0.22 (7913.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:20.210565: step 177380, loss = 0.26 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:20.532367: step 177390, loss = 0.24 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:20.852650: step 177400, loss = 0.15 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:21.321476: step 177410, loss = 0.23 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:21.640874: step 177420, loss = 0.25 (8118.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:21.960655: step 177430, loss = 0.26 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:22.281133: step 177440, loss = 0.22 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:22.604014: step 177450, loss = 0.18 (8129.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:22.925204: step 177460, loss = 0.18 (7795.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:23.249040: step 177470, loss = 0.17 (7801.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:23.569876: step 177480, loss = 0.19 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:23.888839: step 177490, loss = 0.19 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:24.209226: step 177500, loss = 0.19 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:24.668938: step 177510, loss = 0.25 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:24.990948: step 177520, loss = 0.26 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:25.312655: step 177530, loss = 0.18 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:25.631649: step 177540, loss = 0.24 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:25.951619: step 177550, loss = 0.23 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:26.272211: step 177560, loss = 0.26 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:26.592135: step 177570, loss = 0.16 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:26.911881: step 177580, loss = 0.22 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:27.230779: step 177590, loss = 0.22 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:27.551861: step 177600, loss = 0.23 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:28.008517: step 177610, loss = 0.15 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:28.327001: step 177620, loss = 0.15 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:28.646502: step 177630, loss = 0.17 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:28.966560: step 177640, loss = 0.20 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:29.286218: step 177650, loss = 0.18 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:29.609537: step 177660, loss = 0.18 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:29.929159: step 177670, loss = 0.26 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:30.253096: step 177680, loss = 0.21 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:30.570823: step 177690, loss = 0.27 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:30.890997: step 177700, loss = 0.19 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:31.343195: step 177710, loss = 0.21 (8153.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:31.665627: step 177720, loss = 0.25 (7880.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:31.984848: step 177730, loss = 0.20 (7986.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:32.305753: step 177740, loss = 0.19 (8094.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:32.626075: step 177750, loss = 0.16 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:32.946357: step 177760, loss = 0.22 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:33.267202: step 177770, loss = 0.29 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:33.586638: step 177780, loss = 0.21 (8034.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:33.905702: step 177790, loss = 0.20 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:34.227668: step 177800, loss = 0.26 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:34.677397: step 177810, loss = 0.25 (8123.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:34.995624: step 177820, loss = 0.15 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:35.314222: step 177830, loss = 0.24 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:35.633438: step 177840, loss = 0.16 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:35.951662: step 177850, loss = 0.20 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:36.269818: step 177860, loss = 0.26 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:36.589432: step 177870, loss = 0.28 (8051.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:36.911273: step 177880, loss = 0.21 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:37.229919: step 177890, loss = 0.17 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:37.548155: step 177900, loss = 0.19 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:38.011252: step 177910, loss = 0.20 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:38.329401: step 177920, loss = 0.26 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:38.650571: step 177930, loss = 0.19 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:38.971967: step 177940, loss = 0.18 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:39.294049: step 177950, loss = 0.26 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:39.613492: step 177960, loss = 0.15 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:39.936469: step 177970, loss = 0.26 (7357.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:19:40.255914: step 177980, loss = 0.22 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:40.574874: step 177990, loss = 0.27 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:40.892562: step 178000, loss = 0.28 (8079.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:41.502772: step 178010, loss = 0.18 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:41.822864: step 178020, loss = 0.27 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:42.143372: step 178030, loss = 0.21 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:42.462142: step 178040, loss = 0.20 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:42.780874: step 178050, loss = 0.18 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:43.100148: step 178060, loss = 0.27 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:43.421114: step 178070, loss = 0.18 (7930.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:43.741520: step 178080, loss = 0.29 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:44.062771: step 178090, loss = 0.26 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:44.382359: step 178100, loss = 0.28 (7990.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:44.842195: step 178110, loss = 0.22 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:45.163863: step 178120, loss = 0.26 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:45.484824: step 178130, loss = 0.22 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:45.805277: step 178140, loss = 0.19 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:46.125534: step 178150, loss = 0.17 (7821.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:46.445947: step 178160, loss = 0.18 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:46.764571: step 178170, loss = 0.25 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:47.088107: step 178180, loss = 0.17 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:47.408579: step 178190, loss = 0.26 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:47.727129: step 178200, loss = 0.17 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:48.181024: step 178210, loss = 0.22 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:48.501121: step 178220, loss = 0.26 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:48.822352: step 178230, loss = 0.23 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:49.142256: step 178240, loss = 0.25 (7865.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:49.464106: step 178250, loss = 0.28 (7827.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:49.784411: step 178260, loss = 0.20 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:50.107906: step 178270, loss = 0.19 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:50.428324: step 178280, loss = 0.17 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:50.747842: step 178290, loss = 0.19 (8102.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:51.069861: step 178300, loss = 0.22 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:51.520587: step 178310, loss = 0.22 (7958.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:51.841899: step 178320, loss = 0.27 (7967.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:52.165139: step 178330, loss = 0.18 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:52.484510: step 178340, loss = 0.21 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:52.811951: step 178350, loss = 0.27 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:53.132378: step 178360, loss = 0.30 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:53.452589: step 178370, loss = 0.24 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:53.774188: step 178380, loss = 0.18 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:54.093226: step 178390, loss = 0.20 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:54.412263: step 178400, loss = 0.21 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:54.884366: step 178410, loss = 0.19 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:55.203524: step 178420, loss = 0.23 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:55.524158: step 178430, loss = 0.18 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:55.843099: step 178440, loss = 0.23 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:56.162661: step 178450, loss = 0.25 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:56.481447: step 178460, loss = 0.22 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:56.801054: step 178470, loss = 0.16 (7923.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:57.121081: step 178480, loss = 0.19 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:57.442200: step 178490, loss = 0.26 (7860.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:57.763686: step 178500, loss = 0.18 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:58.225209: step 178510, loss = 0.24 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:58.546066: step 178520, loss = 0.31 (8094.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:58.866567: step 178530, loss = 0.20 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:59.187591: step 178540, loss = 0.24 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:59.505951: step 178550, loss = 0.19 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:19:59.826344: step 178560, loss = 0.17 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:00.146831: step 178570, loss = 0.20 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:00.465359: step 178580, loss = 0.19 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:00.786078: step 178590, loss = 0.33 (7875.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:01.105491: step 178600, loss = 0.19 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:01.557705: step 178610, loss = 0.17 (8035.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:01.876777: step 178620, loss = 0.19 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:02.195066: step 178630, loss = 0.25 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:02.514600: step 178640, loss = 0.24 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:02.834886: step 178650, loss = 0.29 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:03.155806: step 178660, loss = 0.19 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:03.475956: step 178670, loss = 0.20 (7818.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:03.795436: step 178680, loss = 0.21 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:04.115142: step 178690, loss = 0.23 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:04.435807: step 178700, loss = 0.22 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:04.891890: step 178710, loss = 0.22 (7120.2 examples/sec; 0.018 sec/batch)
2017-09-16 17:20:05.210619: step 178720, loss = 0.24 (7931.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:05.530478: step 178730, loss = 0.22 (8041.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:05.855171: step 178740, loss = 0.22 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:06.175828: step 178750, loss = 0.26 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:06.496103: step 178760, loss = 0.25 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:06.814952: step 178770, loss = 0.25 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:07.136114: step 178780, loss = 0.27 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:07.456871: step 178790, loss = 0.23 (7816.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:07.779103: step 178800, loss = 0.21 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:08.244070: step 178810, loss = 0.23 (7812.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:08.567349: step 178820, loss = 0.27 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:08.888523: step 178830, loss = 0.23 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:09.206448: step 178840, loss = 0.17 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:09.527626: step 178850, loss = 0.23 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:09.847475: step 178860, loss = 0.27 (8149.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:10.167202: step 178870, loss = 0.20 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:10.486850: step 178880, loss = 0.19 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:10.806604: step 178890, loss = 0.21 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:11.127373: step 178900, loss = 0.19 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:11.588400: step 178910, loss = 0.21 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:11.908971: step 178920, loss = 0.17 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:12.230074: step 178930, loss = 0.20 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:12.549536: step 178940, loss = 0.18 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:12.871618: step 178950, loss = 0.26 (8114.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:13.192825: step 178960, loss = 0.21 (8032.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:13.512077: step 178970, loss = 0.21 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:13.831258: step 178980, loss = 0.21 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:14.149377: step 178990, loss = 0.24 (7900.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:14.470166: step 179000, loss = 0.29 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:15.036625: step 179010, loss = 0.24 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:15.357601: step 179020, loss = 0.15 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:15.679153: step 179030, loss = 0.23 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:15.998092: step 179040, loss = 0.25 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:16.316358: step 179050, loss = 0.22 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:16.635121: step 179060, loss = 0.23 (7940.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:16.956992: step 179070, loss = 0.25 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:17.279014: step 179080, loss = 0.19 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:17.600681: step 179090, loss = 0.24 (7971.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:17.921855: step 179100, loss = 0.21 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:18.388825: step 179110, loss = 0.27 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:18.709251: step 179120, loss = 0.20 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:19.028123: step 179130, loss = 0.18 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:19.350796: step 179140, loss = 0.18 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:19.671101: step 179150, loss = 0.21 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:19.991408: step 179160, loss = 0.20 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:20.312757: step 179170, loss = 0.29 (7931.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:20.635104: step 179180, loss = 0.21 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:20.955126: step 179190, loss = 0.22 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:21.274652: step 179200, loss = 0.19 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:21.740060: step 179210, loss = 0.27 (7882.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:22.065094: step 179220, loss = 0.22 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:22.384531: step 179230, loss = 0.21 (8137.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:22.709400: step 179240, loss = 0.22 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:23.028819: step 179250, loss = 0.27 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:23.348737: step 179260, loss = 0.16 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:23.668885: step 179270, loss = 0.19 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:23.988159: step 179280, loss = 0.23 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:24.307913: step 179290, loss = 0.21 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:24.630362: step 179300, loss = 0.21 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:25.091090: step 179310, loss = 0.26 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:25.411534: step 179320, loss = 0.29 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:25.730321: step 179330, loss = 0.25 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:26.049787: step 179340, loss = 0.18 (7974.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:26.368167: step 179350, loss = 0.21 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:26.690053: step 179360, loss = 0.22 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:27.010590: step 179370, loss = 0.18 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:27.331182: step 179380, loss = 0.33 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:27.650088: step 179390, loss = 0.22 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:27.969321: step 179400, loss = 0.28 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:28.434716: step 179410, loss = 0.17 (7698.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:20:28.761068: step 179420, loss = 0.23 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:29.080191: step 179430, loss = 0.19 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:29.399920: step 179440, loss = 0.18 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:29.720191: step 179450, loss = 0.26 (8026.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:30.042580: step 179460, loss = 0.20 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:30.362559: step 179470, loss = 0.16 (8140.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:30.680389: step 179480, loss = 0.15 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:31.000701: step 179490, loss = 0.17 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:31.319107: step 179500, loss = 0.24 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:31.772410: step 179510, loss = 0.17 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:32.093903: step 179520, loss = 0.22 (7868.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:32.414431: step 179530, loss = 0.23 (8069.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:32.735611: step 179540, loss = 0.19 (7783.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:33.055164: step 179550, loss = 0.18 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:33.375271: step 179560, loss = 0.25 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:33.695158: step 179570, loss = 0.19 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:34.014679: step 179580, loss = 0.19 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:34.333187: step 179590, loss = 0.20 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:34.654959: step 179600, loss = 0.22 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:35.114946: step 179610, loss = 0.25 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:35.434593: step 179620, loss = 0.20 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:35.755464: step 179630, loss = 0.19 (8114.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:36.075452: step 179640, loss = 0.20 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:36.395981: step 179650, loss = 0.17 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:36.716323: step 179660, loss = 0.19 (8096.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:37.037004: step 179670, loss = 0.22 (7764.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:37.354981: step 179680, loss = 0.19 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:37.675927: step 179690, loss = 0.21 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:37.997094: step 179700, loss = 0.19 (7899.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:38.457005: step 179710, loss = 0.21 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:38.777802: step 179720, loss = 0.21 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:39.097721: step 179730, loss = 0.19 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:39.418869: step 179740, loss = 0.19 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:39.737734: step 179750, loss = 0.19 (8146.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:40.058102: step 179760, loss = 0.19 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:40.379732: step 179770, loss = 0.20 (7940.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:40.701436: step 179780, loss = 0.24 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:41.021407: step 179790, loss = 0.26 (7856.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:41.343368: step 179800, loss = 0.19 (8094.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:41.810987: step 179810, loss = 0.29 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:42.133412: step 179820, loss = 0.20 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:42.453742: step 179830, loss = 0.31 (7891.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:42.773076: step 179840, loss = 0.30 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:43.092576: step 179850, loss = 0.19 (7912.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:43.413506: step 179860, loss = 0.20 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:43.737268: step 179870, loss = 0.24 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:44.056869: step 179880, loss = 0.17 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:44.376654: step 179890, loss = 0.20 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:44.698706: step 179900, loss = 0.26 (7941.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:45.154971: step 179910, loss = 0.26 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:45.474360: step 179920, loss = 0.17 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:45.797211: step 179930, loss = 0.21 (7945.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:46.119556: step 179940, loss = 0.17 (7927.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:46.438485: step 179950, loss = 0.18 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:46.764685: step 179960, loss = 0.21 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:47.086218: step 179970, loss = 0.29 (7808.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:47.406823: step 179980, loss = 0.19 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:47.728458: step 179990, loss = 0.22 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:48.046475: step 180000, loss = 0.23 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:48.605974: step 180010, loss = 0.24 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:48.930829: step 180020, loss = 0.32 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:49.251250: step 180030, loss = 0.22 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:49.578028: step 180040, loss = 0.25 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:49.897581: step 180050, loss = 0.19 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:50.218071: step 180060, loss = 0.26 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:50.537666: step 180070, loss = 0.18 (7836.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:50.860736: step 180080, loss = 0.19 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:51.182004: step 180090, loss = 0.24 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:51.502153: step 180100, loss = 0.19 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:51.964159: step 180110, loss = 0.20 (8130.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:52.287287: step 180120, loss = 0.23 (8125.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:52.606083: step 180130, loss = 0.22 (7854.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:52.927482: step 180140, loss = 0.24 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:53.247801: step 180150, loss = 0.23 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:53.568799: step 180160, loss = 0.20 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:53.887473: step 180170, loss = 0.21 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:54.208691: step 180180, loss = 0.23 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:54.526572: step 180190, loss = 0.24 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:54.845573: step 180200, loss = 0.19 (8128.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:55.313530: step 180210, loss = 0.27 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:55.636887: step 180220, loss = 0.22 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:55.956183: step 180230, loss = 0.17 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:56.277065: step 180240, loss = 0.19 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:56.596685: step 180250, loss = 0.20 (8082.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:56.916839: step 180260, loss = 0.21 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:57.235495: step 180270, loss = 0.18 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:57.557991: step 180280, loss = 0.22 (7577.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:20:57.878652: step 180290, loss = 0.20 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:58.198074: step 180300, loss = 0.28 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:58.664355: step 180310, loss = 0.17 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:58.983163: step 180320, loss = 0.29 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:59.300673: step 180330, loss = 0.18 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:59.620901: step 180340, loss = 0.22 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:20:59.942993: step 180350, loss = 0.17 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:00.262855: step 180360, loss = 0.25 (7853.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:00.581983: step 180370, loss = 0.23 (8069.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:00.900267: step 180380, loss = 0.21 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:01.219707: step 180390, loss = 0.23 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:01.540366: step 180400, loss = 0.21 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:02.004260: step 180410, loss = 0.22 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:02.324889: step 180420, loss = 0.17 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:02.644845: step 180430, loss = 0.14 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:02.965075: step 180440, loss = 0.22 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:03.288354: step 180450, loss = 0.19 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:03.606976: step 180460, loss = 0.22 (8083.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:03.925661: step 180470, loss = 0.23 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:04.246578: step 180480, loss = 0.29 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:04.569065: step 180490, loss = 0.22 (8059.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:04.894110: step 180500, loss = 0.17 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:05.346198: step 180510, loss = 0.26 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:05.665291: step 180520, loss = 0.22 (7995.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:05.986524: step 180530, loss = 0.26 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:06.306976: step 180540, loss = 0.23 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:06.625942: step 180550, loss = 0.22 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:06.946937: step 180560, loss = 0.17 (7904.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:07.266966: step 180570, loss = 0.21 (8057.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:07.585987: step 180580, loss = 0.15 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:07.903215: step 180590, loss = 0.22 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:08.222255: step 180600, loss = 0.21 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:08.686773: step 180610, loss = 0.21 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:09.008709: step 180620, loss = 0.23 (7847.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:09.327759: step 180630, loss = 0.16 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:09.648264: step 180640, loss = 0.20 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:09.968455: step 180650, loss = 0.26 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:10.289316: step 180660, loss = 0.20 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:10.609263: step 180670, loss = 0.20 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:10.928994: step 180680, loss = 0.17 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:11.248313: step 180690, loss = 0.17 (8104.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:11.570057: step 180700, loss = 0.18 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:12.020239: step 180710, loss = 0.31 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:12.340868: step 180720, loss = 0.19 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:12.660543: step 180730, loss = 0.16 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:12.980069: step 180740, loss = 0.22 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:13.298928: step 180750, loss = 0.20 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:13.620655: step 180760, loss = 0.25 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:13.942851: step 180770, loss = 0.15 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:14.261684: step 180780, loss = 0.19 (8142.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:14.582161: step 180790, loss = 0.20 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:14.905624: step 180800, loss = 0.17 (7398.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:15.353959: step 180810, loss = 0.17 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:15.674173: step 180820, loss = 0.21 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:15.993499: step 180830, loss = 0.23 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:16.313104: step 180840, loss = 0.16 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:16.631433: step 180850, loss = 0.23 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:16.950681: step 180860, loss = 0.33 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:17.271790: step 180870, loss = 0.23 (7637.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:17.592311: step 180880, loss = 0.23 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:17.914275: step 180890, loss = 0.14 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:18.235571: step 180900, loss = 0.24 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:18.697847: step 180910, loss = 0.23 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:19.018926: step 180920, loss = 0.27 (7797.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:19.338791: step 180930, loss = 0.25 (7902.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:19.657869: step 180940, loss = 0.16 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:19.974529: step 180950, loss = 0.28 (8049.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:20.293464: step 180960, loss = 0.20 (8147.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:20.614671: step 180970, loss = 0.18 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:20.934725: step 180980, loss = 0.23 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:21.254337: step 180990, loss = 0.19 (8142.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:21.575176: step 181000, loss = 0.21 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:22.140314: step 181010, loss = 0.16 (7921.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:22.459695: step 181020, loss = 0.23 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:22.781634: step 181030, loss = 0.22 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:23.101526: step 181040, loss = 0.22 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:23.420912: step 181050, loss = 0.22 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:23.742929: step 181060, loss = 0.17 (7912.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:24.063253: step 181070, loss = 0.24 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:24.385498: step 181080, loss = 0.19 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:24.706182: step 181090, loss = 0.25 (8140.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:25.025755: step 181100, loss = 0.23 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:25.476748: step 181110, loss = 0.16 (7951.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:25.796906: step 181120, loss = 0.18 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:26.116214: step 181130, loss = 0.20 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:26.437648: step 181140, loss = 0.17 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:26.756585: step 181150, loss = 0.14 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:27.074354: step 181160, loss = 0.27 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:27.394840: step 181170, loss = 0.20 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:27.716558: step 181180, loss = 0.19 (7439.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:28.038439: step 181190, loss = 0.21 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:28.363597: step 181200, loss = 0.18 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:28.828961: step 181210, loss = 0.16 (7820.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:29.148100: step 181220, loss = 0.20 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:29.469623: step 181230, loss = 0.18 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:29.786534: step 181240, loss = 0.17 (8155.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:30.111209: step 181250, loss = 0.20 (7762.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:30.440108: step 181260, loss = 0.23 (7772.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:30.764280: step 181270, loss = 0.14 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:31.083112: step 181280, loss = 0.21 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:31.402676: step 181290, loss = 0.30 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:31.722201: step 181300, loss = 0.17 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:32.187124: step 181310, loss = 0.19 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:32.506687: step 181320, loss = 0.19 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:32.826987: step 181330, loss = 0.21 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:33.149103: step 181340, loss = 0.21 (8159.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:33.469404: step 181350, loss = 0.27 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:33.790456: step 181360, loss = 0.24 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:34.107631: step 181370, loss = 0.21 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:34.427914: step 181380, loss = 0.18 (7943.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:34.748803: step 181390, loss = 0.18 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:35.067612: step 181400, loss = 0.19 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:35.518168: step 181410, loss = 0.20 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:35.839218: step 181420, loss = 0.18 (7867.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:36.160156: step 181430, loss = 0.18 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:36.480458: step 181440, loss = 0.24 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:36.799770: step 181450, loss = 0.30 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:37.119586: step 181460, loss = 0.23 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:37.439971: step 181470, loss = 0.21 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:37.761006: step 181480, loss = 0.18 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:38.082768: step 181490, loss = 0.23 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:38.403614: step 181500, loss = 0.23 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:38.867087: step 181510, loss = 0.19 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:39.186362: step 181520, loss = 0.29 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:39.506419: step 181530, loss = 0.19 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:39.826552: step 181540, loss = 0.17 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:40.145559: step 181550, loss = 0.18 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:40.464636: step 181560, loss = 0.26 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:40.788289: step 181570, loss = 0.33 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:41.107769: step 181580, loss = 0.21 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:41.430985: step 181590, loss = 0.16 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:41.751672: step 181600, loss = 0.18 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:42.212682: step 181610, loss = 0.22 (8137.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:42.534260: step 181620, loss = 0.20 (7823.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:42.854056: step 181630, loss = 0.26 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:43.174551: step 181640, loss = 0.19 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:43.497309: step 181650, loss = 0.25 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:43.816608: step 181660, loss = 0.20 (7957.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:44.139959: step 181670, loss = 0.23 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:44.461667: step 181680, loss = 0.22 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:44.782205: step 181690, loss = 0.21 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:45.102745: step 181700, loss = 0.17 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:45.566399: step 181710, loss = 0.30 (7984.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:45.889891: step 181720, loss = 0.20 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:46.211955: step 181730, loss = 0.24 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:46.530166: step 181740, loss = 0.21 (8172.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:46.849396: step 181750, loss = 0.22 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:47.168238: step 181760, loss = 0.26 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:47.487255: step 181770, loss = 0.19 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:47.807688: step 181780, loss = 0.24 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:48.126735: step 181790, loss = 0.24 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:48.445899: step 181800, loss = 0.15 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:48.905495: step 181810, loss = 0.15 (7433.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:49.226050: step 181820, loss = 0.18 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:49.548324: step 181830, loss = 0.22 (7767.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:49.867713: step 181840, loss = 0.19 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:50.189370: step 181850, loss = 0.16 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:50.510615: step 181860, loss = 0.30 (7850.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:50.829743: step 181870, loss = 0.24 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:51.148960: step 181880, loss = 0.23 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:51.470332: step 181890, loss = 0.23 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:51.793151: step 181900, loss = 0.20 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:52.263432: step 181910, loss = 0.20 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:52.584060: step 181920, loss = 0.25 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:52.905353: step 181930, loss = 0.24 (7850.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:53.224540: step 181940, loss = 0.17 (7951.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:53.549616: step 181950, loss = 0.18 (7590.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:53.869429: step 181960, loss = 0.25 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:54.192135: step 181970, loss = 0.15 (7673.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:21:54.510222: step 181980, loss = 0.26 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:54.829014: step 181990, loss = 0.28 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:55.148970: step 182000, loss = 0.22 (7950.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:55.714511: step 182010, loss = 0.20 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:56.033900: step 182020, loss = 0.29 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:56.353594: step 182030, loss = 0.18 (8185.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:56.673432: step 182040, loss = 0.16 (7823.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:56.993941: step 182050, loss = 0.23 (7914.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:57.315321: step 182060, loss = 0.24 (7918.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:57.633928: step 182070, loss = 0.21 (8179.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:57.952951: step 182080, loss = 0.17 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:58.270854: step 182090, loss = 0.18 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:58.590034: step 182100, loss = 0.15 (8004.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:59.046972: step 182110, loss = 0.25 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:59.366677: step 182120, loss = 0.16 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:21:59.685642: step 182130, loss = 0.16 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:00.005631: step 182140, loss = 0.23 (7983.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:00.326382: step 182150, loss = 0.21 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:00.646874: step 182160, loss = 0.21 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:00.967350: step 182170, loss = 0.23 (7822.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:01.286065: step 182180, loss = 0.19 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:01.603962: step 182190, loss = 0.23 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:01.923091: step 182200, loss = 0.19 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:02.385811: step 182210, loss = 0.22 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:02.708558: step 182220, loss = 0.31 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:03.028488: step 182230, loss = 0.21 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:03.351892: step 182240, loss = 0.20 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:03.671987: step 182250, loss = 0.20 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:03.991214: step 182260, loss = 0.22 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:04.318966: step 182270, loss = 0.23 (7801.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:04.638571: step 182280, loss = 0.20 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:04.957282: step 182290, loss = 0.23 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:05.278890: step 182300, loss = 0.26 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:05.741175: step 182310, loss = 0.25 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:06.059393: step 182320, loss = 0.21 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:06.380531: step 182330, loss = 0.19 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:06.698857: step 182340, loss = 0.18 (8115.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:07.018953: step 182350, loss = 0.26 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:07.338351: step 182360, loss = 0.20 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:07.657034: step 182370, loss = 0.23 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:07.976611: step 182380, loss = 0.15 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:08.296500: step 182390, loss = 0.20 (8092.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:08.616178: step 182400, loss = 0.20 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:09.077624: step 182410, loss = 0.24 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:09.397380: step 182420, loss = 0.20 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:09.719262: step 182430, loss = 0.17 (7630.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:10.043195: step 182440, loss = 0.20 (8010.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:10.362586: step 182450, loss = 0.22 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:10.682767: step 182460, loss = 0.21 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:11.005148: step 182470, loss = 0.30 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:11.324239: step 182480, loss = 0.18 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:11.645544: step 182490, loss = 0.22 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:11.965484: step 182500, loss = 0.25 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:12.418279: step 182510, loss = 0.19 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:12.739875: step 182520, loss = 0.25 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:13.060967: step 182530, loss = 0.20 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:13.382426: step 182540, loss = 0.37 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:13.703323: step 182550, loss = 0.19 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:14.024520: step 182560, loss = 0.20 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:14.344007: step 182570, loss = 0.20 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:14.665646: step 182580, loss = 0.20 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:14.989851: step 182590, loss = 0.22 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:15.308035: step 182600, loss = 0.22 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:15.755312: step 182610, loss = 0.38 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:16.073212: step 182620, loss = 0.21 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:16.392724: step 182630, loss = 0.17 (8104.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:16.711927: step 182640, loss = 0.21 (8063.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:17.031133: step 182650, loss = 0.22 (8163.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:17.349128: step 182660, loss = 0.22 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:17.672011: step 182670, loss = 0.24 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:17.994822: step 182680, loss = 0.19 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:18.317206: step 182690, loss = 0.19 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:18.636709: step 182700, loss = 0.23 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:19.091787: step 182710, loss = 0.21 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:19.412836: step 182720, loss = 0.19 (7801.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:19.733985: step 182730, loss = 0.25 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:20.054557: step 182740, loss = 0.25 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:20.374808: step 182750, loss = 0.30 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:20.698719: step 182760, loss = 0.17 (7597.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:21.018349: step 182770, loss = 0.23 (8136.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:21.340388: step 182780, loss = 0.31 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:21.659137: step 182790, loss = 0.20 (7752.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:21.981035: step 182800, loss = 0.22 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:22.443579: step 182810, loss = 0.19 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:22.763287: step 182820, loss = 0.17 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:23.084542: step 182830, loss = 0.24 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:23.405920: step 182840, loss = 0.25 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:23.726538: step 182850, loss = 0.18 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:24.048670: step 182860, loss = 0.23 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:24.367781: step 182870, loss = 0.27 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:24.686451: step 182880, loss = 0.21 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:25.005654: step 182890, loss = 0.21 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:25.329105: step 182900, loss = 0.15 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:25.795503: step 182910, loss = 0.33 (8179.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:26.121803: step 182920, loss = 0.22 (8059.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:26.440122: step 182930, loss = 0.26 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:26.761838: step 182940, loss = 0.16 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:27.081473: step 182950, loss = 0.17 (8035.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:27.401097: step 182960, loss = 0.18 (8133.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:27.721041: step 182970, loss = 0.19 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:28.039472: step 182980, loss = 0.21 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:28.360048: step 182990, loss = 0.20 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:28.684437: step 183000, loss = 0.17 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:29.251347: step 183010, loss = 0.25 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:29.570994: step 183020, loss = 0.25 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:29.890810: step 183030, loss = 0.18 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:30.210854: step 183040, loss = 0.23 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:30.529961: step 183050, loss = 0.26 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:30.849892: step 183060, loss = 0.18 (7815.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:31.171372: step 183070, loss = 0.18 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:31.490943: step 183080, loss = 0.23 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:31.813288: step 183090, loss = 0.18 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:32.132212: step 183100, loss = 0.25 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:32.595196: step 183110, loss = 0.18 (8105.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:32.919456: step 183120, loss = 0.18 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:33.241052: step 183130, loss = 0.19 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:33.560951: step 183140, loss = 0.18 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:33.880114: step 183150, loss = 0.24 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:34.199827: step 183160, loss = 0.21 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:34.522033: step 183170, loss = 0.20 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:34.844133: step 183180, loss = 0.18 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:35.165473: step 183190, loss = 0.17 (7985.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:35.488489: step 183200, loss = 0.22 (7954.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:35.948124: step 183210, loss = 0.19 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:36.271292: step 183220, loss = 0.24 (8036.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:36.590129: step 183230, loss = 0.18 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:36.909711: step 183240, loss = 0.19 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:37.230489: step 183250, loss = 0.21 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:37.548951: step 183260, loss = 0.25 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:37.870330: step 183270, loss = 0.30 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:38.193323: step 183280, loss = 0.24 (7888.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:38.514630: step 183290, loss = 0.21 (8126.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:38.834629: step 183300, loss = 0.28 (8000.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:39.288382: step 183310, loss = 0.21 (7960.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:39.610802: step 183320, loss = 0.25 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:39.933103: step 183330, loss = 0.22 (7567.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:40.255947: step 183340, loss = 0.19 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:40.575821: step 183350, loss = 0.18 (8116.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:40.895131: step 183360, loss = 0.27 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:41.213862: step 183370, loss = 0.18 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:41.538022: step 183380, loss = 0.24 (7677.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:41.858651: step 183390, loss = 0.20 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:42.178343: step 183400, loss = 0.23 (7936.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:42.630728: step 183410, loss = 0.25 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:42.951625: step 183420, loss = 0.24 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:43.272758: step 183430, loss = 0.30 (7670.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:43.601776: step 183440, loss = 0.29 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:43.923096: step 183450, loss = 0.18 (7837.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:44.244370: step 183460, loss = 0.16 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:44.564386: step 183470, loss = 0.24 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:44.881636: step 183480, loss = 0.16 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:45.202226: step 183490, loss = 0.16 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:45.522149: step 183500, loss = 0.21 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:45.979607: step 183510, loss = 0.20 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:46.300571: step 183520, loss = 0.14 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:46.622147: step 183530, loss = 0.21 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:46.943858: step 183540, loss = 0.22 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:47.262610: step 183550, loss = 0.25 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:47.584249: step 183560, loss = 0.27 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:47.904415: step 183570, loss = 0.21 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:48.224354: step 183580, loss = 0.16 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:48.546261: step 183590, loss = 0.20 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:48.867767: step 183600, loss = 0.24 (7527.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:22:49.335008: step 183610, loss = 0.17 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:49.658583: step 183620, loss = 0.24 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:49.977927: step 183630, loss = 0.30 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:50.296965: step 183640, loss = 0.16 (8129.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:50.616039: step 183650, loss = 0.15 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:50.935099: step 183660, loss = 0.21 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:51.258719: step 183670, loss = 0.16 (7805.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:51.580906: step 183680, loss = 0.28 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:51.901447: step 183690, loss = 0.30 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:52.223524: step 183700, loss = 0.20 (8161.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:52.691685: step 183710, loss = 0.17 (7905.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:53.014433: step 183720, loss = 0.19 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:53.337209: step 183730, loss = 0.19 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:53.656704: step 183740, loss = 0.18 (8038.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:53.976279: step 183750, loss = 0.16 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:54.293830: step 183760, loss = 0.18 (8151.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:54.613625: step 183770, loss = 0.21 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:54.932867: step 183780, loss = 0.22 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:55.254215: step 183790, loss = 0.19 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:55.573608: step 183800, loss = 0.24 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:56.038720: step 183810, loss = 0.20 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:56.357327: step 183820, loss = 0.19 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:56.677665: step 183830, loss = 0.26 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:56.997399: step 183840, loss = 0.16 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:57.318020: step 183850, loss = 0.17 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:57.639642: step 183860, loss = 0.22 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:57.959948: step 183870, loss = 0.26 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:58.282603: step 183880, loss = 0.18 (7920.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:58.604390: step 183890, loss = 0.26 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:58.923808: step 183900, loss = 0.23 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:59.397171: step 183910, loss = 0.20 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:22:59.719892: step 183920, loss = 0.27 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:00.042138: step 183930, loss = 0.18 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:00.366875: step 183940, loss = 0.21 (8091.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:00.690051: step 183950, loss = 0.19 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:01.012281: step 183960, loss = 0.22 (8001.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:01.331622: step 183970, loss = 0.18 (7857.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:01.653581: step 183980, loss = 0.26 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:01.972273: step 183990, loss = 0.26 (8001.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:02.292046: step 184000, loss = 0.21 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:02.849759: step 184010, loss = 0.18 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:03.168636: step 184020, loss = 0.17 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:03.489015: step 184030, loss = 0.26 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:03.810623: step 184040, loss = 0.20 (7834.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:04.132142: step 184050, loss = 0.19 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:04.454283: step 184060, loss = 0.22 (7849.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:04.774963: step 184070, loss = 0.23 (7973.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:05.094852: step 184080, loss = 0.25 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:05.415267: step 184090, loss = 0.18 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:05.735390: step 184100, loss = 0.22 (8122.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:06.198889: step 184110, loss = 0.18 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:06.522289: step 184120, loss = 0.22 (8116.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:06.843295: step 184130, loss = 0.16 (7856.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:07.162510: step 184140, loss = 0.23 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:07.482376: step 184150, loss = 0.19 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:07.801236: step 184160, loss = 0.18 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:08.120390: step 184170, loss = 0.23 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:08.441761: step 184180, loss = 0.22 (7976.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:08.761751: step 184190, loss = 0.23 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:09.081979: step 184200, loss = 0.21 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:09.535539: step 184210, loss = 0.23 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:09.857760: step 184220, loss = 0.23 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:10.179551: step 184230, loss = 0.17 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:10.500431: step 184240, loss = 0.21 (7796.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:10.822276: step 184250, loss = 0.21 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:11.143510: step 184260, loss = 0.21 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:11.466168: step 184270, loss = 0.24 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:11.785055: step 184280, loss = 0.20 (7949.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:12.102497: step 184290, loss = 0.24 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:12.422984: step 184300, loss = 0.21 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:12.884130: step 184310, loss = 0.19 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:13.208110: step 184320, loss = 0.22 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:13.525972: step 184330, loss = 0.29 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:13.846491: step 184340, loss = 0.18 (8066.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:14.166025: step 184350, loss = 0.29 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:14.490053: step 184360, loss = 0.22 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:14.810253: step 184370, loss = 0.23 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:15.128070: step 184380, loss = 0.20 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:15.446338: step 184390, loss = 0.21 (8049.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:15.766722: step 184400, loss = 0.18 (8026.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:16.221121: step 184410, loss = 0.31 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:16.539766: step 184420, loss = 0.23 (7837.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:16.858839: step 184430, loss = 0.24 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:17.180468: step 184440, loss = 0.17 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:17.501025: step 184450, loss = 0.26 (7957.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:17.819747: step 184460, loss = 0.16 (7926.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:18.139479: step 184470, loss = 0.21 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:18.459121: step 184480, loss = 0.19 (7722.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:18.779048: step 184490, loss = 0.18 (7864.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:19.099361: step 184500, loss = 0.22 (8126.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:19.551846: step 184510, loss = 0.21 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:19.871124: step 184520, loss = 0.17 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:20.190852: step 184530, loss = 0.23 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:20.510601: step 184540, loss = 0.22 (8065.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:20.831667: step 184550, loss = 0.20 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:21.153686: step 184560, loss = 0.26 (7611.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:21.473097: step 184570, loss = 0.26 (7941.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:21.793808: step 184580, loss = 0.24 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:22.112349: step 184590, loss = 0.21 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:22.431113: step 184600, loss = 0.22 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:22.889480: step 184610, loss = 0.22 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:23.213246: step 184620, loss = 0.23 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:23.532894: step 184630, loss = 0.22 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:23.854397: step 184640, loss = 0.24 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:24.174043: step 184650, loss = 0.30 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:24.494350: step 184660, loss = 0.21 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:24.813203: step 184670, loss = 0.19 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:25.133764: step 184680, loss = 0.22 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:25.452963: step 184690, loss = 0.26 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:25.772992: step 184700, loss = 0.29 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:26.235668: step 184710, loss = 0.15 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:26.556083: step 184720, loss = 0.18 (7961.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:26.878109: step 184730, loss = 0.20 (7922.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:27.197774: step 184740, loss = 0.20 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:27.518009: step 184750, loss = 0.21 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:27.837471: step 184760, loss = 0.17 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:28.159909: step 184770, loss = 0.17 (7708.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:28.482189: step 184780, loss = 0.20 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:28.802729: step 184790, loss = 0.21 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:29.128440: step 184800, loss = 0.20 (7538.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:29.591824: step 184810, loss = 0.18 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:29.913050: step 184820, loss = 0.21 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:30.234001: step 184830, loss = 0.23 (7915.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:30.555767: step 184840, loss = 0.24 (7925.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:30.876455: step 184850, loss = 0.26 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:31.197428: step 184860, loss = 0.21 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:31.519189: step 184870, loss = 0.18 (8137.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:31.845595: step 184880, loss = 0.21 (7852.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:32.168238: step 184890, loss = 0.21 (7455.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:32.490071: step 184900, loss = 0.22 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:32.956345: step 184910, loss = 0.17 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:33.275787: step 184920, loss = 0.21 (8070.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:33.596180: step 184930, loss = 0.16 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:33.919277: step 184940, loss = 0.19 (7481.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:34.240349: step 184950, loss = 0.30 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:34.559768: step 184960, loss = 0.17 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:34.880793: step 184970, loss = 0.19 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:35.199080: step 184980, loss = 0.20 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:35.518029: step 184990, loss = 0.27 (8068.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:35.838350: step 185000, loss = 0.21 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:36.436521: step 185010, loss = 0.22 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:36.756571: step 185020, loss = 0.20 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:37.081010: step 185030, loss = 0.20 (7975.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:37.404165: step 185040, loss = 0.21 (7993.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:37.724750: step 185050, loss = 0.24 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:38.044815: step 185060, loss = 0.18 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:38.364089: step 185070, loss = 0.24 (7954.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:38.688183: step 185080, loss = 0.16 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:39.008888: step 185090, loss = 0.22 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:39.330685: step 185100, loss = 0.23 (7829.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:39.790257: step 185110, loss = 0.20 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:40.109985: step 185120, loss = 0.20 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:40.432458: step 185130, loss = 0.19 (7908.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:40.756436: step 185140, loss = 0.25 (7956.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:41.075578: step 185150, loss = 0.22 (7925.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:41.395675: step 185160, loss = 0.18 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:41.715470: step 185170, loss = 0.25 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:42.038555: step 185180, loss = 0.25 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:42.360049: step 185190, loss = 0.27 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:42.681814: step 185200, loss = 0.25 (7682.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:43.146177: step 185210, loss = 0.26 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:43.464944: step 185220, loss = 0.19 (8104.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:43.786332: step 185230, loss = 0.15 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:44.108298: step 185240, loss = 0.29 (7764.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:44.429197: step 185250, loss = 0.19 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:44.749892: step 185260, loss = 0.20 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:45.070296: step 185270, loss = 0.23 (8010.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:45.391064: step 185280, loss = 0.17 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:45.711517: step 185290, loss = 0.25 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:46.031952: step 185300, loss = 0.24 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:46.481065: step 185310, loss = 0.17 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:46.802623: step 185320, loss = 0.19 (7980.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:47.122534: step 185330, loss = 0.26 (7715.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:47.443553: step 185340, loss = 0.18 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:47.763298: step 185350, loss = 0.22 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:48.082732: step 185360, loss = 0.21 (8074.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:48.402238: step 185370, loss = 0.23 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:48.729035: step 185380, loss = 0.23 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:49.053694: step 185390, loss = 0.34 (7590.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:49.376041: step 185400, loss = 0.19 (7728.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:49.837613: step 185410, loss = 0.23 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:50.158997: step 185420, loss = 0.19 (7799.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:50.481635: step 185430, loss = 0.23 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:50.801752: step 185440, loss = 0.22 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:51.121146: step 185450, loss = 0.18 (8094.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:51.439431: step 185460, loss = 0.20 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:51.761079: step 185470, loss = 0.16 (8060.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:52.084305: step 185480, loss = 0.23 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:52.404242: step 185490, loss = 0.18 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:52.724352: step 185500, loss = 0.17 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:53.186375: step 185510, loss = 0.18 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:53.509442: step 185520, loss = 0.20 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:53.829114: step 185530, loss = 0.22 (8129.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:54.148809: step 185540, loss = 0.20 (8106.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:54.472189: step 185550, loss = 0.31 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:54.795492: step 185560, loss = 0.21 (7539.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:55.114074: step 185570, loss = 0.20 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:55.434262: step 185580, loss = 0.24 (7919.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:55.757527: step 185590, loss = 0.22 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:56.075891: step 185600, loss = 0.23 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:56.539605: step 185610, loss = 0.21 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:56.858792: step 185620, loss = 0.15 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:57.182014: step 185630, loss = 0.25 (7355.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:23:57.504958: step 185640, loss = 0.15 (7895.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:57.826364: step 185650, loss = 0.22 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:58.147069: step 185660, loss = 0.20 (7843.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:58.472394: step 185670, loss = 0.17 (7878.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:58.792396: step 185680, loss = 0.18 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:59.112193: step 185690, loss = 0.23 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:59.435984: step 185700, loss = 0.22 (7928.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:23:59.888390: step 185710, loss = 0.17 (8084.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:00.208851: step 185720, loss = 0.25 (7883.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:00.531390: step 185730, loss = 0.19 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:00.853920: step 185740, loss = 0.28 (7901.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:01.175477: step 185750, loss = 0.20 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:01.497654: step 185760, loss = 0.25 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:01.819046: step 185770, loss = 0.23 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:02.139380: step 185780, loss = 0.21 (7937.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:02.461012: step 185790, loss = 0.19 (7939.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:02.780073: step 185800, loss = 0.20 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:03.237127: step 185810, loss = 0.18 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:03.556715: step 185820, loss = 0.23 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:03.876596: step 185830, loss = 0.21 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:04.195364: step 185840, loss = 0.19 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:04.515202: step 185850, loss = 0.20 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:04.835119: step 185860, loss = 0.20 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:05.154200: step 185870, loss = 0.21 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:05.477338: step 185880, loss = 0.21 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:05.797687: step 185890, loss = 0.22 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:06.118201: step 185900, loss = 0.22 (7821.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:06.575201: step 185910, loss = 0.23 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:06.895094: step 185920, loss = 0.20 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:07.213307: step 185930, loss = 0.23 (8128.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:07.536510: step 185940, loss = 0.18 (7878.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:07.857652: step 185950, loss = 0.21 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:08.178263: step 185960, loss = 0.29 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:08.500173: step 185970, loss = 0.22 (7875.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:08.825369: step 185980, loss = 0.24 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:09.147305: step 185990, loss = 0.21 (7850.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:09.465946: step 186000, loss = 0.22 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:10.027082: step 186010, loss = 0.20 (7836.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:10.350881: step 186020, loss = 0.18 (7828.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:10.671987: step 186030, loss = 0.20 (8078.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:10.993076: step 186040, loss = 0.21 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:11.315280: step 186050, loss = 0.17 (7532.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:11.635311: step 186060, loss = 0.20 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:11.955377: step 186070, loss = 0.16 (7918.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:12.274635: step 186080, loss = 0.20 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:12.593962: step 186090, loss = 0.24 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:12.914456: step 186100, loss = 0.24 (7940.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:13.379058: step 186110, loss = 0.20 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:13.698762: step 186120, loss = 0.18 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:14.019273: step 186130, loss = 0.27 (8057.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:14.338237: step 186140, loss = 0.18 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:14.660613: step 186150, loss = 0.22 (7854.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:14.979001: step 186160, loss = 0.25 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:15.299556: step 186170, loss = 0.21 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:15.619547: step 186180, loss = 0.17 (8141.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:15.938942: step 186190, loss = 0.36 (8169.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:16.257573: step 186200, loss = 0.22 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:16.707639: step 186210, loss = 0.23 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:17.026216: step 186220, loss = 0.24 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:17.346393: step 186230, loss = 0.31 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:17.667185: step 186240, loss = 0.20 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:17.988124: step 186250, loss = 0.20 (7911.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:18.307569: step 186260, loss = 0.19 (7960.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:18.627285: step 186270, loss = 0.21 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:18.947294: step 186280, loss = 0.19 (7920.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:19.268512: step 186290, loss = 0.22 (8020.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:19.587573: step 186300, loss = 0.26 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:20.049874: step 186310, loss = 0.21 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:20.370259: step 186320, loss = 0.23 (7826.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:20.689264: step 186330, loss = 0.21 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:21.011341: step 186340, loss = 0.20 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:21.329465: step 186350, loss = 0.23 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:21.650278: step 186360, loss = 0.24 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:21.968778: step 186370, loss = 0.21 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:22.289445: step 186380, loss = 0.25 (8092.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:22.609018: step 186390, loss = 0.21 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:22.927223: step 186400, loss = 0.21 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:23.388950: step 186410, loss = 0.21 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:23.709820: step 186420, loss = 0.23 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:24.029620: step 186430, loss = 0.21 (8046.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:24.349935: step 186440, loss = 0.28 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:24.670654: step 186450, loss = 0.25 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:24.989062: step 186460, loss = 0.27 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:25.309784: step 186470, loss = 0.22 (8017.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:25.631762: step 186480, loss = 0.23 (7786.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:25.956797: step 186490, loss = 0.16 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:26.275931: step 186500, loss = 0.19 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:26.726396: step 186510, loss = 0.19 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:27.046740: step 186520, loss = 0.25 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:27.365902: step 186530, loss = 0.22 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:27.685075: step 186540, loss = 0.19 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:28.006744: step 186550, loss = 0.26 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:28.326513: step 186560, loss = 0.17 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:28.649517: step 186570, loss = 0.15 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:28.967409: step 186580, loss = 0.18 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:29.288901: step 186590, loss = 0.18 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:29.608407: step 186600, loss = 0.23 (8091.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:30.071465: step 186610, loss = 0.17 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:30.389505: step 186620, loss = 0.25 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:30.714273: step 186630, loss = 0.19 (7363.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:31.036073: step 186640, loss = 0.21 (8118.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:31.356059: step 186650, loss = 0.21 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:31.676856: step 186660, loss = 0.27 (7770.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:31.998448: step 186670, loss = 0.28 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:32.318390: step 186680, loss = 0.20 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:32.637185: step 186690, loss = 0.16 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:32.959669: step 186700, loss = 0.24 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:33.419239: step 186710, loss = 0.28 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:33.743505: step 186720, loss = 0.25 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:34.063919: step 186730, loss = 0.25 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:34.384670: step 186740, loss = 0.20 (7899.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:34.702692: step 186750, loss = 0.18 (8100.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:35.023409: step 186760, loss = 0.20 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:35.346014: step 186770, loss = 0.18 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:35.663375: step 186780, loss = 0.22 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:35.983594: step 186790, loss = 0.21 (8009.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:36.304228: step 186800, loss = 0.20 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:36.763896: step 186810, loss = 0.18 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:37.083950: step 186820, loss = 0.17 (7634.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:37.406995: step 186830, loss = 0.23 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:37.727514: step 186840, loss = 0.20 (7934.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:38.047560: step 186850, loss = 0.21 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:38.366732: step 186860, loss = 0.23 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:38.685265: step 186870, loss = 0.24 (7942.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:39.005708: step 186880, loss = 0.21 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:39.325455: step 186890, loss = 0.20 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:39.646632: step 186900, loss = 0.19 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:40.114370: step 186910, loss = 0.17 (7884.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:40.432836: step 186920, loss = 0.27 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:40.751921: step 186930, loss = 0.18 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:41.072385: step 186940, loss = 0.19 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:41.390867: step 186950, loss = 0.23 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:41.710252: step 186960, loss = 0.25 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:42.033027: step 186970, loss = 0.26 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:42.354620: step 186980, loss = 0.24 (7493.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:42.675211: step 186990, loss = 0.22 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:42.995898: step 187000, loss = 0.20 (7890.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:43.555360: step 187010, loss = 0.19 (7698.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:43.873544: step 187020, loss = 0.21 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:44.192912: step 187030, loss = 0.22 (7835.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:44.512776: step 187040, loss = 0.22 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:44.833163: step 187050, loss = 0.24 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:45.154053: step 187060, loss = 0.19 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:45.475839: step 187070, loss = 0.24 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:45.797999: step 187080, loss = 0.23 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:46.117515: step 187090, loss = 0.17 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:46.439714: step 187100, loss = 0.17 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:46.902008: step 187110, loss = 0.17 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:47.221773: step 187120, loss = 0.21 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:47.541636: step 187130, loss = 0.14 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:47.862064: step 187140, loss = 0.17 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:48.181269: step 187150, loss = 0.25 (7995.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:48.500095: step 187160, loss = 0.19 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:48.822762: step 187170, loss = 0.21 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:49.148785: step 187180, loss = 0.17 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:49.470452: step 187190, loss = 0.23 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:49.790466: step 187200, loss = 0.18 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:50.253837: step 187210, loss = 0.20 (7988.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:50.573144: step 187220, loss = 0.26 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:50.897154: step 187230, loss = 0.27 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:51.219023: step 187240, loss = 0.24 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:51.536205: step 187250, loss = 0.25 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:51.856910: step 187260, loss = 0.14 (8001.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:52.176079: step 187270, loss = 0.19 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:52.496032: step 187280, loss = 0.24 (7985.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:52.815906: step 187290, loss = 0.16 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:53.136102: step 187300, loss = 0.22 (8138.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:53.597599: step 187310, loss = 0.19 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:53.918583: step 187320, loss = 0.23 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:54.240706: step 187330, loss = 0.24 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:54.568745: step 187340, loss = 0.28 (7569.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:54.889535: step 187350, loss = 0.22 (7440.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:24:55.209436: step 187360, loss = 0.19 (8082.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:55.527628: step 187370, loss = 0.17 (7991.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:55.848216: step 187380, loss = 0.21 (7948.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:56.168990: step 187390, loss = 0.17 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:56.488440: step 187400, loss = 0.21 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:56.956084: step 187410, loss = 0.19 (8045.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:57.274103: step 187420, loss = 0.17 (7802.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:57.594749: step 187430, loss = 0.19 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:57.915245: step 187440, loss = 0.23 (8027.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:58.237142: step 187450, loss = 0.21 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:58.562258: step 187460, loss = 0.20 (7869.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:58.882577: step 187470, loss = 0.24 (8077.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:59.202572: step 187480, loss = 0.21 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:59.523410: step 187490, loss = 0.18 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:24:59.844246: step 187500, loss = 0.19 (7978.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:00.303792: step 187510, loss = 0.22 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:00.623097: step 187520, loss = 0.32 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:00.944675: step 187530, loss = 0.23 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:01.267146: step 187540, loss = 0.20 (7923.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:01.587608: step 187550, loss = 0.29 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:01.907394: step 187560, loss = 0.26 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:02.232715: step 187570, loss = 0.22 (7303.4 examples/sec; 0.018 sec/batch)
2017-09-16 17:25:02.553226: step 187580, loss = 0.21 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:02.879688: step 187590, loss = 0.28 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:03.200020: step 187600, loss = 0.22 (7894.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:03.670034: step 187610, loss = 0.21 (7725.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:03.989730: step 187620, loss = 0.18 (8023.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:04.310339: step 187630, loss = 0.23 (7929.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:04.630304: step 187640, loss = 0.22 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:04.951685: step 187650, loss = 0.21 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:05.272490: step 187660, loss = 0.19 (7739.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:05.590858: step 187670, loss = 0.17 (8005.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:05.911905: step 187680, loss = 0.17 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:06.231353: step 187690, loss = 0.25 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:06.553136: step 187700, loss = 0.18 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:07.015655: step 187710, loss = 0.26 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:07.335194: step 187720, loss = 0.23 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:07.654715: step 187730, loss = 0.20 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:07.974786: step 187740, loss = 0.23 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:08.296230: step 187750, loss = 0.21 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:08.615626: step 187760, loss = 0.24 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:08.935037: step 187770, loss = 0.18 (7964.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:09.255401: step 187780, loss = 0.23 (7941.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:09.578096: step 187790, loss = 0.21 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:09.897455: step 187800, loss = 0.17 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:10.356247: step 187810, loss = 0.20 (7913.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:10.675012: step 187820, loss = 0.25 (7912.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:10.993875: step 187830, loss = 0.17 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:11.312848: step 187840, loss = 0.22 (8030.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:11.633269: step 187850, loss = 0.29 (8142.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:11.952383: step 187860, loss = 0.27 (8102.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:12.271331: step 187870, loss = 0.19 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:12.596064: step 187880, loss = 0.22 (7645.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:12.916864: step 187890, loss = 0.22 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:13.236493: step 187900, loss = 0.22 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:13.692312: step 187910, loss = 0.24 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:14.011459: step 187920, loss = 0.20 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:14.331871: step 187930, loss = 0.19 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:14.652568: step 187940, loss = 0.26 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:14.973045: step 187950, loss = 0.28 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:15.295119: step 187960, loss = 0.19 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:15.614139: step 187970, loss = 0.18 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:15.934593: step 187980, loss = 0.18 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:16.257286: step 187990, loss = 0.24 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:16.578366: step 188000, loss = 0.18 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:17.139126: step 188010, loss = 0.26 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:17.458401: step 188020, loss = 0.17 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:17.780969: step 188030, loss = 0.19 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:18.101536: step 188040, loss = 0.26 (7660.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:18.423122: step 188050, loss = 0.20 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:18.745211: step 188060, loss = 0.19 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:19.065434: step 188070, loss = 0.21 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:19.386309: step 188080, loss = 0.24 (8101.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:19.705707: step 188090, loss = 0.17 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:20.025452: step 188100, loss = 0.19 (7878.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:20.492637: step 188110, loss = 0.22 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:20.811841: step 188120, loss = 0.20 (7800.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:21.135703: step 188130, loss = 0.19 (7858.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:21.455161: step 188140, loss = 0.14 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:21.774607: step 188150, loss = 0.24 (8198.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:22.094267: step 188160, loss = 0.19 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:22.414325: step 188170, loss = 0.26 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:22.736264: step 188180, loss = 0.25 (8052.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:23.054865: step 188190, loss = 0.27 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:23.372021: step 188200, loss = 0.19 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:23.824564: step 188210, loss = 0.21 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:24.145885: step 188220, loss = 0.16 (7796.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:24.462779: step 188230, loss = 0.22 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:24.784664: step 188240, loss = 0.22 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:25.106794: step 188250, loss = 0.21 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:25.433691: step 188260, loss = 0.17 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:25.753120: step 188270, loss = 0.14 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:26.073725: step 188280, loss = 0.29 (8178.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:26.392447: step 188290, loss = 0.19 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:26.710960: step 188300, loss = 0.18 (7987.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:27.172555: step 188310, loss = 0.32 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:27.495870: step 188320, loss = 0.17 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:27.817873: step 188330, loss = 0.20 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:28.137784: step 188340, loss = 0.20 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:28.460865: step 188350, loss = 0.15 (8073.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:28.782085: step 188360, loss = 0.25 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:29.101441: step 188370, loss = 0.25 (8055.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:29.421986: step 188380, loss = 0.19 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:29.743746: step 188390, loss = 0.27 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:30.066544: step 188400, loss = 0.23 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:30.534037: step 188410, loss = 0.18 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:30.856633: step 188420, loss = 0.23 (8128.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:31.178864: step 188430, loss = 0.21 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:31.500301: step 188440, loss = 0.14 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:31.821872: step 188450, loss = 0.21 (7885.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:32.140416: step 188460, loss = 0.31 (8022.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:32.462414: step 188470, loss = 0.16 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:32.781190: step 188480, loss = 0.25 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:33.100625: step 188490, loss = 0.22 (7946.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:33.421000: step 188500, loss = 0.24 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:33.881498: step 188510, loss = 0.23 (8066.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:34.201881: step 188520, loss = 0.19 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:34.521629: step 188530, loss = 0.20 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:34.842380: step 188540, loss = 0.21 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:35.164297: step 188550, loss = 0.18 (7999.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:35.488201: step 188560, loss = 0.17 (8095.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:35.809120: step 188570, loss = 0.20 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:36.130152: step 188580, loss = 0.24 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:36.451251: step 188590, loss = 0.20 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:36.771109: step 188600, loss = 0.18 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:37.232336: step 188610, loss = 0.21 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:37.555588: step 188620, loss = 0.16 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:37.873651: step 188630, loss = 0.24 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:38.202889: step 188640, loss = 0.22 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:38.523937: step 188650, loss = 0.22 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:38.842863: step 188660, loss = 0.22 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:39.160294: step 188670, loss = 0.17 (8146.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:39.480741: step 188680, loss = 0.19 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:39.801679: step 188690, loss = 0.14 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:40.122218: step 188700, loss = 0.18 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:40.594051: step 188710, loss = 0.22 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:40.918012: step 188720, loss = 0.17 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:41.239906: step 188730, loss = 0.19 (7898.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:41.561144: step 188740, loss = 0.20 (8032.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:41.880537: step 188750, loss = 0.21 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:42.201909: step 188760, loss = 0.23 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:42.521513: step 188770, loss = 0.18 (8182.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:42.840494: step 188780, loss = 0.28 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:43.159683: step 188790, loss = 0.16 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:43.480734: step 188800, loss = 0.14 (8113.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:43.938537: step 188810, loss = 0.23 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:44.257640: step 188820, loss = 0.20 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:44.576741: step 188830, loss = 0.15 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:44.897998: step 188840, loss = 0.18 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:45.217277: step 188850, loss = 0.15 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:45.536927: step 188860, loss = 0.25 (8123.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:45.856340: step 188870, loss = 0.16 (8087.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:46.175566: step 188880, loss = 0.20 (7954.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:46.495690: step 188890, loss = 0.22 (8143.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:46.815292: step 188900, loss = 0.21 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:47.271372: step 188910, loss = 0.18 (7586.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:47.592654: step 188920, loss = 0.20 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:47.914794: step 188930, loss = 0.19 (8039.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:48.234370: step 188940, loss = 0.23 (8139.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:48.556766: step 188950, loss = 0.20 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:48.876539: step 188960, loss = 0.32 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:49.199246: step 188970, loss = 0.21 (7995.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:49.521199: step 188980, loss = 0.18 (7638.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:49.843594: step 188990, loss = 0.22 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:50.169190: step 189000, loss = 0.19 (7860.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:50.726796: step 189010, loss = 0.17 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:51.048519: step 189020, loss = 0.21 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:51.381856: step 189030, loss = 0.23 (7958.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:51.701077: step 189040, loss = 0.22 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:52.021216: step 189050, loss = 0.17 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:52.343194: step 189060, loss = 0.20 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:52.661640: step 189070, loss = 0.22 (8089.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:52.984235: step 189080, loss = 0.22 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:53.312901: step 189090, loss = 0.22 (7927.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:53.632363: step 189100, loss = 0.18 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:54.088396: step 189110, loss = 0.26 (8048.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:54.410123: step 189120, loss = 0.25 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:54.730498: step 189130, loss = 0.22 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:55.051373: step 189140, loss = 0.17 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:55.371371: step 189150, loss = 0.24 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:55.692417: step 189160, loss = 0.19 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:56.011741: step 189170, loss = 0.19 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:56.331541: step 189180, loss = 0.22 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:56.651506: step 189190, loss = 0.25 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:56.975746: step 189200, loss = 0.25 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:57.444885: step 189210, loss = 0.20 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:57.765816: step 189220, loss = 0.22 (7572.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:25:58.086071: step 189230, loss = 0.23 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:58.405469: step 189240, loss = 0.20 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:58.724993: step 189250, loss = 0.28 (8134.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:59.046474: step 189260, loss = 0.24 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:59.368596: step 189270, loss = 0.20 (7958.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:25:59.692099: step 189280, loss = 0.29 (8105.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:00.011697: step 189290, loss = 0.23 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:00.333919: step 189300, loss = 0.27 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:00.797329: step 189310, loss = 0.19 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:01.117082: step 189320, loss = 0.16 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:01.436249: step 189330, loss = 0.27 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:01.754866: step 189340, loss = 0.26 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:02.075436: step 189350, loss = 0.19 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:02.396192: step 189360, loss = 0.27 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:02.714672: step 189370, loss = 0.26 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:03.035198: step 189380, loss = 0.21 (8148.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:03.357377: step 189390, loss = 0.23 (7974.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:03.678261: step 189400, loss = 0.18 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:04.141202: step 189410, loss = 0.22 (7861.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:04.462736: step 189420, loss = 0.24 (7939.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:04.784020: step 189430, loss = 0.16 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:05.103926: step 189440, loss = 0.17 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:05.425059: step 189450, loss = 0.17 (8150.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:05.746453: step 189460, loss = 0.29 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:06.067897: step 189470, loss = 0.20 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:06.390224: step 189480, loss = 0.21 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:06.712143: step 189490, loss = 0.21 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:07.030252: step 189500, loss = 0.19 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:07.485757: step 189510, loss = 0.23 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:07.808607: step 189520, loss = 0.23 (7872.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:08.128290: step 189530, loss = 0.26 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:08.447870: step 189540, loss = 0.20 (7949.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:08.768108: step 189550, loss = 0.30 (8116.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:09.094424: step 189560, loss = 0.27 (7390.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:09.414076: step 189570, loss = 0.22 (7945.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:09.734162: step 189580, loss = 0.21 (8062.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:10.054795: step 189590, loss = 0.18 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:10.375499: step 189600, loss = 0.20 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:10.834190: step 189610, loss = 0.21 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:11.155983: step 189620, loss = 0.14 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:11.474529: step 189630, loss = 0.21 (8129.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:11.797803: step 189640, loss = 0.19 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:12.119694: step 189650, loss = 0.21 (7818.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:12.439815: step 189660, loss = 0.16 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:12.760722: step 189670, loss = 0.23 (7785.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:13.081388: step 189680, loss = 0.20 (8091.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:13.401599: step 189690, loss = 0.15 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:13.721183: step 189700, loss = 0.18 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:14.187711: step 189710, loss = 0.18 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:14.508123: step 189720, loss = 0.21 (7982.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:14.833378: step 189730, loss = 0.23 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:15.157941: step 189740, loss = 0.20 (7700.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:15.479244: step 189750, loss = 0.25 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:15.801760: step 189760, loss = 0.17 (8147.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:16.121958: step 189770, loss = 0.18 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:16.440650: step 189780, loss = 0.17 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:16.763182: step 189790, loss = 0.26 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:17.086141: step 189800, loss = 0.23 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:17.546001: step 189810, loss = 0.24 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:17.864154: step 189820, loss = 0.30 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:18.187057: step 189830, loss = 0.20 (7896.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:18.507751: step 189840, loss = 0.27 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:18.827258: step 189850, loss = 0.25 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:19.144807: step 189860, loss = 0.17 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:19.465978: step 189870, loss = 0.14 (7839.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:19.786750: step 189880, loss = 0.22 (7994.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:20.110099: step 189890, loss = 0.20 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:20.430626: step 189900, loss = 0.19 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:20.882115: step 189910, loss = 0.16 (7932.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:21.206368: step 189920, loss = 0.17 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:21.527385: step 189930, loss = 0.22 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:21.847560: step 189940, loss = 0.22 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:22.169686: step 189950, loss = 0.22 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:22.490322: step 189960, loss = 0.30 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:22.811613: step 189970, loss = 0.17 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:23.129834: step 189980, loss = 0.25 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:23.450974: step 189990, loss = 0.20 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:23.771361: step 190000, loss = 0.20 (8107.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:24.340912: step 190010, loss = 0.21 (7802.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:24.661578: step 190020, loss = 0.27 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:24.982632: step 190030, loss = 0.19 (7993.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:25.304619: step 190040, loss = 0.18 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:25.629735: step 190050, loss = 0.28 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:25.950395: step 190060, loss = 0.20 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:26.271251: step 190070, loss = 0.21 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:26.594192: step 190080, loss = 0.20 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:26.914295: step 190090, loss = 0.18 (7927.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:27.233765: step 190100, loss = 0.25 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:27.683171: step 190110, loss = 0.22 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:28.003909: step 190120, loss = 0.18 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:28.326528: step 190130, loss = 0.26 (7771.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:28.647128: step 190140, loss = 0.17 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:28.969520: step 190150, loss = 0.23 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:29.292413: step 190160, loss = 0.15 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:29.614489: step 190170, loss = 0.17 (7830.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:29.936712: step 190180, loss = 0.21 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:30.259906: step 190190, loss = 0.23 (7829.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:30.580938: step 190200, loss = 0.20 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:31.048886: step 190210, loss = 0.17 (7825.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:31.370345: step 190220, loss = 0.21 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:31.690742: step 190230, loss = 0.24 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:32.013023: step 190240, loss = 0.15 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:32.335153: step 190250, loss = 0.19 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:32.656061: step 190260, loss = 0.22 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:32.978093: step 190270, loss = 0.18 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:33.297565: step 190280, loss = 0.25 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:33.619085: step 190290, loss = 0.18 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:33.941868: step 190300, loss = 0.20 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:34.405179: step 190310, loss = 0.23 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:34.724655: step 190320, loss = 0.19 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:35.045263: step 190330, loss = 0.28 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:35.366188: step 190340, loss = 0.16 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:35.687857: step 190350, loss = 0.22 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:36.007205: step 190360, loss = 0.16 (7975.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:36.327204: step 190370, loss = 0.25 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:36.646843: step 190380, loss = 0.22 (7770.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:36.967125: step 190390, loss = 0.22 (7803.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:37.287498: step 190400, loss = 0.21 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:37.752865: step 190410, loss = 0.31 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:38.071411: step 190420, loss = 0.20 (8071.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:38.393174: step 190430, loss = 0.23 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:38.713979: step 190440, loss = 0.19 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:39.035524: step 190450, loss = 0.24 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:39.354348: step 190460, loss = 0.22 (8003.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:39.676065: step 190470, loss = 0.22 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:39.997352: step 190480, loss = 0.21 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:40.319744: step 190490, loss = 0.20 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:40.639199: step 190500, loss = 0.23 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:41.101413: step 190510, loss = 0.20 (7900.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:41.421362: step 190520, loss = 0.23 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:41.741724: step 190530, loss = 0.18 (7930.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:42.063648: step 190540, loss = 0.18 (7531.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:42.388903: step 190550, loss = 0.23 (7511.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:42.710761: step 190560, loss = 0.23 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:43.037177: step 190570, loss = 0.21 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:43.360290: step 190580, loss = 0.17 (7692.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:43.681799: step 190590, loss = 0.27 (7808.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:43.999639: step 190600, loss = 0.24 (8070.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:44.450503: step 190610, loss = 0.27 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:44.769356: step 190620, loss = 0.24 (8030.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:45.094127: step 190630, loss = 0.21 (8081.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:45.412940: step 190640, loss = 0.27 (7953.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:45.732513: step 190650, loss = 0.25 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:46.050338: step 190660, loss = 0.18 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:46.369896: step 190670, loss = 0.20 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:46.691763: step 190680, loss = 0.14 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:47.014321: step 190690, loss = 0.15 (7884.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:47.336197: step 190700, loss = 0.19 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:47.787287: step 190710, loss = 0.19 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:48.109445: step 190720, loss = 0.25 (7598.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:48.430403: step 190730, loss = 0.21 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:48.754678: step 190740, loss = 0.22 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:49.075479: step 190750, loss = 0.22 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:49.396418: step 190760, loss = 0.21 (7810.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:49.715808: step 190770, loss = 0.22 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:50.034081: step 190780, loss = 0.30 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:50.358624: step 190790, loss = 0.16 (7908.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:50.681986: step 190800, loss = 0.28 (8054.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:51.148445: step 190810, loss = 0.19 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:51.468886: step 190820, loss = 0.14 (8111.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:51.790007: step 190830, loss = 0.23 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:52.108895: step 190840, loss = 0.16 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:52.430083: step 190850, loss = 0.21 (8097.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:52.752335: step 190860, loss = 0.25 (7813.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:53.071364: step 190870, loss = 0.16 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:53.395678: step 190880, loss = 0.26 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:53.714079: step 190890, loss = 0.26 (8090.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:54.033187: step 190900, loss = 0.21 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:54.489201: step 190910, loss = 0.17 (7713.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:26:54.808719: step 190920, loss = 0.21 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:55.128829: step 190930, loss = 0.23 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:55.451170: step 190940, loss = 0.24 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:55.772088: step 190950, loss = 0.27 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:56.091069: step 190960, loss = 0.22 (7861.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:56.410875: step 190970, loss = 0.22 (7972.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:56.733398: step 190980, loss = 0.20 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:57.052931: step 190990, loss = 0.16 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:57.373924: step 191000, loss = 0.24 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:57.934734: step 191010, loss = 0.25 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:58.256552: step 191020, loss = 0.28 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:58.575433: step 191030, loss = 0.19 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:58.896042: step 191040, loss = 0.23 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:59.216800: step 191050, loss = 0.19 (7795.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:59.544175: step 191060, loss = 0.19 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:26:59.866107: step 191070, loss = 0.16 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:00.186535: step 191080, loss = 0.17 (8073.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:00.504870: step 191090, loss = 0.18 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:00.826247: step 191100, loss = 0.20 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:01.283673: step 191110, loss = 0.17 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:01.603099: step 191120, loss = 0.18 (8025.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:01.921190: step 191130, loss = 0.21 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:02.241887: step 191140, loss = 0.19 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:02.560198: step 191150, loss = 0.25 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:02.879028: step 191160, loss = 0.30 (7990.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:03.198014: step 191170, loss = 0.24 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:03.516743: step 191180, loss = 0.22 (8129.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:03.836870: step 191190, loss = 0.26 (7829.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:04.157654: step 191200, loss = 0.21 (7930.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:04.620684: step 191210, loss = 0.18 (8014.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:04.939848: step 191220, loss = 0.16 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:05.258332: step 191230, loss = 0.29 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:05.579289: step 191240, loss = 0.28 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:05.899994: step 191250, loss = 0.18 (7925.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:06.219609: step 191260, loss = 0.23 (7952.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:06.539595: step 191270, loss = 0.21 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:06.860324: step 191280, loss = 0.26 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:07.182197: step 191290, loss = 0.19 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:07.501968: step 191300, loss = 0.19 (8080.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:07.966237: step 191310, loss = 0.15 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:08.291855: step 191320, loss = 0.26 (7901.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:08.612903: step 191330, loss = 0.19 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:08.932352: step 191340, loss = 0.20 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:09.252747: step 191350, loss = 0.17 (7812.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:09.574605: step 191360, loss = 0.29 (7806.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:09.894546: step 191370, loss = 0.20 (7979.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:10.213517: step 191380, loss = 0.19 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:10.531954: step 191390, loss = 0.17 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:10.854006: step 191400, loss = 0.24 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:11.319427: step 191410, loss = 0.21 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:11.638645: step 191420, loss = 0.19 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:11.958737: step 191430, loss = 0.19 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:12.278995: step 191440, loss = 0.19 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:12.599136: step 191450, loss = 0.19 (8126.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:12.920567: step 191460, loss = 0.18 (7757.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:13.241322: step 191470, loss = 0.20 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:13.568783: step 191480, loss = 0.19 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:13.889391: step 191490, loss = 0.20 (7817.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:14.209005: step 191500, loss = 0.19 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:14.673379: step 191510, loss = 0.26 (8030.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:14.994357: step 191520, loss = 0.32 (7931.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:15.313945: step 191530, loss = 0.17 (8037.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:15.633895: step 191540, loss = 0.25 (8120.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:15.953700: step 191550, loss = 0.23 (8181.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:16.274821: step 191560, loss = 0.24 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:16.594194: step 191570, loss = 0.19 (8096.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:16.916123: step 191580, loss = 0.19 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:17.239928: step 191590, loss = 0.21 (7723.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:17.559865: step 191600, loss = 0.23 (8150.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:18.017116: step 191610, loss = 0.21 (8170.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:18.337315: step 191620, loss = 0.21 (7989.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:18.663271: step 191630, loss = 0.17 (7614.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:18.982754: step 191640, loss = 0.20 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:19.304869: step 191650, loss = 0.25 (7500.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:19.624896: step 191660, loss = 0.20 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:19.946223: step 191670, loss = 0.17 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:20.264683: step 191680, loss = 0.30 (7920.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:20.584135: step 191690, loss = 0.22 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:20.906010: step 191700, loss = 0.19 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:21.367324: step 191710, loss = 0.22 (8119.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:21.687731: step 191720, loss = 0.22 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:22.007397: step 191730, loss = 0.17 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:22.328366: step 191740, loss = 0.20 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:22.647362: step 191750, loss = 0.19 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:22.968116: step 191760, loss = 0.21 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:23.288344: step 191770, loss = 0.18 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:23.610710: step 191780, loss = 0.19 (7791.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:23.933078: step 191790, loss = 0.33 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:24.252518: step 191800, loss = 0.19 (8117.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:24.703122: step 191810, loss = 0.21 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:25.022223: step 191820, loss = 0.23 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:25.343580: step 191830, loss = 0.18 (8079.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:25.661001: step 191840, loss = 0.20 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:25.980816: step 191850, loss = 0.17 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:26.300734: step 191860, loss = 0.23 (8131.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:26.622366: step 191870, loss = 0.25 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:26.943687: step 191880, loss = 0.18 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:27.263249: step 191890, loss = 0.22 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:27.584474: step 191900, loss = 0.23 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:28.051970: step 191910, loss = 0.20 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:28.371683: step 191920, loss = 0.29 (8015.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:28.691999: step 191930, loss = 0.19 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:29.013402: step 191940, loss = 0.20 (7979.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:29.333370: step 191950, loss = 0.25 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:29.653710: step 191960, loss = 0.15 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:29.973045: step 191970, loss = 0.17 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:30.293157: step 191980, loss = 0.17 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:30.613928: step 191990, loss = 0.33 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:30.935306: step 192000, loss = 0.20 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:31.489853: step 192010, loss = 0.22 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:31.809089: step 192020, loss = 0.23 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:32.130063: step 192030, loss = 0.16 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:32.450894: step 192040, loss = 0.18 (8072.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:32.771830: step 192050, loss = 0.27 (7983.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:33.094818: step 192060, loss = 0.16 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:33.414767: step 192070, loss = 0.29 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:33.735563: step 192080, loss = 0.25 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:34.058161: step 192090, loss = 0.22 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:34.377840: step 192100, loss = 0.23 (7920.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:34.853324: step 192110, loss = 0.20 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:35.173584: step 192120, loss = 0.23 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:35.499198: step 192130, loss = 0.31 (7908.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:35.819229: step 192140, loss = 0.18 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:36.140239: step 192150, loss = 0.18 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:36.462108: step 192160, loss = 0.26 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:36.782636: step 192170, loss = 0.21 (7952.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:37.103428: step 192180, loss = 0.22 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:37.422604: step 192190, loss = 0.20 (8044.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:37.741419: step 192200, loss = 0.22 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:38.195924: step 192210, loss = 0.19 (7910.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:38.519720: step 192220, loss = 0.23 (7976.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:38.839211: step 192230, loss = 0.20 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:39.159301: step 192240, loss = 0.35 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:39.478354: step 192250, loss = 0.20 (7898.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:39.799510: step 192260, loss = 0.19 (7843.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:40.121537: step 192270, loss = 0.17 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:40.441840: step 192280, loss = 0.31 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:40.764621: step 192290, loss = 0.25 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:41.083652: step 192300, loss = 0.32 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:41.553107: step 192310, loss = 0.20 (7695.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:41.877325: step 192320, loss = 0.21 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:42.198000: step 192330, loss = 0.17 (7915.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:42.516869: step 192340, loss = 0.24 (8100.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:42.835979: step 192350, loss = 0.20 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:43.157855: step 192360, loss = 0.22 (7960.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:43.479076: step 192370, loss = 0.19 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:43.798746: step 192380, loss = 0.22 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:44.120427: step 192390, loss = 0.22 (8156.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:44.439565: step 192400, loss = 0.26 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:44.896840: step 192410, loss = 0.21 (7997.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:45.216372: step 192420, loss = 0.18 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:45.536438: step 192430, loss = 0.20 (8034.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:45.858027: step 192440, loss = 0.27 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:46.179509: step 192450, loss = 0.26 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:46.498481: step 192460, loss = 0.16 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:46.817547: step 192470, loss = 0.21 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:47.139123: step 192480, loss = 0.21 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:47.460727: step 192490, loss = 0.19 (7868.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:47.789575: step 192500, loss = 0.29 (7478.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:48.247465: step 192510, loss = 0.22 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:48.568675: step 192520, loss = 0.27 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:48.889706: step 192530, loss = 0.22 (8106.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:49.211051: step 192540, loss = 0.20 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:49.531227: step 192550, loss = 0.18 (8110.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:49.849443: step 192560, loss = 0.24 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:50.168548: step 192570, loss = 0.24 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:50.488919: step 192580, loss = 0.20 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:50.807733: step 192590, loss = 0.21 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:51.126670: step 192600, loss = 0.16 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:51.592396: step 192610, loss = 0.30 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:51.914353: step 192620, loss = 0.23 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:52.234330: step 192630, loss = 0.22 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:52.553961: step 192640, loss = 0.20 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:52.875580: step 192650, loss = 0.24 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:53.195047: step 192660, loss = 0.24 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:53.515024: step 192670, loss = 0.17 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:53.836163: step 192680, loss = 0.16 (8038.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:54.157304: step 192690, loss = 0.17 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:54.478766: step 192700, loss = 0.18 (7545.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:54.935675: step 192710, loss = 0.20 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:55.257389: step 192720, loss = 0.15 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:55.580151: step 192730, loss = 0.20 (7730.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:27:55.901671: step 192740, loss = 0.22 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:56.221313: step 192750, loss = 0.19 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:56.543943: step 192760, loss = 0.19 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:56.864849: step 192770, loss = 0.20 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:57.184022: step 192780, loss = 0.21 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:57.503518: step 192790, loss = 0.22 (8046.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:57.827487: step 192800, loss = 0.25 (7914.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:58.297219: step 192810, loss = 0.22 (8108.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:58.617680: step 192820, loss = 0.19 (7968.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:58.936366: step 192830, loss = 0.23 (8036.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:59.257980: step 192840, loss = 0.21 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:59.577910: step 192850, loss = 0.24 (7974.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:27:59.897498: step 192860, loss = 0.18 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:00.218057: step 192870, loss = 0.16 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:00.538854: step 192880, loss = 0.28 (7729.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:28:00.866193: step 192890, loss = 0.23 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:01.185390: step 192900, loss = 0.16 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:01.651599: step 192910, loss = 0.31 (8086.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:01.970896: step 192920, loss = 0.35 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:02.288470: step 192930, loss = 0.21 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:02.609502: step 192940, loss = 0.24 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:02.930589: step 192950, loss = 0.20 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:03.250912: step 192960, loss = 0.21 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:03.572626: step 192970, loss = 0.21 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:03.897040: step 192980, loss = 0.17 (7395.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:28:04.216488: step 192990, loss = 0.19 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:04.540293: step 193000, loss = 0.18 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:05.147690: step 193010, loss = 0.20 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:05.466128: step 193020, loss = 0.16 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:05.788934: step 193030, loss = 0.19 (7860.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:06.108139: step 193040, loss = 0.22 (7818.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:06.427106: step 193050, loss = 0.24 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:06.749589: step 193060, loss = 0.30 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:07.068278: step 193070, loss = 0.21 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:07.387075: step 193080, loss = 0.22 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:07.708947: step 193090, loss = 0.16 (8000.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:08.029881: step 193100, loss = 0.21 (7871.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:08.488701: step 193110, loss = 0.19 (7892.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:08.810860: step 193120, loss = 0.25 (7869.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:09.130589: step 193130, loss = 0.20 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:09.451670: step 193140, loss = 0.29 (7835.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:09.770755: step 193150, loss = 0.20 (8124.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:10.091989: step 193160, loss = 0.17 (7953.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:10.412744: step 193170, loss = 0.22 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:10.731953: step 193180, loss = 0.24 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:11.053274: step 193190, loss = 0.25 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:11.370613: step 193200, loss = 0.17 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:11.829253: step 193210, loss = 0.21 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:12.149024: step 193220, loss = 0.17 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:12.468150: step 193230, loss = 0.28 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:12.790306: step 193240, loss = 0.22 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:13.116120: step 193250, loss = 0.23 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:13.436518: step 193260, loss = 0.18 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:13.755735: step 193270, loss = 0.16 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:14.074388: step 193280, loss = 0.14 (8159.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:14.394330: step 193290, loss = 0.16 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:14.718939: step 193300, loss = 0.26 (7912.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:15.167977: step 193310, loss = 0.21 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:15.487663: step 193320, loss = 0.20 (8029.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:15.808427: step 193330, loss = 0.16 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:16.130356: step 193340, loss = 0.22 (7826.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:16.448768: step 193350, loss = 0.15 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:16.768279: step 193360, loss = 0.23 (8067.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:17.088342: step 193370, loss = 0.18 (8134.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:17.408248: step 193380, loss = 0.24 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:17.729056: step 193390, loss = 0.21 (7999.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:18.050568: step 193400, loss = 0.23 (7807.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:18.512250: step 193410, loss = 0.19 (7938.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:18.839787: step 193420, loss = 0.23 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:19.159351: step 193430, loss = 0.24 (8115.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:19.478696: step 193440, loss = 0.20 (7946.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:19.800389: step 193450, loss = 0.19 (7988.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:20.119853: step 193460, loss = 0.26 (8005.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:20.440538: step 193470, loss = 0.27 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:20.763394: step 193480, loss = 0.21 (7665.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:28:21.083568: step 193490, loss = 0.22 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:21.403332: step 193500, loss = 0.23 (8012.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:21.869439: step 193510, loss = 0.18 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:22.189355: step 193520, loss = 0.19 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:22.508322: step 193530, loss = 0.19 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:22.828743: step 193540, loss = 0.26 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:23.150524: step 193550, loss = 0.21 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:23.471435: step 193560, loss = 0.26 (8120.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:23.791411: step 193570, loss = 0.23 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:24.108787: step 193580, loss = 0.19 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:24.429021: step 193590, loss = 0.20 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:24.748473: step 193600, loss = 0.34 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:25.199352: step 193610, loss = 0.23 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:25.524717: step 193620, loss = 0.21 (7520.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:28:25.844243: step 193630, loss = 0.23 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:26.170573: step 193640, loss = 0.21 (8078.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:26.491350: step 193650, loss = 0.17 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:26.810188: step 193660, loss = 0.33 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:27.129818: step 193670, loss = 0.25 (7926.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:27.447754: step 193680, loss = 0.25 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:27.766718: step 193690, loss = 0.25 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:28.086670: step 193700, loss = 0.22 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:28.536119: step 193710, loss = 0.14 (7824.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:28.855201: step 193720, loss = 0.26 (7910.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:29.175438: step 193730, loss = 0.20 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:29.493282: step 193740, loss = 0.21 (8114.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:29.813403: step 193750, loss = 0.15 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:30.131704: step 193760, loss = 0.28 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:30.451085: step 193770, loss = 0.23 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:30.772315: step 193780, loss = 0.22 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:31.091558: step 193790, loss = 0.27 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:31.412611: step 193800, loss = 0.27 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:31.869946: step 193810, loss = 0.21 (7990.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:32.189943: step 193820, loss = 0.16 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:32.508731: step 193830, loss = 0.21 (7925.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:32.829100: step 193840, loss = 0.18 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:33.148442: step 193850, loss = 0.17 (8076.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:33.467867: step 193860, loss = 0.20 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:33.789079: step 193870, loss = 0.21 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:34.108122: step 193880, loss = 0.22 (7842.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:34.428455: step 193890, loss = 0.27 (7816.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:34.746749: step 193900, loss = 0.18 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:35.201421: step 193910, loss = 0.32 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:35.520301: step 193920, loss = 0.18 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:35.841355: step 193930, loss = 0.14 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:36.162202: step 193940, loss = 0.21 (7993.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:36.480785: step 193950, loss = 0.18 (8043.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:36.801303: step 193960, loss = 0.24 (8098.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:37.120779: step 193970, loss = 0.19 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:37.441544: step 193980, loss = 0.23 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:37.763478: step 193990, loss = 0.25 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:38.082772: step 194000, loss = 0.20 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:38.628590: step 194010, loss = 0.19 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:38.949482: step 194020, loss = 0.15 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:39.267355: step 194030, loss = 0.12 (7958.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:39.587248: step 194040, loss = 0.20 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:39.907471: step 194050, loss = 0.20 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:40.226487: step 194060, loss = 0.19 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:40.546106: step 194070, loss = 0.22 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:40.865909: step 194080, loss = 0.21 (7992.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:41.184792: step 194090, loss = 0.19 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:41.505791: step 194100, loss = 0.17 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:41.964594: step 194110, loss = 0.22 (7867.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:42.284037: step 194120, loss = 0.23 (8060.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:42.603632: step 194130, loss = 0.24 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:42.923297: step 194140, loss = 0.21 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:43.244869: step 194150, loss = 0.19 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:43.567036: step 194160, loss = 0.21 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:43.887752: step 194170, loss = 0.15 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:44.207906: step 194180, loss = 0.20 (7961.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:44.528110: step 194190, loss = 0.19 (7982.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:44.849353: step 194200, loss = 0.17 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:45.310439: step 194210, loss = 0.22 (7982.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:45.632562: step 194220, loss = 0.24 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:45.952068: step 194230, loss = 0.27 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:46.275703: step 194240, loss = 0.25 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:46.594616: step 194250, loss = 0.22 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:46.913548: step 194260, loss = 0.25 (8131.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:47.233761: step 194270, loss = 0.21 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:47.552505: step 194280, loss = 0.18 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:47.871524: step 194290, loss = 0.15 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:48.191605: step 194300, loss = 0.21 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:48.655590: step 194310, loss = 0.20 (8022.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:48.977888: step 194320, loss = 0.18 (7831.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:49.297300: step 194330, loss = 0.18 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:49.620433: step 194340, loss = 0.21 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:49.941573: step 194350, loss = 0.23 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:50.263085: step 194360, loss = 0.18 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:50.582319: step 194370, loss = 0.23 (8082.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:50.901349: step 194380, loss = 0.13 (8118.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:51.222144: step 194390, loss = 0.18 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:51.541204: step 194400, loss = 0.21 (7892.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:52.006079: step 194410, loss = 0.20 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:52.325245: step 194420, loss = 0.17 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:52.644196: step 194430, loss = 0.21 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:52.963804: step 194440, loss = 0.24 (8016.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:53.285444: step 194450, loss = 0.17 (7893.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:53.606311: step 194460, loss = 0.17 (8047.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:53.926935: step 194470, loss = 0.25 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:54.247034: step 194480, loss = 0.25 (8148.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:54.568621: step 194490, loss = 0.27 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:54.892146: step 194500, loss = 0.23 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:55.346930: step 194510, loss = 0.21 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:55.666275: step 194520, loss = 0.24 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:55.985787: step 194530, loss = 0.21 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:56.306532: step 194540, loss = 0.21 (7847.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:56.626022: step 194550, loss = 0.23 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:56.944030: step 194560, loss = 0.18 (8076.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:57.262236: step 194570, loss = 0.16 (8055.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:57.581717: step 194580, loss = 0.24 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:57.902604: step 194590, loss = 0.18 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:58.224468: step 194600, loss = 0.19 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:58.688112: step 194610, loss = 0.16 (7970.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:59.008604: step 194620, loss = 0.20 (7879.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:59.328609: step 194630, loss = 0.19 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:59.647448: step 194640, loss = 0.16 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:28:59.970266: step 194650, loss = 0.18 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:00.289481: step 194660, loss = 0.23 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:00.612134: step 194670, loss = 0.23 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:00.932135: step 194680, loss = 0.23 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:01.252758: step 194690, loss = 0.19 (7894.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:01.574449: step 194700, loss = 0.23 (7882.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:02.035284: step 194710, loss = 0.26 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:02.353652: step 194720, loss = 0.26 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:02.673726: step 194730, loss = 0.19 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:02.995127: step 194740, loss = 0.19 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:03.314475: step 194750, loss = 0.15 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:03.632345: step 194760, loss = 0.18 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:03.953335: step 194770, loss = 0.17 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:04.272265: step 194780, loss = 0.23 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:04.592702: step 194790, loss = 0.22 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:04.914328: step 194800, loss = 0.17 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:05.366045: step 194810, loss = 0.22 (8109.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:05.688625: step 194820, loss = 0.24 (7832.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:06.011616: step 194830, loss = 0.21 (7896.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:06.334603: step 194840, loss = 0.21 (7843.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:06.654363: step 194850, loss = 0.24 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:06.974578: step 194860, loss = 0.17 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:07.297645: step 194870, loss = 0.32 (7846.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:07.618598: step 194880, loss = 0.23 (8099.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:07.944584: step 194890, loss = 0.21 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:08.265617: step 194900, loss = 0.27 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:08.731875: step 194910, loss = 0.19 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:09.052471: step 194920, loss = 0.21 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:09.370795: step 194930, loss = 0.21 (7967.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:09.692563: step 194940, loss = 0.16 (7872.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:10.012686: step 194950, loss = 0.21 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:10.331059: step 194960, loss = 0.22 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:10.650537: step 194970, loss = 0.15 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:10.970947: step 194980, loss = 0.19 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:11.289659: step 194990, loss = 0.17 (7999.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:11.610947: step 195000, loss = 0.27 (7825.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:12.174909: step 195010, loss = 0.25 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:12.495496: step 195020, loss = 0.23 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:12.816500: step 195030, loss = 0.28 (7933.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:13.136855: step 195040, loss = 0.17 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:13.458685: step 195050, loss = 0.26 (8089.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:13.779307: step 195060, loss = 0.24 (8007.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:14.101034: step 195070, loss = 0.18 (8037.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:14.419336: step 195080, loss = 0.16 (7983.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:14.739708: step 195090, loss = 0.18 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:15.060897: step 195100, loss = 0.20 (7997.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:15.527389: step 195110, loss = 0.28 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:15.846106: step 195120, loss = 0.21 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:16.168442: step 195130, loss = 0.26 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:16.488466: step 195140, loss = 0.21 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:16.810005: step 195150, loss = 0.25 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:17.130329: step 195160, loss = 0.26 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:17.450351: step 195170, loss = 0.22 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:17.770601: step 195180, loss = 0.24 (8021.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:18.090504: step 195190, loss = 0.14 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:18.409113: step 195200, loss = 0.25 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:18.876228: step 195210, loss = 0.27 (7885.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:19.198362: step 195220, loss = 0.17 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:19.516513: step 195230, loss = 0.16 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:19.837127: step 195240, loss = 0.20 (7890.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:20.155498: step 195250, loss = 0.25 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:20.475266: step 195260, loss = 0.18 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:20.792417: step 195270, loss = 0.19 (8157.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:21.115835: step 195280, loss = 0.19 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:21.441010: step 195290, loss = 0.25 (7496.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:21.763691: step 195300, loss = 0.18 (7903.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:22.229868: step 195310, loss = 0.19 (7641.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:22.550995: step 195320, loss = 0.23 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:22.870604: step 195330, loss = 0.16 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:23.190423: step 195340, loss = 0.18 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:23.509767: step 195350, loss = 0.35 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:23.828431: step 195360, loss = 0.22 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:24.155038: step 195370, loss = 0.22 (7330.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:24.477630: step 195380, loss = 0.27 (7562.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:24.800506: step 195390, loss = 0.18 (8053.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:25.120747: step 195400, loss = 0.20 (7879.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:25.584258: step 195410, loss = 0.20 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:25.904096: step 195420, loss = 0.29 (8101.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:26.226957: step 195430, loss = 0.19 (8024.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:26.545261: step 195440, loss = 0.16 (8039.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:26.865283: step 195450, loss = 0.23 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:27.187149: step 195460, loss = 0.26 (7978.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:27.507975: step 195470, loss = 0.20 (7859.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:27.827245: step 195480, loss = 0.19 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:28.145194: step 195490, loss = 0.28 (8114.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:28.466432: step 195500, loss = 0.18 (7675.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:28.917557: step 195510, loss = 0.22 (8017.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:29.236789: step 195520, loss = 0.31 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:29.557512: step 195530, loss = 0.19 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:29.878510: step 195540, loss = 0.26 (7741.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:30.197524: step 195550, loss = 0.21 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:30.515266: step 195560, loss = 0.19 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:30.834436: step 195570, loss = 0.23 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:31.154522: step 195580, loss = 0.20 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:31.479559: step 195590, loss = 0.21 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:31.802302: step 195600, loss = 0.20 (7615.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:32.256278: step 195610, loss = 0.19 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:32.574296: step 195620, loss = 0.25 (8133.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:32.894561: step 195630, loss = 0.18 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:33.213911: step 195640, loss = 0.21 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:33.533353: step 195650, loss = 0.18 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:33.851880: step 195660, loss = 0.21 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:34.172848: step 195670, loss = 0.21 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:34.493652: step 195680, loss = 0.23 (8071.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:34.815016: step 195690, loss = 0.19 (8095.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:35.135514: step 195700, loss = 0.20 (7966.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:35.593150: step 195710, loss = 0.26 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:35.914703: step 195720, loss = 0.19 (7881.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:36.236627: step 195730, loss = 0.29 (7883.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:36.557763: step 195740, loss = 0.21 (7865.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:36.879131: step 195750, loss = 0.29 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:37.198508: step 195760, loss = 0.15 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:37.518368: step 195770, loss = 0.24 (8138.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:37.841191: step 195780, loss = 0.27 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:38.163932: step 195790, loss = 0.21 (7712.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:38.488310: step 195800, loss = 0.25 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:38.952747: step 195810, loss = 0.20 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:39.273224: step 195820, loss = 0.24 (7903.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:39.593635: step 195830, loss = 0.20 (8005.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:39.913838: step 195840, loss = 0.21 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:40.234722: step 195850, loss = 0.20 (7956.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:40.558137: step 195860, loss = 0.15 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:40.879164: step 195870, loss = 0.18 (8062.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:41.200277: step 195880, loss = 0.28 (7918.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:41.521247: step 195890, loss = 0.19 (8156.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:41.841815: step 195900, loss = 0.18 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:42.304386: step 195910, loss = 0.23 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:42.625245: step 195920, loss = 0.19 (7923.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:42.945430: step 195930, loss = 0.22 (8048.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:43.265679: step 195940, loss = 0.27 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:43.588971: step 195950, loss = 0.27 (7876.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:43.908181: step 195960, loss = 0.26 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:44.230212: step 195970, loss = 0.16 (7556.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:44.551708: step 195980, loss = 0.16 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:44.875652: step 195990, loss = 0.22 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:45.196421: step 196000, loss = 0.28 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:45.765474: step 196010, loss = 0.21 (7809.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:46.085107: step 196020, loss = 0.21 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:46.406750: step 196030, loss = 0.20 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:46.725548: step 196040, loss = 0.24 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:47.048328: step 196050, loss = 0.23 (7765.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:47.367853: step 196060, loss = 0.19 (8085.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:47.689782: step 196070, loss = 0.21 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:48.010507: step 196080, loss = 0.28 (8031.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:48.332498: step 196090, loss = 0.19 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:48.652146: step 196100, loss = 0.18 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:49.102433: step 196110, loss = 0.17 (7453.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:49.428243: step 196120, loss = 0.17 (7950.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:49.753292: step 196130, loss = 0.14 (7880.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:50.073073: step 196140, loss = 0.17 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:50.390909: step 196150, loss = 0.15 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:50.712179: step 196160, loss = 0.30 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:51.031563: step 196170, loss = 0.24 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:51.350480: step 196180, loss = 0.16 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:51.671968: step 196190, loss = 0.20 (7923.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:51.991049: step 196200, loss = 0.23 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:52.452927: step 196210, loss = 0.21 (7922.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:52.772324: step 196220, loss = 0.19 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:53.092282: step 196230, loss = 0.17 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:53.412972: step 196240, loss = 0.21 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:53.730938: step 196250, loss = 0.18 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:54.050115: step 196260, loss = 0.16 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:54.368852: step 196270, loss = 0.28 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:54.689053: step 196280, loss = 0.20 (7843.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:55.009978: step 196290, loss = 0.21 (7963.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:55.330646: step 196300, loss = 0.23 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:55.792954: step 196310, loss = 0.17 (8121.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:56.111601: step 196320, loss = 0.18 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:56.433861: step 196330, loss = 0.21 (8124.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:56.757210: step 196340, loss = 0.17 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:57.080380: step 196350, loss = 0.29 (7750.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:29:57.400639: step 196360, loss = 0.20 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:57.729600: step 196370, loss = 0.19 (7941.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:58.047173: step 196380, loss = 0.17 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:58.367335: step 196390, loss = 0.21 (7843.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:58.686804: step 196400, loss = 0.24 (8099.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:59.139966: step 196410, loss = 0.26 (7906.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:59.459727: step 196420, loss = 0.20 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:29:59.780751: step 196430, loss = 0.26 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:00.102151: step 196440, loss = 0.21 (8144.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:00.422457: step 196450, loss = 0.22 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:00.744767: step 196460, loss = 0.16 (8006.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:01.064852: step 196470, loss = 0.23 (8021.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:01.385379: step 196480, loss = 0.20 (7877.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:01.702995: step 196490, loss = 0.16 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:02.021743: step 196500, loss = 0.23 (8040.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:02.483374: step 196510, loss = 0.23 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:02.805499: step 196520, loss = 0.20 (7805.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:03.123751: step 196530, loss = 0.16 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:03.442419: step 196540, loss = 0.23 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:03.762853: step 196550, loss = 0.19 (7846.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:04.083622: step 196560, loss = 0.17 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:04.404106: step 196570, loss = 0.23 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:04.725207: step 196580, loss = 0.19 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:05.046091: step 196590, loss = 0.22 (7936.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:05.365175: step 196600, loss = 0.20 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:05.833608: step 196610, loss = 0.21 (7792.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:06.153358: step 196620, loss = 0.21 (8070.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:06.472084: step 196630, loss = 0.19 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:06.795564: step 196640, loss = 0.16 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:07.117354: step 196650, loss = 0.31 (7841.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:07.435927: step 196660, loss = 0.19 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:07.755041: step 196670, loss = 0.19 (7988.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:08.074928: step 196680, loss = 0.13 (8137.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:08.394172: step 196690, loss = 0.20 (7943.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:08.713354: step 196700, loss = 0.21 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:09.173033: step 196710, loss = 0.24 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:09.494783: step 196720, loss = 0.16 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:09.817056: step 196730, loss = 0.19 (7890.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:10.137444: step 196740, loss = 0.25 (8018.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:10.457193: step 196750, loss = 0.20 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:10.777035: step 196760, loss = 0.19 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:11.098279: step 196770, loss = 0.20 (7913.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:11.417187: step 196780, loss = 0.16 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:11.737661: step 196790, loss = 0.25 (8089.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:12.056655: step 196800, loss = 0.14 (8065.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:12.521728: step 196810, loss = 0.16 (8021.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:12.842570: step 196820, loss = 0.17 (8011.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:13.163985: step 196830, loss = 0.23 (8107.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:13.483739: step 196840, loss = 0.16 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:13.804378: step 196850, loss = 0.17 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:14.123544: step 196860, loss = 0.27 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:14.445090: step 196870, loss = 0.20 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:14.764453: step 196880, loss = 0.20 (8088.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:15.084769: step 196890, loss = 0.18 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:15.403078: step 196900, loss = 0.21 (8115.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:15.869945: step 196910, loss = 0.23 (8135.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:16.193399: step 196920, loss = 0.24 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:16.512536: step 196930, loss = 0.19 (8128.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:16.833083: step 196940, loss = 0.19 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:17.151482: step 196950, loss = 0.21 (7996.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:17.471908: step 196960, loss = 0.27 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:17.791569: step 196970, loss = 0.26 (8018.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:18.116257: step 196980, loss = 0.24 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:18.438041: step 196990, loss = 0.15 (7972.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:18.758224: step 197000, loss = 0.20 (7864.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:19.316366: step 197010, loss = 0.19 (8093.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:19.638641: step 197020, loss = 0.18 (7808.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:19.959047: step 197030, loss = 0.27 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:20.282103: step 197040, loss = 0.26 (8065.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:20.610523: step 197050, loss = 0.19 (7962.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:20.932873: step 197060, loss = 0.26 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:21.256014: step 197070, loss = 0.26 (7794.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:21.577223: step 197080, loss = 0.24 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:21.897988: step 197090, loss = 0.21 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:22.218602: step 197100, loss = 0.26 (8035.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:22.682696: step 197110, loss = 0.23 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:23.002231: step 197120, loss = 0.25 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:23.324339: step 197130, loss = 0.20 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:23.650430: step 197140, loss = 0.18 (7448.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:30:23.970401: step 197150, loss = 0.20 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:24.292580: step 197160, loss = 0.21 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:24.611853: step 197170, loss = 0.20 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:24.931418: step 197180, loss = 0.22 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:25.251063: step 197190, loss = 0.19 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:25.571000: step 197200, loss = 0.24 (7901.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:26.040661: step 197210, loss = 0.20 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:26.359381: step 197220, loss = 0.17 (8188.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:26.679590: step 197230, loss = 0.20 (7996.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:27.000181: step 197240, loss = 0.28 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:27.320982: step 197250, loss = 0.19 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:27.641845: step 197260, loss = 0.19 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:27.961646: step 197270, loss = 0.23 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:28.282096: step 197280, loss = 0.33 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:28.601825: step 197290, loss = 0.24 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:28.921496: step 197300, loss = 0.21 (8103.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:29.383704: step 197310, loss = 0.26 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:29.703036: step 197320, loss = 0.20 (7818.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:30.024522: step 197330, loss = 0.15 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:30.346627: step 197340, loss = 0.18 (7850.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:30.666919: step 197350, loss = 0.25 (7772.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:30.986355: step 197360, loss = 0.23 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:31.306005: step 197370, loss = 0.21 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:31.626354: step 197380, loss = 0.20 (8030.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:31.946877: step 197390, loss = 0.24 (8081.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:32.271182: step 197400, loss = 0.21 (8100.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:32.728654: step 197410, loss = 0.17 (7991.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:33.047614: step 197420, loss = 0.19 (8132.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:33.369203: step 197430, loss = 0.20 (7868.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:33.688214: step 197440, loss = 0.17 (8029.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:34.010239: step 197450, loss = 0.25 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:34.330044: step 197460, loss = 0.21 (7908.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:34.650830: step 197470, loss = 0.17 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:34.970814: step 197480, loss = 0.20 (7719.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:30:35.293162: step 197490, loss = 0.22 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:35.618597: step 197500, loss = 0.14 (8116.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:36.080226: step 197510, loss = 0.22 (8178.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:36.402214: step 197520, loss = 0.19 (7818.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:36.722833: step 197530, loss = 0.20 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:37.045073: step 197540, loss = 0.19 (8171.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:37.369681: step 197550, loss = 0.20 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:37.689764: step 197560, loss = 0.29 (7931.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:38.010286: step 197570, loss = 0.21 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:38.330798: step 197580, loss = 0.21 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:38.656731: step 197590, loss = 0.27 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:38.979911: step 197600, loss = 0.18 (7797.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:39.427970: step 197610, loss = 0.24 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:39.747307: step 197620, loss = 0.20 (8094.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:40.064441: step 197630, loss = 0.22 (8159.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:40.383414: step 197640, loss = 0.27 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:40.702157: step 197650, loss = 0.19 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:41.021316: step 197660, loss = 0.21 (8123.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:41.342869: step 197670, loss = 0.20 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:41.664505: step 197680, loss = 0.22 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:41.985807: step 197690, loss = 0.22 (7555.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:30:42.307748: step 197700, loss = 0.17 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:42.771246: step 197710, loss = 0.28 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:43.089412: step 197720, loss = 0.30 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:43.411201: step 197730, loss = 0.19 (7916.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:43.732054: step 197740, loss = 0.18 (7985.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:44.052380: step 197750, loss = 0.23 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:44.373733: step 197760, loss = 0.22 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:44.695450: step 197770, loss = 0.23 (8148.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:45.014123: step 197780, loss = 0.34 (8144.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:45.331479: step 197790, loss = 0.19 (8020.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:45.651337: step 197800, loss = 0.19 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:46.108760: step 197810, loss = 0.20 (7971.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:46.428812: step 197820, loss = 0.19 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:46.747357: step 197830, loss = 0.19 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:47.066312: step 197840, loss = 0.13 (8043.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:47.388346: step 197850, loss = 0.24 (7877.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:47.708220: step 197860, loss = 0.23 (7945.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:48.027421: step 197870, loss = 0.24 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:48.350085: step 197880, loss = 0.22 (7900.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:48.672597: step 197890, loss = 0.18 (7928.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:48.996167: step 197900, loss = 0.19 (7889.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:49.458951: step 197910, loss = 0.17 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:49.779074: step 197920, loss = 0.17 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:50.097554: step 197930, loss = 0.22 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:50.418081: step 197940, loss = 0.20 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:50.739232: step 197950, loss = 0.20 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:51.063416: step 197960, loss = 0.18 (7521.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:30:51.385678: step 197970, loss = 0.18 (8067.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:51.706620: step 197980, loss = 0.20 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:52.027299: step 197990, loss = 0.23 (8143.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:52.348351: step 198000, loss = 0.19 (7960.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:52.898709: step 198010, loss = 0.25 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:53.216454: step 198020, loss = 0.16 (8135.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:53.538171: step 198030, loss = 0.20 (8153.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:53.857902: step 198040, loss = 0.15 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:54.177216: step 198050, loss = 0.19 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:54.495322: step 198060, loss = 0.28 (8113.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:54.814512: step 198070, loss = 0.21 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:55.134327: step 198080, loss = 0.21 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:55.455797: step 198090, loss = 0.30 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:55.775918: step 198100, loss = 0.22 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:56.232935: step 198110, loss = 0.26 (7633.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:30:56.553243: step 198120, loss = 0.25 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:56.871300: step 198130, loss = 0.20 (8183.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:57.191223: step 198140, loss = 0.28 (8163.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:57.511316: step 198150, loss = 0.19 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:57.830017: step 198160, loss = 0.26 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:58.148114: step 198170, loss = 0.20 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:58.467243: step 198180, loss = 0.22 (8075.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:58.787047: step 198190, loss = 0.17 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:59.107031: step 198200, loss = 0.21 (7882.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:59.558933: step 198210, loss = 0.24 (7821.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:30:59.883174: step 198220, loss = 0.21 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:00.205992: step 198230, loss = 0.25 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:00.528903: step 198240, loss = 0.22 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:00.849624: step 198250, loss = 0.19 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:01.170057: step 198260, loss = 0.22 (8079.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:01.489303: step 198270, loss = 0.30 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:01.810023: step 198280, loss = 0.15 (7858.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:02.130839: step 198290, loss = 0.19 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:02.451057: step 198300, loss = 0.18 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:02.912662: step 198310, loss = 0.30 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:03.230944: step 198320, loss = 0.18 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:03.550937: step 198330, loss = 0.25 (7961.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:03.870673: step 198340, loss = 0.20 (7984.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:04.192190: step 198350, loss = 0.18 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:04.512285: step 198360, loss = 0.26 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:04.830981: step 198370, loss = 0.23 (7986.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:05.149042: step 198380, loss = 0.24 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:05.467652: step 198390, loss = 0.21 (8156.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:05.788230: step 198400, loss = 0.22 (8149.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:06.248039: step 198410, loss = 0.26 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:06.568532: step 198420, loss = 0.18 (8098.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:06.886890: step 198430, loss = 0.24 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:07.207113: step 198440, loss = 0.19 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:07.525428: step 198450, loss = 0.22 (8077.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:07.845772: step 198460, loss = 0.24 (8086.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:08.163494: step 198470, loss = 0.20 (8113.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:08.488797: step 198480, loss = 0.33 (7811.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:08.810118: step 198490, loss = 0.17 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:09.134451: step 198500, loss = 0.24 (7912.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:09.592386: step 198510, loss = 0.25 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:09.913082: step 198520, loss = 0.18 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:10.237050: step 198530, loss = 0.24 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:10.557314: step 198540, loss = 0.26 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:10.876261: step 198550, loss = 0.19 (8095.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:11.196359: step 198560, loss = 0.21 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:11.518637: step 198570, loss = 0.17 (7969.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:11.841080: step 198580, loss = 0.19 (7511.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:31:12.166035: step 198590, loss = 0.19 (7815.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:12.487407: step 198600, loss = 0.22 (7914.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:12.955097: step 198610, loss = 0.25 (8003.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:13.276681: step 198620, loss = 0.22 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:13.595938: step 198630, loss = 0.23 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:13.925874: step 198640, loss = 0.21 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:14.246660: step 198650, loss = 0.21 (8003.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:14.566686: step 198660, loss = 0.23 (8134.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:14.887939: step 198670, loss = 0.26 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:15.208021: step 198680, loss = 0.19 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:15.529901: step 198690, loss = 0.21 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:15.848251: step 198700, loss = 0.20 (8078.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:16.316871: step 198710, loss = 0.23 (7841.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:16.640863: step 198720, loss = 0.18 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:16.962965: step 198730, loss = 0.18 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:17.282342: step 198740, loss = 0.24 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:17.601821: step 198750, loss = 0.33 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:17.921196: step 198760, loss = 0.18 (8090.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:18.242744: step 198770, loss = 0.25 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:18.561521: step 198780, loss = 0.17 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:18.882421: step 198790, loss = 0.20 (7823.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:19.202199: step 198800, loss = 0.21 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:19.675134: step 198810, loss = 0.19 (7062.8 examples/sec; 0.018 sec/batch)
2017-09-16 17:31:19.995988: step 198820, loss = 0.18 (7874.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:20.316169: step 198830, loss = 0.29 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:20.635539: step 198840, loss = 0.22 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:20.954047: step 198850, loss = 0.20 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:21.275924: step 198860, loss = 0.21 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:21.596212: step 198870, loss = 0.18 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:21.917940: step 198880, loss = 0.20 (7956.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:22.238116: step 198890, loss = 0.20 (8028.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:22.560566: step 198900, loss = 0.20 (8083.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:23.018552: step 198910, loss = 0.24 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:23.338220: step 198920, loss = 0.18 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:23.658525: step 198930, loss = 0.21 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:23.978861: step 198940, loss = 0.32 (8103.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:24.299485: step 198950, loss = 0.21 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:24.624493: step 198960, loss = 0.13 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:24.946567: step 198970, loss = 0.22 (7911.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:25.268868: step 198980, loss = 0.19 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:25.589725: step 198990, loss = 0.21 (8081.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:25.910093: step 199000, loss = 0.20 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:26.477683: step 199010, loss = 0.25 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:26.799526: step 199020, loss = 0.20 (7858.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:27.120912: step 199030, loss = 0.23 (7998.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:27.440344: step 199040, loss = 0.20 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:27.762845: step 199050, loss = 0.20 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:28.084372: step 199060, loss = 0.17 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:28.402422: step 199070, loss = 0.17 (8079.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:28.723459: step 199080, loss = 0.21 (7920.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:29.043939: step 199090, loss = 0.16 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:29.363620: step 199100, loss = 0.19 (8127.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:29.819648: step 199110, loss = 0.17 (8095.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:30.140895: step 199120, loss = 0.18 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:30.461040: step 199130, loss = 0.17 (7835.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:30.778922: step 199140, loss = 0.19 (8132.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:31.101089: step 199150, loss = 0.17 (7959.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:31.420184: step 199160, loss = 0.24 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:31.741905: step 199170, loss = 0.17 (7847.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:32.061275: step 199180, loss = 0.26 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:32.384361: step 199190, loss = 0.22 (7715.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:31:32.706212: step 199200, loss = 0.19 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:33.168969: step 199210, loss = 0.21 (7890.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:33.488584: step 199220, loss = 0.21 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:33.809697: step 199230, loss = 0.22 (8133.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:34.130462: step 199240, loss = 0.19 (7980.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:34.450698: step 199250, loss = 0.31 (8124.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:34.772865: step 199260, loss = 0.28 (7823.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:35.096291: step 199270, loss = 0.19 (7541.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:31:35.417051: step 199280, loss = 0.19 (7778.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:35.737730: step 199290, loss = 0.19 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:36.058892: step 199300, loss = 0.28 (8164.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:36.516249: step 199310, loss = 0.18 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:36.836270: step 199320, loss = 0.34 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:37.158913: step 199330, loss = 0.23 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:37.480387: step 199340, loss = 0.25 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:37.801337: step 199350, loss = 0.19 (7862.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:38.123392: step 199360, loss = 0.24 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:38.444013: step 199370, loss = 0.23 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:38.764984: step 199380, loss = 0.26 (8043.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:39.086033: step 199390, loss = 0.17 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:39.406307: step 199400, loss = 0.20 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:39.865342: step 199410, loss = 0.17 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:40.183896: step 199420, loss = 0.23 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:40.505374: step 199430, loss = 0.28 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:40.826856: step 199440, loss = 0.17 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:41.153725: step 199450, loss = 0.21 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:41.473039: step 199460, loss = 0.19 (7917.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:41.793972: step 199470, loss = 0.18 (7854.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:42.113726: step 199480, loss = 0.21 (7892.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:42.434367: step 199490, loss = 0.28 (7981.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:42.754180: step 199500, loss = 0.21 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:43.223622: step 199510, loss = 0.18 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:43.543863: step 199520, loss = 0.20 (8015.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:43.865369: step 199530, loss = 0.14 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:44.185724: step 199540, loss = 0.21 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:44.509175: step 199550, loss = 0.14 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:44.830542: step 199560, loss = 0.22 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:45.151964: step 199570, loss = 0.24 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:45.472783: step 199580, loss = 0.17 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:45.793182: step 199590, loss = 0.22 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:46.113173: step 199600, loss = 0.16 (8126.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:46.578060: step 199610, loss = 0.21 (8052.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:46.903121: step 199620, loss = 0.23 (8103.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:47.223472: step 199630, loss = 0.24 (7709.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:31:47.545693: step 199640, loss = 0.20 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:47.865498: step 199650, loss = 0.19 (7924.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:48.184801: step 199660, loss = 0.18 (8040.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:48.504012: step 199670, loss = 0.21 (8128.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:48.826718: step 199680, loss = 0.28 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:49.151282: step 199690, loss = 0.21 (7860.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:49.472508: step 199700, loss = 0.16 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:49.938286: step 199710, loss = 0.19 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:50.259372: step 199720, loss = 0.27 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:50.581071: step 199730, loss = 0.22 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:50.900601: step 199740, loss = 0.21 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:51.221809: step 199750, loss = 0.21 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:51.541812: step 199760, loss = 0.22 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:51.863446: step 199770, loss = 0.21 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:52.184625: step 199780, loss = 0.15 (8112.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:52.505323: step 199790, loss = 0.23 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:52.824303: step 199800, loss = 0.21 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:53.281757: step 199810, loss = 0.23 (8054.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:53.600790: step 199820, loss = 0.22 (8022.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:53.921402: step 199830, loss = 0.23 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:54.243815: step 199840, loss = 0.23 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:54.563950: step 199850, loss = 0.16 (7928.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:54.884714: step 199860, loss = 0.19 (7886.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:55.204932: step 199870, loss = 0.21 (7915.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:55.527975: step 199880, loss = 0.16 (7952.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:55.850268: step 199890, loss = 0.23 (7974.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:56.170233: step 199900, loss = 0.17 (8041.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:56.625685: step 199910, loss = 0.21 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:56.947464: step 199920, loss = 0.23 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:57.267406: step 199930, loss = 0.22 (7946.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:57.588481: step 199940, loss = 0.25 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:57.911919: step 199950, loss = 0.21 (8031.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:58.232235: step 199960, loss = 0.17 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:58.551268: step 199970, loss = 0.28 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:58.872057: step 199980, loss = 0.25 (7831.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:59.191952: step 199990, loss = 0.18 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:31:59.509976: step 200000, loss = 0.21 (8042.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:00.139194: step 200010, loss = 0.21 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:00.460090: step 200020, loss = 0.18 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:00.783720: step 200030, loss = 0.17 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:01.106113: step 200040, loss = 0.21 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:01.426694: step 200050, loss = 0.17 (7921.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:01.747687: step 200060, loss = 0.23 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:02.068533: step 200070, loss = 0.15 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:02.390218: step 200080, loss = 0.21 (8002.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:02.715070: step 200090, loss = 0.21 (7968.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:03.037867: step 200100, loss = 0.18 (8051.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:03.501552: step 200110, loss = 0.15 (7947.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:03.823816: step 200120, loss = 0.26 (7945.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:04.142632: step 200130, loss = 0.20 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:04.463151: step 200140, loss = 0.24 (7956.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:04.783940: step 200150, loss = 0.24 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:05.104681: step 200160, loss = 0.25 (8002.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:05.425025: step 200170, loss = 0.26 (7915.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:05.744571: step 200180, loss = 0.16 (7887.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:06.064927: step 200190, loss = 0.20 (8131.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:06.382968: step 200200, loss = 0.18 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:06.853889: step 200210, loss = 0.23 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:07.176858: step 200220, loss = 0.15 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:07.496629: step 200230, loss = 0.19 (7773.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:07.815821: step 200240, loss = 0.24 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:08.136301: step 200250, loss = 0.17 (8112.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:08.454463: step 200260, loss = 0.20 (7886.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:08.774474: step 200270, loss = 0.23 (7909.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:09.092561: step 200280, loss = 0.20 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:09.415062: step 200290, loss = 0.21 (8037.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:09.734464: step 200300, loss = 0.23 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:10.200988: step 200310, loss = 0.16 (7741.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:10.521619: step 200320, loss = 0.24 (8011.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:10.844003: step 200330, loss = 0.24 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:11.163242: step 200340, loss = 0.26 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:11.487046: step 200350, loss = 0.20 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:11.806417: step 200360, loss = 0.26 (7954.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:12.124952: step 200370, loss = 0.23 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:12.447217: step 200380, loss = 0.17 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:12.769718: step 200390, loss = 0.17 (7968.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:13.091197: step 200400, loss = 0.23 (8006.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:13.545919: step 200410, loss = 0.20 (7849.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:13.869301: step 200420, loss = 0.19 (8081.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:14.187671: step 200430, loss = 0.20 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:14.506980: step 200440, loss = 0.19 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:14.829387: step 200450, loss = 0.16 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:15.148241: step 200460, loss = 0.23 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:15.472353: step 200470, loss = 0.15 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:15.792900: step 200480, loss = 0.16 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:16.112627: step 200490, loss = 0.17 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:16.435134: step 200500, loss = 0.22 (7885.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:16.888716: step 200510, loss = 0.20 (7814.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:17.208478: step 200520, loss = 0.16 (7948.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:17.527727: step 200530, loss = 0.19 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:17.850568: step 200540, loss = 0.24 (7369.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:18.180064: step 200550, loss = 0.25 (7944.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:18.501129: step 200560, loss = 0.32 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:18.823162: step 200570, loss = 0.18 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:19.142056: step 200580, loss = 0.23 (8102.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:19.462740: step 200590, loss = 0.18 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:19.781993: step 200600, loss = 0.18 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:20.242719: step 200610, loss = 0.22 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:20.563623: step 200620, loss = 0.17 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:20.882464: step 200630, loss = 0.26 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:21.204095: step 200640, loss = 0.19 (7676.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:21.526008: step 200650, loss = 0.20 (7667.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:21.846053: step 200660, loss = 0.23 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:22.167559: step 200670, loss = 0.19 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:22.487803: step 200680, loss = 0.26 (7831.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:22.809065: step 200690, loss = 0.24 (7814.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:23.128862: step 200700, loss = 0.19 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:23.586732: step 200710, loss = 0.20 (8098.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:23.907268: step 200720, loss = 0.25 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:24.226749: step 200730, loss = 0.22 (7811.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:24.546880: step 200740, loss = 0.21 (7989.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:24.867195: step 200750, loss = 0.17 (8072.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:25.191227: step 200760, loss = 0.28 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:25.510775: step 200770, loss = 0.21 (8014.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:25.828393: step 200780, loss = 0.19 (8119.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:26.148537: step 200790, loss = 0.13 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:26.470223: step 200800, loss = 0.26 (7885.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:26.935266: step 200810, loss = 0.22 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:27.258259: step 200820, loss = 0.17 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:27.578633: step 200830, loss = 0.30 (8090.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:27.899412: step 200840, loss = 0.20 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:28.218695: step 200850, loss = 0.21 (8147.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:28.537546: step 200860, loss = 0.18 (8132.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:28.859722: step 200870, loss = 0.24 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:29.179212: step 200880, loss = 0.23 (8059.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:29.498868: step 200890, loss = 0.19 (8052.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:29.820275: step 200900, loss = 0.19 (8128.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:30.270912: step 200910, loss = 0.23 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:30.592978: step 200920, loss = 0.25 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:30.911522: step 200930, loss = 0.26 (8020.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:31.232141: step 200940, loss = 0.12 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:31.551542: step 200950, loss = 0.23 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:31.872092: step 200960, loss = 0.23 (8037.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:32.192749: step 200970, loss = 0.18 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:32.515238: step 200980, loss = 0.21 (7972.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:32.837334: step 200990, loss = 0.16 (7474.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:33.157556: step 201000, loss = 0.18 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:33.719886: step 201010, loss = 0.22 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:34.039577: step 201020, loss = 0.23 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:34.359001: step 201030, loss = 0.23 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:34.685277: step 201040, loss = 0.17 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:35.006208: step 201050, loss = 0.15 (8154.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:35.326249: step 201060, loss = 0.17 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:35.648279: step 201070, loss = 0.25 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:35.972147: step 201080, loss = 0.20 (7847.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:36.291077: step 201090, loss = 0.23 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:36.611403: step 201100, loss = 0.17 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:37.062108: step 201110, loss = 0.19 (7653.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:32:37.384635: step 201120, loss = 0.23 (7855.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:37.705193: step 201130, loss = 0.25 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:38.028555: step 201140, loss = 0.14 (8117.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:38.349606: step 201150, loss = 0.21 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:38.669013: step 201160, loss = 0.21 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:38.989773: step 201170, loss = 0.27 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:39.311390: step 201180, loss = 0.20 (7930.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:39.631138: step 201190, loss = 0.23 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:39.953113: step 201200, loss = 0.17 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:40.416956: step 201210, loss = 0.22 (7919.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:40.738363: step 201220, loss = 0.15 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:41.057088: step 201230, loss = 0.18 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:41.378406: step 201240, loss = 0.20 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:41.698866: step 201250, loss = 0.19 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:42.018880: step 201260, loss = 0.20 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:42.337960: step 201270, loss = 0.15 (8065.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:42.659680: step 201280, loss = 0.31 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:42.979089: step 201290, loss = 0.27 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:43.297798: step 201300, loss = 0.23 (8108.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:43.750384: step 201310, loss = 0.16 (8041.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:44.071261: step 201320, loss = 0.15 (8098.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:44.392216: step 201330, loss = 0.22 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:44.715282: step 201340, loss = 0.17 (8201.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:45.035571: step 201350, loss = 0.18 (8090.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:45.353340: step 201360, loss = 0.24 (8150.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:45.673649: step 201370, loss = 0.19 (7877.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:45.994457: step 201380, loss = 0.21 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:46.314191: step 201390, loss = 0.22 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:46.635158: step 201400, loss = 0.24 (7996.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:47.099404: step 201410, loss = 0.22 (7960.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:47.421198: step 201420, loss = 0.18 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:47.740345: step 201430, loss = 0.18 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:48.065899: step 201440, loss = 0.19 (7907.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:48.386457: step 201450, loss = 0.19 (7994.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:48.709715: step 201460, loss = 0.19 (7903.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:49.029660: step 201470, loss = 0.17 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:49.352125: step 201480, loss = 0.19 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:49.671296: step 201490, loss = 0.21 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:49.991524: step 201500, loss = 0.21 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:50.444430: step 201510, loss = 0.22 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:50.763982: step 201520, loss = 0.21 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:51.087951: step 201530, loss = 0.21 (7899.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:51.408523: step 201540, loss = 0.25 (7965.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:51.726126: step 201550, loss = 0.29 (7980.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:52.045736: step 201560, loss = 0.22 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:52.367778: step 201570, loss = 0.19 (8085.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:52.689312: step 201580, loss = 0.24 (7861.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:53.007920: step 201590, loss = 0.17 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:53.328231: step 201600, loss = 0.17 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:53.787831: step 201610, loss = 0.23 (8088.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:54.108179: step 201620, loss = 0.18 (7904.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:54.428185: step 201630, loss = 0.20 (7893.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:54.748327: step 201640, loss = 0.17 (8006.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:55.067893: step 201650, loss = 0.19 (7896.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:55.386156: step 201660, loss = 0.25 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:55.707529: step 201670, loss = 0.17 (7820.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:56.029810: step 201680, loss = 0.24 (8015.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:56.348714: step 201690, loss = 0.23 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:56.669788: step 201700, loss = 0.19 (7889.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:57.136166: step 201710, loss = 0.19 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:57.455074: step 201720, loss = 0.24 (8021.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:57.774604: step 201730, loss = 0.24 (7844.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:58.095694: step 201740, loss = 0.27 (7877.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:58.415676: step 201750, loss = 0.20 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:58.737060: step 201760, loss = 0.21 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:59.055573: step 201770, loss = 0.17 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:59.375665: step 201780, loss = 0.21 (7955.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:32:59.696884: step 201790, loss = 0.16 (7936.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:00.017271: step 201800, loss = 0.20 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:00.473290: step 201810, loss = 0.26 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:00.796089: step 201820, loss = 0.18 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:01.115087: step 201830, loss = 0.24 (8101.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:01.438917: step 201840, loss = 0.21 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:01.757677: step 201850, loss = 0.22 (8114.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:02.077093: step 201860, loss = 0.28 (8100.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:02.397630: step 201870, loss = 0.22 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:02.720471: step 201880, loss = 0.17 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:03.041495: step 201890, loss = 0.21 (8046.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:03.364609: step 201900, loss = 0.21 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:03.825471: step 201910, loss = 0.25 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:04.144789: step 201920, loss = 0.23 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:04.466782: step 201930, loss = 0.18 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:04.786767: step 201940, loss = 0.17 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:05.108606: step 201950, loss = 0.23 (8028.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:05.431275: step 201960, loss = 0.24 (7756.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:05.750669: step 201970, loss = 0.24 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:06.071005: step 201980, loss = 0.18 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:06.391104: step 201990, loss = 0.21 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:06.713152: step 202000, loss = 0.15 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:07.274302: step 202010, loss = 0.29 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:07.592945: step 202020, loss = 0.21 (8125.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:07.914058: step 202030, loss = 0.19 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:08.235658: step 202040, loss = 0.19 (7923.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:08.556093: step 202050, loss = 0.23 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:08.876157: step 202060, loss = 0.24 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:09.197075: step 202070, loss = 0.16 (7904.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:09.517708: step 202080, loss = 0.22 (7914.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:09.835907: step 202090, loss = 0.16 (8145.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:10.154694: step 202100, loss = 0.24 (8085.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:10.624089: step 202110, loss = 0.16 (7573.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:10.949229: step 202120, loss = 0.17 (7862.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:11.269940: step 202130, loss = 0.19 (7967.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:11.590978: step 202140, loss = 0.19 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:11.911905: step 202150, loss = 0.18 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:12.233101: step 202160, loss = 0.23 (8149.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:12.552947: step 202170, loss = 0.21 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:12.871952: step 202180, loss = 0.21 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:13.192239: step 202190, loss = 0.21 (7979.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:13.513743: step 202200, loss = 0.18 (7815.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:13.979796: step 202210, loss = 0.23 (7871.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:14.299047: step 202220, loss = 0.16 (8045.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:14.619866: step 202230, loss = 0.20 (7988.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:14.940148: step 202240, loss = 0.23 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:15.261972: step 202250, loss = 0.22 (7686.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:15.581303: step 202260, loss = 0.24 (7993.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:15.902587: step 202270, loss = 0.17 (7429.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:16.229205: step 202280, loss = 0.16 (7957.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:16.550797: step 202290, loss = 0.22 (8053.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:16.870532: step 202300, loss = 0.20 (8043.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:17.335878: step 202310, loss = 0.20 (8050.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:17.658093: step 202320, loss = 0.22 (8038.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:17.981394: step 202330, loss = 0.23 (7993.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:18.302099: step 202340, loss = 0.18 (7942.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:18.623506: step 202350, loss = 0.31 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:18.943517: step 202360, loss = 0.18 (8016.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:19.262797: step 202370, loss = 0.18 (8045.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:19.583631: step 202380, loss = 0.20 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:19.905107: step 202390, loss = 0.24 (8020.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:20.231605: step 202400, loss = 0.25 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:20.680895: step 202410, loss = 0.23 (7983.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:21.003287: step 202420, loss = 0.14 (7933.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:21.321994: step 202430, loss = 0.21 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:21.643149: step 202440, loss = 0.24 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:21.963524: step 202450, loss = 0.30 (7888.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:22.286245: step 202460, loss = 0.22 (7684.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:22.607742: step 202470, loss = 0.19 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:22.931733: step 202480, loss = 0.19 (7611.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:23.252525: step 202490, loss = 0.27 (7997.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:23.570902: step 202500, loss = 0.22 (8061.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:24.029076: step 202510, loss = 0.14 (7728.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:24.354496: step 202520, loss = 0.22 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:24.673807: step 202530, loss = 0.20 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:24.996260: step 202540, loss = 0.22 (7918.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:25.317710: step 202550, loss = 0.20 (7870.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:25.640656: step 202560, loss = 0.24 (8161.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:25.963517: step 202570, loss = 0.18 (7870.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:26.283076: step 202580, loss = 0.24 (7999.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:26.603483: step 202590, loss = 0.23 (7904.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:26.925034: step 202600, loss = 0.18 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:27.374317: step 202610, loss = 0.24 (7898.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:27.693072: step 202620, loss = 0.18 (8095.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:28.013034: step 202630, loss = 0.20 (8154.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:28.332541: step 202640, loss = 0.14 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:28.652556: step 202650, loss = 0.20 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:28.972500: step 202660, loss = 0.32 (7810.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:29.293515: step 202670, loss = 0.26 (8008.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:29.611778: step 202680, loss = 0.16 (8027.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:29.931588: step 202690, loss = 0.27 (8042.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:30.258263: step 202700, loss = 0.21 (7575.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:30.719043: step 202710, loss = 0.19 (7885.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:31.041421: step 202720, loss = 0.19 (8142.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:31.363511: step 202730, loss = 0.23 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:31.682154: step 202740, loss = 0.30 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:32.002844: step 202750, loss = 0.22 (7863.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:32.323388: step 202760, loss = 0.24 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:32.648098: step 202770, loss = 0.18 (8008.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:32.967820: step 202780, loss = 0.13 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:33.287290: step 202790, loss = 0.19 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:33.608299: step 202800, loss = 0.21 (8047.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:34.070604: step 202810, loss = 0.14 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:34.394917: step 202820, loss = 0.20 (7972.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:34.712967: step 202830, loss = 0.23 (8092.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:35.033740: step 202840, loss = 0.21 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:35.353756: step 202850, loss = 0.22 (7824.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:35.673135: step 202860, loss = 0.22 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:35.993511: step 202870, loss = 0.19 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:36.312703: step 202880, loss = 0.19 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:36.630999: step 202890, loss = 0.20 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:36.952234: step 202900, loss = 0.17 (7948.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:37.404431: step 202910, loss = 0.17 (7977.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:37.724978: step 202920, loss = 0.19 (7909.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:38.044502: step 202930, loss = 0.20 (7922.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:38.365578: step 202940, loss = 0.17 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:38.687823: step 202950, loss = 0.24 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:39.007964: step 202960, loss = 0.19 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:39.327655: step 202970, loss = 0.17 (7893.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:39.648765: step 202980, loss = 0.23 (8096.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:39.968212: step 202990, loss = 0.17 (8186.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:40.288747: step 203000, loss = 0.27 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:40.860193: step 203010, loss = 0.20 (7693.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:41.181839: step 203020, loss = 0.22 (7585.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:41.504853: step 203030, loss = 0.19 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:41.826008: step 203040, loss = 0.15 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:42.146322: step 203050, loss = 0.24 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:42.467156: step 203060, loss = 0.17 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:42.787499: step 203070, loss = 0.22 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:43.108133: step 203080, loss = 0.27 (7967.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:43.428461: step 203090, loss = 0.25 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:43.748944: step 203100, loss = 0.16 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:44.214700: step 203110, loss = 0.35 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:44.535691: step 203120, loss = 0.22 (8111.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:44.855501: step 203130, loss = 0.17 (8062.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:45.175641: step 203140, loss = 0.27 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:45.494925: step 203150, loss = 0.19 (8009.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:45.816026: step 203160, loss = 0.20 (7858.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:46.139438: step 203170, loss = 0.24 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:46.459979: step 203180, loss = 0.22 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:46.780995: step 203190, loss = 0.16 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:47.102131: step 203200, loss = 0.25 (7924.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:47.565558: step 203210, loss = 0.16 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:47.887024: step 203220, loss = 0.18 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:48.209455: step 203230, loss = 0.21 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:48.531351: step 203240, loss = 0.17 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:48.851217: step 203250, loss = 0.20 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:49.173332: step 203260, loss = 0.16 (7986.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:49.495342: step 203270, loss = 0.18 (7838.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:49.812782: step 203280, loss = 0.21 (8086.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:50.133245: step 203290, loss = 0.19 (7902.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:50.453463: step 203300, loss = 0.24 (8024.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:50.927752: step 203310, loss = 0.24 (7840.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:51.250028: step 203320, loss = 0.25 (7960.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:51.571097: step 203330, loss = 0.20 (7575.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:51.891744: step 203340, loss = 0.21 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:52.212026: step 203350, loss = 0.18 (8040.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:52.531270: step 203360, loss = 0.20 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:52.850141: step 203370, loss = 0.19 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:53.170763: step 203380, loss = 0.18 (8069.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:53.493974: step 203390, loss = 0.18 (8122.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:53.814916: step 203400, loss = 0.18 (7742.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:54.279720: step 203410, loss = 0.18 (7825.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:54.603308: step 203420, loss = 0.23 (7777.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:54.922611: step 203430, loss = 0.21 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:55.242169: step 203440, loss = 0.23 (7947.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:55.571368: step 203450, loss = 0.23 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:55.892447: step 203460, loss = 0.20 (7868.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:56.211171: step 203470, loss = 0.20 (8136.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:56.530992: step 203480, loss = 0.20 (7978.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:56.852506: step 203490, loss = 0.18 (7877.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:57.179220: step 203500, loss = 0.17 (8128.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:57.647884: step 203510, loss = 0.18 (8085.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:57.969524: step 203520, loss = 0.26 (7779.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:58.290587: step 203530, loss = 0.23 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:58.610050: step 203540, loss = 0.22 (8035.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:58.931009: step 203550, loss = 0.16 (7740.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:33:59.251201: step 203560, loss = 0.23 (8129.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:59.571738: step 203570, loss = 0.20 (8145.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:33:59.893370: step 203580, loss = 0.18 (8087.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:00.212988: step 203590, loss = 0.21 (7987.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:00.533637: step 203600, loss = 0.16 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:00.997240: step 203610, loss = 0.24 (8044.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:01.319103: step 203620, loss = 0.17 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:01.640000: step 203630, loss = 0.18 (8056.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:01.960109: step 203640, loss = 0.20 (8083.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:02.280494: step 203650, loss = 0.19 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:02.600668: step 203660, loss = 0.23 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:02.917938: step 203670, loss = 0.25 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:03.238781: step 203680, loss = 0.25 (8019.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:03.561919: step 203690, loss = 0.18 (7835.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:03.884096: step 203700, loss = 0.18 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:04.336608: step 203710, loss = 0.19 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:04.656832: step 203720, loss = 0.33 (8043.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:04.984365: step 203730, loss = 0.21 (8177.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:05.306197: step 203740, loss = 0.23 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:05.629447: step 203750, loss = 0.18 (7910.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:05.950670: step 203760, loss = 0.21 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:06.269250: step 203770, loss = 0.26 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:06.590040: step 203780, loss = 0.20 (7872.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:06.913019: step 203790, loss = 0.21 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:07.234975: step 203800, loss = 0.17 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:07.699445: step 203810, loss = 0.18 (7678.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:08.022043: step 203820, loss = 0.16 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:08.342476: step 203830, loss = 0.26 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:08.663410: step 203840, loss = 0.25 (8104.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:08.985227: step 203850, loss = 0.20 (7866.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:09.305497: step 203860, loss = 0.23 (7978.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:09.624705: step 203870, loss = 0.25 (7939.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:09.945580: step 203880, loss = 0.23 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:10.265904: step 203890, loss = 0.17 (7953.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:10.587417: step 203900, loss = 0.19 (8190.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:11.050765: step 203910, loss = 0.21 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:11.370675: step 203920, loss = 0.22 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:11.693005: step 203930, loss = 0.18 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:12.010964: step 203940, loss = 0.16 (8105.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:12.334056: step 203950, loss = 0.14 (7919.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:12.655006: step 203960, loss = 0.24 (8057.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:12.975990: step 203970, loss = 0.27 (7848.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:13.295638: step 203980, loss = 0.23 (7979.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:13.615292: step 203990, loss = 0.19 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:13.936499: step 204000, loss = 0.21 (8126.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:14.495116: step 204010, loss = 0.18 (7745.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:14.814046: step 204020, loss = 0.18 (8070.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:15.140950: step 204030, loss = 0.28 (7899.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:15.460648: step 204040, loss = 0.18 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:15.780443: step 204050, loss = 0.19 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:16.101475: step 204060, loss = 0.22 (7906.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:16.422871: step 204070, loss = 0.23 (8035.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:16.745904: step 204080, loss = 0.20 (8085.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:17.064862: step 204090, loss = 0.23 (8106.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:17.385840: step 204100, loss = 0.27 (7994.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:17.848658: step 204110, loss = 0.27 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:18.172285: step 204120, loss = 0.19 (7932.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:18.492887: step 204130, loss = 0.24 (7848.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:18.813307: step 204140, loss = 0.21 (8047.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:19.134791: step 204150, loss = 0.18 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:19.455658: step 204160, loss = 0.20 (7847.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:19.778670: step 204170, loss = 0.20 (7801.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:20.098713: step 204180, loss = 0.21 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:20.417875: step 204190, loss = 0.20 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:20.737142: step 204200, loss = 0.17 (8074.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:21.197377: step 204210, loss = 0.20 (7981.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:21.516784: step 204220, loss = 0.29 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:21.835248: step 204230, loss = 0.23 (7968.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:22.157694: step 204240, loss = 0.17 (7766.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:22.478390: step 204250, loss = 0.16 (7895.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:22.799293: step 204260, loss = 0.21 (7992.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:23.117558: step 204270, loss = 0.25 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:23.437787: step 204280, loss = 0.21 (8146.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:23.761670: step 204290, loss = 0.24 (8151.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:24.081612: step 204300, loss = 0.17 (7866.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:24.545065: step 204310, loss = 0.17 (8008.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:24.864754: step 204320, loss = 0.24 (8083.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:25.186037: step 204330, loss = 0.20 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:25.506313: step 204340, loss = 0.16 (7880.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:25.827826: step 204350, loss = 0.24 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:26.152621: step 204360, loss = 0.18 (7593.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:26.471016: step 204370, loss = 0.24 (7955.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:26.797669: step 204380, loss = 0.18 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:27.117114: step 204390, loss = 0.21 (8067.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:27.435143: step 204400, loss = 0.18 (8119.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:27.895673: step 204410, loss = 0.19 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:28.215850: step 204420, loss = 0.16 (8057.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:28.536993: step 204430, loss = 0.20 (7586.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:28.855898: step 204440, loss = 0.21 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:29.179634: step 204450, loss = 0.17 (8028.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:29.498929: step 204460, loss = 0.16 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:29.818584: step 204470, loss = 0.17 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:30.138346: step 204480, loss = 0.18 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:30.457916: step 204490, loss = 0.18 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:30.780404: step 204500, loss = 0.21 (7463.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:31.242614: step 204510, loss = 0.21 (8048.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:31.563198: step 204520, loss = 0.18 (7959.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:31.883747: step 204530, loss = 0.16 (7843.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:32.204067: step 204540, loss = 0.17 (7992.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:32.523357: step 204550, loss = 0.18 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:32.849013: step 204560, loss = 0.15 (7486.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:33.170074: step 204570, loss = 0.22 (7987.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:33.492154: step 204580, loss = 0.21 (7987.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:33.815094: step 204590, loss = 0.25 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:34.135975: step 204600, loss = 0.22 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:34.603706: step 204610, loss = 0.17 (7844.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:34.924171: step 204620, loss = 0.20 (7991.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:35.246587: step 204630, loss = 0.20 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:35.569713: step 204640, loss = 0.21 (8025.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:35.889071: step 204650, loss = 0.18 (7977.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:36.209499: step 204660, loss = 0.20 (7970.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:36.531169: step 204670, loss = 0.20 (8073.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:36.851324: step 204680, loss = 0.13 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:37.171282: step 204690, loss = 0.23 (7900.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:37.489599: step 204700, loss = 0.17 (8025.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:37.945245: step 204710, loss = 0.27 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:38.267357: step 204720, loss = 0.20 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:38.589378: step 204730, loss = 0.25 (7584.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:38.910892: step 204740, loss = 0.25 (7952.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:39.232899: step 204750, loss = 0.18 (8023.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:39.555756: step 204760, loss = 0.17 (7973.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:39.876353: step 204770, loss = 0.23 (7969.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:40.196758: step 204780, loss = 0.19 (7975.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:40.518990: step 204790, loss = 0.19 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:40.839146: step 204800, loss = 0.18 (7842.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:41.309159: step 204810, loss = 0.26 (7845.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:41.632578: step 204820, loss = 0.24 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:41.954592: step 204830, loss = 0.20 (7681.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:42.277051: step 204840, loss = 0.17 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:42.597546: step 204850, loss = 0.19 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:42.916765: step 204860, loss = 0.26 (7994.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:43.238045: step 204870, loss = 0.25 (7797.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:43.556086: step 204880, loss = 0.25 (8117.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:43.874833: step 204890, loss = 0.15 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:44.197280: step 204900, loss = 0.20 (7791.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:44.648822: step 204910, loss = 0.18 (7973.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:44.969826: step 204920, loss = 0.19 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:45.291416: step 204930, loss = 0.16 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:45.612260: step 204940, loss = 0.22 (8024.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:45.934114: step 204950, loss = 0.29 (8052.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:46.255092: step 204960, loss = 0.27 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:46.574227: step 204970, loss = 0.19 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:46.894526: step 204980, loss = 0.20 (7816.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:47.216419: step 204990, loss = 0.21 (8045.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:47.534526: step 205000, loss = 0.21 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:48.089306: step 205010, loss = 0.21 (7522.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:48.408184: step 205020, loss = 0.24 (8032.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:48.732797: step 205030, loss = 0.19 (7846.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:49.054350: step 205040, loss = 0.20 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:49.374534: step 205050, loss = 0.17 (8021.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:49.695802: step 205060, loss = 0.23 (8064.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:50.015537: step 205070, loss = 0.14 (7966.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:50.337181: step 205080, loss = 0.24 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:50.657115: step 205090, loss = 0.24 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:50.977259: step 205100, loss = 0.18 (7851.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:51.443571: step 205110, loss = 0.21 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:51.764314: step 205120, loss = 0.18 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:52.085681: step 205130, loss = 0.19 (8060.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:52.409095: step 205140, loss = 0.21 (7496.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:34:52.729602: step 205150, loss = 0.23 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:53.055123: step 205160, loss = 0.25 (7873.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:53.377500: step 205170, loss = 0.28 (7886.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:53.699441: step 205180, loss = 0.17 (7935.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:54.021290: step 205190, loss = 0.19 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:54.345797: step 205200, loss = 0.20 (7897.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:54.803667: step 205210, loss = 0.24 (7868.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:55.123041: step 205220, loss = 0.17 (8112.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:55.445328: step 205230, loss = 0.22 (7929.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:55.766574: step 205240, loss = 0.22 (7987.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:56.086259: step 205250, loss = 0.23 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:56.406945: step 205260, loss = 0.17 (8011.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:56.727162: step 205270, loss = 0.16 (8038.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:57.052998: step 205280, loss = 0.24 (7993.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:57.373780: step 205290, loss = 0.25 (8082.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:57.692484: step 205300, loss = 0.16 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:58.157678: step 205310, loss = 0.18 (8010.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:58.479008: step 205320, loss = 0.23 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:58.803093: step 205330, loss = 0.26 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:59.123953: step 205340, loss = 0.19 (8036.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:59.445402: step 205350, loss = 0.26 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:34:59.769375: step 205360, loss = 0.30 (7976.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:00.090255: step 205370, loss = 0.25 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:00.408455: step 205380, loss = 0.27 (8015.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:00.729569: step 205390, loss = 0.17 (8153.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:01.050095: step 205400, loss = 0.26 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:01.514121: step 205410, loss = 0.22 (7897.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:01.836867: step 205420, loss = 0.20 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:02.156286: step 205430, loss = 0.26 (8021.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:02.477942: step 205440, loss = 0.24 (7944.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:02.798464: step 205450, loss = 0.19 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:03.119385: step 205460, loss = 0.24 (7970.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:03.438083: step 205470, loss = 0.21 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:03.757709: step 205480, loss = 0.26 (7990.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:04.077168: step 205490, loss = 0.21 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:04.400051: step 205500, loss = 0.20 (7825.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:04.872552: step 205510, loss = 0.20 (7463.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:05.192586: step 205520, loss = 0.21 (7969.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:05.516992: step 205530, loss = 0.20 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:05.839636: step 205540, loss = 0.21 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:06.159176: step 205550, loss = 0.23 (7765.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:06.480967: step 205560, loss = 0.18 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:06.800965: step 205570, loss = 0.22 (7794.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:07.121947: step 205580, loss = 0.21 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:07.444186: step 205590, loss = 0.20 (8091.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:07.766099: step 205600, loss = 0.19 (8022.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:08.232516: step 205610, loss = 0.24 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:08.555479: step 205620, loss = 0.24 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:08.878484: step 205630, loss = 0.20 (7855.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:09.201736: step 205640, loss = 0.18 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:09.522480: step 205650, loss = 0.14 (7918.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:09.842044: step 205660, loss = 0.24 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:10.162390: step 205670, loss = 0.20 (8077.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:10.482697: step 205680, loss = 0.26 (8056.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:10.804284: step 205690, loss = 0.18 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:11.127093: step 205700, loss = 0.28 (7549.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:11.592722: step 205710, loss = 0.15 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:11.912350: step 205720, loss = 0.21 (8110.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:12.232009: step 205730, loss = 0.22 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:12.552504: step 205740, loss = 0.17 (8073.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:12.874295: step 205750, loss = 0.18 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:13.194941: step 205760, loss = 0.14 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:13.513326: step 205770, loss = 0.27 (8034.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:13.832808: step 205780, loss = 0.17 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:14.153952: step 205790, loss = 0.14 (7921.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:14.478382: step 205800, loss = 0.16 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:14.944164: step 205810, loss = 0.23 (8053.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:15.264081: step 205820, loss = 0.26 (7998.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:15.584441: step 205830, loss = 0.17 (7937.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:15.903899: step 205840, loss = 0.21 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:16.227505: step 205850, loss = 0.17 (7971.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:16.548395: step 205860, loss = 0.18 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:16.870074: step 205870, loss = 0.20 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:17.190959: step 205880, loss = 0.24 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:17.510513: step 205890, loss = 0.24 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:17.828729: step 205900, loss = 0.17 (8100.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:18.296781: step 205910, loss = 0.17 (7909.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:18.619408: step 205920, loss = 0.19 (7921.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:18.942691: step 205930, loss = 0.21 (8076.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:19.264403: step 205940, loss = 0.23 (8086.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:19.583602: step 205950, loss = 0.17 (8088.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:19.903434: step 205960, loss = 0.25 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:20.224011: step 205970, loss = 0.17 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:20.543333: step 205980, loss = 0.17 (7965.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:20.861985: step 205990, loss = 0.19 (7902.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:21.185644: step 206000, loss = 0.29 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:21.747894: step 206010, loss = 0.21 (8057.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:22.067309: step 206020, loss = 0.20 (8069.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:22.386588: step 206030, loss = 0.31 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:22.709404: step 206040, loss = 0.25 (7836.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:23.030889: step 206050, loss = 0.20 (7860.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:23.350654: step 206060, loss = 0.19 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:23.670762: step 206070, loss = 0.22 (8025.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:23.989567: step 206080, loss = 0.22 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:24.308028: step 206090, loss = 0.20 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:24.628528: step 206100, loss = 0.20 (8088.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:25.092931: step 206110, loss = 0.19 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:25.415013: step 206120, loss = 0.20 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:25.740992: step 206130, loss = 0.20 (7486.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:26.062654: step 206140, loss = 0.16 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:26.382404: step 206150, loss = 0.20 (8145.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:26.701382: step 206160, loss = 0.21 (8160.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:27.019318: step 206170, loss = 0.13 (8123.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:27.341145: step 206180, loss = 0.16 (8098.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:27.663548: step 206190, loss = 0.22 (7980.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:27.987411: step 206200, loss = 0.19 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:28.445399: step 206210, loss = 0.16 (8006.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:28.763871: step 206220, loss = 0.16 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:29.083863: step 206230, loss = 0.17 (7918.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:29.403777: step 206240, loss = 0.17 (8007.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:29.723413: step 206250, loss = 0.19 (7873.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:30.041559: step 206260, loss = 0.26 (8077.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:30.365209: step 206270, loss = 0.21 (7487.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:30.687353: step 206280, loss = 0.14 (7879.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:31.006875: step 206290, loss = 0.24 (8081.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:31.327151: step 206300, loss = 0.17 (7935.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:31.783000: step 206310, loss = 0.21 (7857.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:32.104518: step 206320, loss = 0.21 (8102.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:32.426251: step 206330, loss = 0.24 (8019.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:32.747081: step 206340, loss = 0.21 (7962.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:33.066036: step 206350, loss = 0.27 (8147.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:33.390390: step 206360, loss = 0.15 (8075.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:33.709086: step 206370, loss = 0.20 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:34.035165: step 206380, loss = 0.18 (8076.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:34.355852: step 206390, loss = 0.26 (8031.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:34.676065: step 206400, loss = 0.22 (8070.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:35.139261: step 206410, loss = 0.23 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:35.459422: step 206420, loss = 0.21 (8066.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:35.779726: step 206430, loss = 0.22 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:36.101952: step 206440, loss = 0.27 (7849.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:36.424090: step 206450, loss = 0.21 (8087.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:36.744221: step 206460, loss = 0.22 (8049.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:37.066175: step 206470, loss = 0.23 (8068.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:37.391013: step 206480, loss = 0.18 (7922.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:37.711431: step 206490, loss = 0.22 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:38.033010: step 206500, loss = 0.22 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:38.495311: step 206510, loss = 0.17 (7982.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:38.815786: step 206520, loss = 0.25 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:39.134921: step 206530, loss = 0.22 (8109.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:39.454147: step 206540, loss = 0.20 (7906.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:39.774022: step 206550, loss = 0.21 (8024.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:40.094875: step 206560, loss = 0.24 (8072.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:40.413663: step 206570, loss = 0.17 (8031.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:40.738152: step 206580, loss = 0.22 (7924.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:41.059089: step 206590, loss = 0.27 (8010.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:41.377914: step 206600, loss = 0.24 (8107.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:41.842245: step 206610, loss = 0.21 (8040.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:42.161897: step 206620, loss = 0.25 (8130.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:42.482579: step 206630, loss = 0.27 (7995.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:42.802791: step 206640, loss = 0.22 (7989.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:43.123148: step 206650, loss = 0.17 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:43.447739: step 206660, loss = 0.20 (8069.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:43.769137: step 206670, loss = 0.28 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:44.088389: step 206680, loss = 0.19 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:44.408836: step 206690, loss = 0.24 (8139.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:44.728583: step 206700, loss = 0.21 (8074.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:45.195088: step 206710, loss = 0.25 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:45.516030: step 206720, loss = 0.21 (7944.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:45.835590: step 206730, loss = 0.17 (8014.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:46.157581: step 206740, loss = 0.18 (8030.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:46.478013: step 206750, loss = 0.18 (8065.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:46.802118: step 206760, loss = 0.20 (7883.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:47.125176: step 206770, loss = 0.17 (7950.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:47.445550: step 206780, loss = 0.23 (7917.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:47.768137: step 206790, loss = 0.23 (7894.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:48.088304: step 206800, loss = 0.23 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:48.550849: step 206810, loss = 0.20 (7955.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:48.876708: step 206820, loss = 0.16 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:49.198745: step 206830, loss = 0.17 (8002.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:49.517053: step 206840, loss = 0.19 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:49.840090: step 206850, loss = 0.17 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:50.160979: step 206860, loss = 0.20 (8084.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:50.480547: step 206870, loss = 0.23 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:50.800709: step 206880, loss = 0.20 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:51.123295: step 206890, loss = 0.16 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:51.445270: step 206900, loss = 0.19 (8016.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:51.907244: step 206910, loss = 0.20 (7726.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:52.227876: step 206920, loss = 0.21 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:52.548979: step 206930, loss = 0.19 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:52.868638: step 206940, loss = 0.25 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:53.191163: step 206950, loss = 0.17 (7831.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:53.511178: step 206960, loss = 0.28 (8084.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:53.833616: step 206970, loss = 0.22 (8025.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:54.154937: step 206980, loss = 0.28 (7788.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:54.474246: step 206990, loss = 0.20 (8087.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:54.796099: step 207000, loss = 0.22 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:55.342408: step 207010, loss = 0.23 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:55.664571: step 207020, loss = 0.19 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:55.982529: step 207030, loss = 0.21 (8001.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:56.302748: step 207040, loss = 0.21 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:56.624212: step 207050, loss = 0.22 (7893.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:56.947869: step 207060, loss = 0.30 (7841.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:57.272994: step 207070, loss = 0.20 (8124.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:57.593239: step 207080, loss = 0.19 (8121.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:57.915962: step 207090, loss = 0.18 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:58.235291: step 207100, loss = 0.21 (8109.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:58.702534: step 207110, loss = 0.25 (7988.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:59.025494: step 207120, loss = 0.22 (7719.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:35:59.348176: step 207130, loss = 0.15 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:59.668745: step 207140, loss = 0.15 (7824.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:35:59.989995: step 207150, loss = 0.32 (8107.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:00.316350: step 207160, loss = 0.22 (8000.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:00.640125: step 207170, loss = 0.16 (7796.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:00.967938: step 207180, loss = 0.21 (7832.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:01.288518: step 207190, loss = 0.21 (7949.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:01.609556: step 207200, loss = 0.29 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:02.074836: step 207210, loss = 0.20 (8050.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:02.393526: step 207220, loss = 0.26 (7986.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:02.712763: step 207230, loss = 0.19 (7960.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:03.033223: step 207240, loss = 0.22 (7892.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:03.351759: step 207250, loss = 0.17 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:03.670920: step 207260, loss = 0.24 (8008.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:03.991951: step 207270, loss = 0.19 (7936.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:04.314303: step 207280, loss = 0.16 (7991.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:04.636028: step 207290, loss = 0.22 (8081.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:04.956443: step 207300, loss = 0.27 (7895.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:05.423133: step 207310, loss = 0.18 (7956.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:05.742748: step 207320, loss = 0.19 (8056.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:06.064356: step 207330, loss = 0.22 (8125.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:06.384478: step 207340, loss = 0.21 (8091.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:06.704508: step 207350, loss = 0.23 (8061.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:07.025212: step 207360, loss = 0.23 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:07.346738: step 207370, loss = 0.18 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:07.666342: step 207380, loss = 0.18 (8052.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:07.985290: step 207390, loss = 0.22 (8102.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:08.304114: step 207400, loss = 0.25 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:08.769318: step 207410, loss = 0.19 (7996.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:09.090260: step 207420, loss = 0.20 (7832.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:09.411126: step 207430, loss = 0.27 (8051.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:09.732198: step 207440, loss = 0.20 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:10.053281: step 207450, loss = 0.16 (7983.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:10.374472: step 207460, loss = 0.16 (8012.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:10.692707: step 207470, loss = 0.23 (8078.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:11.011181: step 207480, loss = 0.15 (7963.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:11.331454: step 207490, loss = 0.28 (8066.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:11.650158: step 207500, loss = 0.21 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:12.113780: step 207510, loss = 0.34 (8069.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:12.432465: step 207520, loss = 0.20 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:12.752237: step 207530, loss = 0.24 (8095.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:13.072243: step 207540, loss = 0.18 (7925.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:13.394085: step 207550, loss = 0.19 (7898.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:13.713441: step 207560, loss = 0.18 (8120.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:14.034098: step 207570, loss = 0.16 (8036.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:14.354026: step 207580, loss = 0.28 (8063.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:14.677892: step 207590, loss = 0.23 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:14.999706: step 207600, loss = 0.24 (7971.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:15.467231: step 207610, loss = 0.20 (8103.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:15.786544: step 207620, loss = 0.17 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:16.107313: step 207630, loss = 0.18 (8013.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:16.426723: step 207640, loss = 0.19 (8092.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:16.747333: step 207650, loss = 0.17 (7866.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:17.067045: step 207660, loss = 0.19 (8046.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:17.386823: step 207670, loss = 0.23 (7932.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:17.704792: step 207680, loss = 0.21 (8212.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:18.025905: step 207690, loss = 0.21 (8093.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:18.345318: step 207700, loss = 0.22 (8137.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:18.814561: step 207710, loss = 0.21 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:19.137148: step 207720, loss = 0.29 (8024.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:19.457391: step 207730, loss = 0.19 (8086.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:19.778616: step 207740, loss = 0.17 (7975.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:20.103591: step 207750, loss = 0.18 (7819.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:20.424126: step 207760, loss = 0.21 (7641.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:36:20.747495: step 207770, loss = 0.20 (8057.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:21.067793: step 207780, loss = 0.22 (7867.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:21.386663: step 207790, loss = 0.24 (7882.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:21.710141: step 207800, loss = 0.19 (7101.6 examples/sec; 0.018 sec/batch)
2017-09-16 17:36:22.177076: step 207810, loss = 0.28 (8119.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:22.495658: step 207820, loss = 0.18 (7943.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:22.817901: step 207830, loss = 0.19 (7779.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:23.136573: step 207840, loss = 0.21 (8074.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:23.454594: step 207850, loss = 0.14 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:23.773584: step 207860, loss = 0.21 (7887.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:24.093802: step 207870, loss = 0.19 (8056.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:24.412524: step 207880, loss = 0.19 (8118.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:24.731985: step 207890, loss = 0.13 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:25.051781: step 207900, loss = 0.19 (8040.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:25.513363: step 207910, loss = 0.15 (7908.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:25.832179: step 207920, loss = 0.18 (8101.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:26.153137: step 207930, loss = 0.24 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:26.474560: step 207940, loss = 0.20 (7838.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:26.793100: step 207950, loss = 0.21 (8121.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:27.115515: step 207960, loss = 0.26 (8098.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:27.434895: step 207970, loss = 0.27 (8109.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:27.755589: step 207980, loss = 0.16 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:28.075995: step 207990, loss = 0.21 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:28.395681: step 208000, loss = 0.22 (7864.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:29.006651: step 208010, loss = 0.24 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:29.325360: step 208020, loss = 0.26 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:29.647683: step 208030, loss = 0.21 (7969.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:29.967877: step 208040, loss = 0.28 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:30.288051: step 208050, loss = 0.20 (8137.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:30.607962: step 208060, loss = 0.23 (8165.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:30.928500: step 208070, loss = 0.20 (8122.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:31.247796: step 208080, loss = 0.18 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:31.566286: step 208090, loss = 0.19 (8020.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:31.885901: step 208100, loss = 0.17 (7851.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:32.337882: step 208110, loss = 0.20 (8082.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:32.657828: step 208120, loss = 0.25 (7998.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:32.976586: step 208130, loss = 0.22 (8062.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:33.297665: step 208140, loss = 0.20 (7981.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:33.614454: step 208150, loss = 0.21 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:33.931904: step 208160, loss = 0.21 (8059.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:34.252245: step 208170, loss = 0.19 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:34.570446: step 208180, loss = 0.18 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:34.892476: step 208190, loss = 0.22 (8018.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:35.215996: step 208200, loss = 0.25 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:35.681854: step 208210, loss = 0.26 (7997.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:36.002114: step 208220, loss = 0.23 (7870.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:36.321813: step 208230, loss = 0.23 (8116.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:36.642243: step 208240, loss = 0.25 (7821.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:36.964578: step 208250, loss = 0.27 (8032.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:37.283713: step 208260, loss = 0.24 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:37.604091: step 208270, loss = 0.23 (8039.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:37.923836: step 208280, loss = 0.19 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:38.244444: step 208290, loss = 0.18 (7878.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:38.563420: step 208300, loss = 0.17 (8139.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:39.027204: step 208310, loss = 0.20 (7875.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:39.346712: step 208320, loss = 0.15 (7979.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:39.668774: step 208330, loss = 0.21 (8083.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:39.993232: step 208340, loss = 0.23 (8039.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:40.314778: step 208350, loss = 0.16 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:40.637149: step 208360, loss = 0.19 (7906.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:40.961483: step 208370, loss = 0.31 (7633.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:36:41.281824: step 208380, loss = 0.21 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:41.602732: step 208390, loss = 0.23 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:41.920903: step 208400, loss = 0.18 (8089.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:42.385404: step 208410, loss = 0.23 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:42.704800: step 208420, loss = 0.20 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:43.023906: step 208430, loss = 0.15 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:43.344765: step 208440, loss = 0.23 (7917.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:43.665014: step 208450, loss = 0.15 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:43.987479: step 208460, loss = 0.24 (7826.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:44.306621: step 208470, loss = 0.25 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:44.626382: step 208480, loss = 0.20 (8064.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:44.945917: step 208490, loss = 0.23 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:45.267748: step 208500, loss = 0.17 (8038.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:45.733112: step 208510, loss = 0.15 (8051.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:46.054274: step 208520, loss = 0.25 (8005.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:46.374271: step 208530, loss = 0.18 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:46.693703: step 208540, loss = 0.24 (8028.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:47.013801: step 208550, loss = 0.15 (7936.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:47.333688: step 208560, loss = 0.21 (8009.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:47.655387: step 208570, loss = 0.16 (7901.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:47.975239: step 208580, loss = 0.25 (7964.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:48.298223: step 208590, loss = 0.21 (7808.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:48.617358: step 208600, loss = 0.28 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:49.079268: step 208610, loss = 0.21 (8121.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:49.397535: step 208620, loss = 0.23 (7995.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:49.714769: step 208630, loss = 0.20 (8045.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:50.033748: step 208640, loss = 0.21 (8015.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:50.354300: step 208650, loss = 0.19 (8096.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:50.673070: step 208660, loss = 0.18 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:50.993195: step 208670, loss = 0.18 (7956.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:51.314099: step 208680, loss = 0.23 (7953.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:51.632929: step 208690, loss = 0.18 (8083.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:51.958375: step 208700, loss = 0.26 (6885.4 examples/sec; 0.019 sec/batch)
2017-09-16 17:36:52.422483: step 208710, loss = 0.23 (8028.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:52.744352: step 208720, loss = 0.17 (7871.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:53.064558: step 208730, loss = 0.22 (7969.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:53.385592: step 208740, loss = 0.24 (7864.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:53.705419: step 208750, loss = 0.25 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:54.027281: step 208760, loss = 0.32 (7808.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:54.346822: step 208770, loss = 0.21 (8034.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:54.665708: step 208780, loss = 0.19 (7888.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:54.987249: step 208790, loss = 0.24 (7913.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:55.305353: step 208800, loss = 0.25 (8156.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:55.760360: step 208810, loss = 0.18 (8024.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:56.081066: step 208820, loss = 0.19 (7866.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:56.401034: step 208830, loss = 0.23 (7963.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:56.719642: step 208840, loss = 0.21 (8032.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:57.041602: step 208850, loss = 0.17 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:57.360192: step 208860, loss = 0.25 (7917.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:57.681292: step 208870, loss = 0.15 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:58.001145: step 208880, loss = 0.20 (8019.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:58.323134: step 208890, loss = 0.14 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:58.644320: step 208900, loss = 0.17 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:59.117793: step 208910, loss = 0.25 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:59.436795: step 208920, loss = 0.20 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:36:59.754461: step 208930, loss = 0.22 (8111.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:00.074155: step 208940, loss = 0.25 (8098.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:00.394720: step 208950, loss = 0.19 (7873.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:00.716320: step 208960, loss = 0.18 (7374.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:01.042869: step 208970, loss = 0.21 (8026.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:01.362908: step 208980, loss = 0.24 (7990.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:01.681337: step 208990, loss = 0.24 (8025.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:02.004120: step 209000, loss = 0.19 (7744.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:02.576919: step 209010, loss = 0.21 (8090.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:02.896513: step 209020, loss = 0.17 (8064.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:03.217452: step 209030, loss = 0.20 (7910.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:03.537790: step 209040, loss = 0.26 (8160.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:03.858912: step 209050, loss = 0.16 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:04.179379: step 209060, loss = 0.24 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:04.498250: step 209070, loss = 0.22 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:04.817708: step 209080, loss = 0.19 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:05.138442: step 209090, loss = 0.23 (7984.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:05.459096: step 209100, loss = 0.21 (8050.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:05.928312: step 209110, loss = 0.20 (8048.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:06.248575: step 209120, loss = 0.17 (7988.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:06.572124: step 209130, loss = 0.25 (7972.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:06.892954: step 209140, loss = 0.16 (7874.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:07.210697: step 209150, loss = 0.25 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:07.529869: step 209160, loss = 0.16 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:07.848910: step 209170, loss = 0.24 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:08.170555: step 209180, loss = 0.21 (7939.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:08.489842: step 209190, loss = 0.26 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:08.808322: step 209200, loss = 0.19 (8175.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:09.271745: step 209210, loss = 0.18 (8097.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:09.591621: step 209220, loss = 0.23 (8079.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:09.911072: step 209230, loss = 0.18 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:10.230535: step 209240, loss = 0.22 (8127.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:10.551538: step 209250, loss = 0.19 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:10.871521: step 209260, loss = 0.19 (8018.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:11.189592: step 209270, loss = 0.23 (8122.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:11.511051: step 209280, loss = 0.19 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:11.832439: step 209290, loss = 0.22 (7934.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:12.154161: step 209300, loss = 0.17 (8034.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:12.617411: step 209310, loss = 0.25 (8016.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:12.937192: step 209320, loss = 0.15 (7997.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:13.258146: step 209330, loss = 0.16 (8049.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:13.578164: step 209340, loss = 0.23 (8091.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:13.899608: step 209350, loss = 0.26 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:14.220661: step 209360, loss = 0.26 (8014.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:14.541820: step 209370, loss = 0.26 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:14.861526: step 209380, loss = 0.18 (8046.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:15.182514: step 209390, loss = 0.21 (8039.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:15.503979: step 209400, loss = 0.23 (8102.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:15.967262: step 209410, loss = 0.24 (8036.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:16.286827: step 209420, loss = 0.20 (8074.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:16.610233: step 209430, loss = 0.19 (8058.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:16.931821: step 209440, loss = 0.18 (7905.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:17.253277: step 209450, loss = 0.18 (8033.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:17.574792: step 209460, loss = 0.19 (7888.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:17.895747: step 209470, loss = 0.19 (8022.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:18.215683: step 209480, loss = 0.20 (8016.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:18.534971: step 209490, loss = 0.24 (8080.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:18.853733: step 209500, loss = 0.18 (8023.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:19.317447: step 209510, loss = 0.21 (8095.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:19.638296: step 209520, loss = 0.22 (7994.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:19.958924: step 209530, loss = 0.19 (7789.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:20.276209: step 209540, loss = 0.24 (8127.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:20.596100: step 209550, loss = 0.17 (8204.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:20.917518: step 209560, loss = 0.22 (8029.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:21.236801: step 209570, loss = 0.25 (8034.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:21.557210: step 209580, loss = 0.18 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:21.879433: step 209590, loss = 0.25 (8136.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:22.198975: step 209600, loss = 0.17 (8051.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:22.651889: step 209610, loss = 0.23 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:22.974177: step 209620, loss = 0.18 (8106.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:23.293313: step 209630, loss = 0.26 (8115.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:23.612415: step 209640, loss = 0.25 (8002.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:23.930262: step 209650, loss = 0.18 (8042.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:24.249166: step 209660, loss = 0.22 (8119.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:24.567895: step 209670, loss = 0.29 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:24.887114: step 209680, loss = 0.26 (7854.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:25.208106: step 209690, loss = 0.22 (8016.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:25.528123: step 209700, loss = 0.24 (8067.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:25.991563: step 209710, loss = 0.22 (8054.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:26.313522: step 209720, loss = 0.21 (7862.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:26.631469: step 209730, loss = 0.21 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:26.952284: step 209740, loss = 0.14 (8155.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:27.276330: step 209750, loss = 0.26 (8018.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:27.596986: step 209760, loss = 0.20 (8011.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:27.916517: step 209770, loss = 0.22 (7982.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:28.236831: step 209780, loss = 0.19 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:28.557784: step 209790, loss = 0.16 (8093.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:28.879499: step 209800, loss = 0.18 (7989.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:29.335142: step 209810, loss = 0.29 (7854.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:29.656485: step 209820, loss = 0.15 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:29.974940: step 209830, loss = 0.16 (8075.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:30.292451: step 209840, loss = 0.18 (8060.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:30.612830: step 209850, loss = 0.23 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:30.932903: step 209860, loss = 0.24 (8071.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:31.257992: step 209870, loss = 0.22 (8130.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:31.580285: step 209880, loss = 0.21 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:31.899839: step 209890, loss = 0.17 (8124.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:32.217998: step 209900, loss = 0.19 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:32.683495: step 209910, loss = 0.21 (8010.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:33.008386: step 209920, loss = 0.24 (7687.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:33.327719: step 209930, loss = 0.24 (8048.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:33.648358: step 209940, loss = 0.20 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:33.970109: step 209950, loss = 0.22 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:34.291058: step 209960, loss = 0.21 (8136.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:34.612159: step 209970, loss = 0.19 (8070.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:34.934152: step 209980, loss = 0.19 (7661.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:35.253411: step 209990, loss = 0.26 (8055.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:35.575817: step 210000, loss = 0.17 (7852.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:36.123325: step 210010, loss = 0.21 (8042.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:36.443757: step 210020, loss = 0.22 (7944.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:36.763321: step 210030, loss = 0.20 (8058.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:37.083079: step 210040, loss = 0.22 (8014.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:37.401362: step 210050, loss = 0.24 (8070.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:37.719734: step 210060, loss = 0.22 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:38.045314: step 210070, loss = 0.17 (7212.5 examples/sec; 0.018 sec/batch)
2017-09-16 17:37:38.365387: step 210080, loss = 0.20 (7908.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:38.685547: step 210090, loss = 0.16 (7658.3 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:39.032091: step 210100, loss = 0.16 (7333.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:39.557019: step 210110, loss = 0.28 (7657.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:39.915169: step 210120, loss = 0.30 (7132.9 examples/sec; 0.018 sec/batch)
2017-09-16 17:37:40.271778: step 210130, loss = 0.24 (6958.8 examples/sec; 0.018 sec/batch)
2017-09-16 17:37:40.618346: step 210140, loss = 0.26 (6979.7 examples/sec; 0.018 sec/batch)
2017-09-16 17:37:41.004135: step 210150, loss = 0.16 (7095.7 examples/sec; 0.018 sec/batch)
2017-09-16 17:37:41.422550: step 210160, loss = 0.19 (5892.5 examples/sec; 0.022 sec/batch)
2017-09-16 17:37:41.868388: step 210170, loss = 0.21 (5631.7 examples/sec; 0.023 sec/batch)
2017-09-16 17:37:42.313223: step 210180, loss = 0.16 (5743.5 examples/sec; 0.022 sec/batch)
2017-09-16 17:37:42.762826: step 210190, loss = 0.22 (5247.7 examples/sec; 0.024 sec/batch)
2017-09-16 17:37:43.204353: step 210200, loss = 0.20 (5249.9 examples/sec; 0.024 sec/batch)
2017-09-16 17:37:43.797122: step 210210, loss = 0.19 (5320.8 examples/sec; 0.024 sec/batch)
2017-09-16 17:37:44.237865: step 210220, loss = 0.27 (5527.1 examples/sec; 0.023 sec/batch)
2017-09-16 17:37:44.675874: step 210230, loss = 0.21 (6680.4 examples/sec; 0.019 sec/batch)
2017-09-16 17:37:45.120392: step 210240, loss = 0.32 (5953.4 examples/sec; 0.022 sec/batch)
2017-09-16 17:37:45.557575: step 210250, loss = 0.26 (5744.5 examples/sec; 0.022 sec/batch)
2017-09-16 17:37:45.995419: step 210260, loss = 0.23 (6423.1 examples/sec; 0.020 sec/batch)
2017-09-16 17:37:46.438533: step 210270, loss = 0.21 (6089.3 examples/sec; 0.021 sec/batch)
2017-09-16 17:37:46.884663: step 210280, loss = 0.24 (6323.5 examples/sec; 0.020 sec/batch)
2017-09-16 17:37:47.331615: step 210290, loss = 0.19 (5587.6 examples/sec; 0.023 sec/batch)
2017-09-16 17:37:47.774269: step 210300, loss = 0.24 (5689.3 examples/sec; 0.022 sec/batch)
2017-09-16 17:37:48.288122: step 210310, loss = 0.22 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:48.612737: step 210320, loss = 0.17 (7927.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:48.931892: step 210330, loss = 0.17 (8082.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:49.253545: step 210340, loss = 0.26 (8063.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:49.574792: step 210350, loss = 0.21 (8023.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:49.895476: step 210360, loss = 0.19 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:50.216967: step 210370, loss = 0.26 (7651.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:50.538973: step 210380, loss = 0.16 (8058.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:50.863039: step 210390, loss = 0.24 (7972.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:51.182946: step 210400, loss = 0.21 (8100.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:51.651872: step 210410, loss = 0.23 (8041.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:51.972692: step 210420, loss = 0.24 (8094.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:52.295521: step 210430, loss = 0.20 (8027.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:52.618231: step 210440, loss = 0.18 (8009.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:52.938290: step 210450, loss = 0.19 (8093.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:53.259072: step 210460, loss = 0.21 (7977.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:53.578804: step 210470, loss = 0.13 (8005.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:53.901217: step 210480, loss = 0.15 (7998.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:54.219803: step 210490, loss = 0.22 (8066.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:54.540042: step 210500, loss = 0.19 (8088.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:54.991467: step 210510, loss = 0.18 (7577.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:55.311730: step 210520, loss = 0.19 (8012.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:55.636170: step 210530, loss = 0.24 (7569.5 examples/sec; 0.017 sec/batch)
2017-09-16 17:37:55.957130: step 210540, loss = 0.22 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:56.277907: step 210550, loss = 0.30 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:56.596786: step 210560, loss = 0.26 (7996.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:56.917155: step 210570, loss = 0.14 (7851.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:57.237322: step 210580, loss = 0.27 (7965.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:57.558574: step 210590, loss = 0.24 (8077.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:57.878071: step 210600, loss = 0.22 (7981.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:58.342245: step 210610, loss = 0.20 (8085.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:58.663382: step 210620, loss = 0.18 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:58.985233: step 210630, loss = 0.20 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:59.305111: step 210640, loss = 0.21 (7955.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:59.625127: step 210650, loss = 0.18 (8023.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:37:59.945760: step 210660, loss = 0.21 (7842.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:00.266791: step 210670, loss = 0.30 (7904.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:00.586922: step 210680, loss = 0.26 (8051.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:00.907889: step 210690, loss = 0.24 (7671.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:01.228129: step 210700, loss = 0.22 (8060.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:01.691892: step 210710, loss = 0.16 (7938.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:02.012005: step 210720, loss = 0.23 (7803.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:02.332028: step 210730, loss = 0.21 (8044.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:02.650931: step 210740, loss = 0.16 (8060.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:02.972966: step 210750, loss = 0.17 (8124.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:03.292301: step 210760, loss = 0.15 (8050.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:03.617904: step 210770, loss = 0.18 (7449.0 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:03.941073: step 210780, loss = 0.17 (8116.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:04.262535: step 210790, loss = 0.16 (8108.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:04.582263: step 210800, loss = 0.19 (8113.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:05.043404: step 210810, loss = 0.19 (8045.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:05.363685: step 210820, loss = 0.25 (7951.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:05.689196: step 210830, loss = 0.22 (7619.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:06.011841: step 210840, loss = 0.35 (7698.1 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:06.331180: step 210850, loss = 0.21 (7907.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:06.651871: step 210860, loss = 0.27 (7971.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:06.970643: step 210870, loss = 0.19 (8020.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:07.293343: step 210880, loss = 0.22 (7853.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:07.613324: step 210890, loss = 0.18 (7905.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:07.932683: step 210900, loss = 0.28 (8104.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:08.396659: step 210910, loss = 0.17 (8017.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:08.718244: step 210920, loss = 0.17 (7996.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:09.038824: step 210930, loss = 0.20 (8012.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:09.361799: step 210940, loss = 0.29 (7849.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:09.684267: step 210950, loss = 0.17 (7961.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:10.008247: step 210960, loss = 0.22 (7986.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:10.328786: step 210970, loss = 0.29 (7892.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:10.649520: step 210980, loss = 0.22 (8084.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:10.968339: step 210990, loss = 0.22 (8095.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:11.290242: step 211000, loss = 0.20 (7919.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:11.855572: step 211010, loss = 0.22 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:12.175996: step 211020, loss = 0.28 (8004.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:12.496884: step 211030, loss = 0.22 (8033.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:12.817125: step 211040, loss = 0.17 (8050.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:13.137880: step 211050, loss = 0.24 (7827.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:13.458934: step 211060, loss = 0.26 (7869.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:13.781192: step 211070, loss = 0.20 (8035.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:14.100493: step 211080, loss = 0.20 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:14.420846: step 211090, loss = 0.23 (8084.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:14.741454: step 211100, loss = 0.16 (8049.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:15.201060: step 211110, loss = 0.16 (7938.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:15.526265: step 211120, loss = 0.25 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:15.849684: step 211130, loss = 0.26 (7967.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:16.175538: step 211140, loss = 0.18 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:16.497016: step 211150, loss = 0.21 (8029.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:16.819877: step 211160, loss = 0.20 (8018.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:17.139472: step 211170, loss = 0.24 (7930.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:17.459770: step 211180, loss = 0.21 (8133.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:17.779368: step 211190, loss = 0.15 (7823.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:18.102001: step 211200, loss = 0.19 (7984.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:18.567141: step 211210, loss = 0.19 (8078.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:18.888050: step 211220, loss = 0.17 (8001.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:19.214442: step 211230, loss = 0.19 (8053.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:19.536179: step 211240, loss = 0.19 (8043.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:19.857093: step 211250, loss = 0.26 (7854.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:20.176784: step 211260, loss = 0.20 (7770.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:20.495165: step 211270, loss = 0.16 (8057.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:20.816364: step 211280, loss = 0.17 (7866.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:21.136793: step 211290, loss = 0.32 (7917.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:21.456690: step 211300, loss = 0.22 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:21.920898: step 211310, loss = 0.19 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:22.240505: step 211320, loss = 0.24 (8062.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:22.560982: step 211330, loss = 0.23 (7863.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:22.879719: step 211340, loss = 0.16 (8028.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:23.198558: step 211350, loss = 0.14 (8029.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:23.518654: step 211360, loss = 0.16 (8096.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:23.838575: step 211370, loss = 0.24 (7911.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:24.159777: step 211380, loss = 0.22 (7858.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:24.480174: step 211390, loss = 0.31 (7844.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:24.799551: step 211400, loss = 0.23 (8123.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:25.260213: step 211410, loss = 0.23 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:25.579166: step 211420, loss = 0.23 (7862.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:25.900176: step 211430, loss = 0.20 (7882.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:26.220885: step 211440, loss = 0.22 (7818.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:26.540422: step 211450, loss = 0.22 (7941.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:26.861212: step 211460, loss = 0.23 (8021.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:27.180824: step 211470, loss = 0.19 (8073.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:27.502967: step 211480, loss = 0.23 (7908.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:27.821224: step 211490, loss = 0.18 (8000.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:28.141811: step 211500, loss = 0.19 (7954.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:28.607798: step 211510, loss = 0.17 (7840.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:28.928296: step 211520, loss = 0.16 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:29.246262: step 211530, loss = 0.14 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:29.567039: step 211540, loss = 0.16 (8063.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:29.888336: step 211550, loss = 0.22 (7896.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:30.213019: step 211560, loss = 0.17 (7473.8 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:30.536686: step 211570, loss = 0.16 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:30.859443: step 211580, loss = 0.15 (8111.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:31.182773: step 211590, loss = 0.23 (7574.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:31.503606: step 211600, loss = 0.19 (7985.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:31.968630: step 211610, loss = 0.23 (8080.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:32.289092: step 211620, loss = 0.18 (7964.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:32.609403: step 211630, loss = 0.29 (8099.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:32.929828: step 211640, loss = 0.21 (8072.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:33.250064: step 211650, loss = 0.21 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:33.569847: step 211660, loss = 0.27 (8029.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:33.889366: step 211670, loss = 0.20 (7933.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:34.209869: step 211680, loss = 0.24 (8013.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:34.530740: step 211690, loss = 0.22 (7998.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:34.852680: step 211700, loss = 0.18 (7760.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:35.317069: step 211710, loss = 0.16 (8121.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:35.636548: step 211720, loss = 0.25 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:35.956124: step 211730, loss = 0.17 (7886.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:36.276144: step 211740, loss = 0.20 (8107.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:36.596383: step 211750, loss = 0.20 (8164.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:36.916850: step 211760, loss = 0.19 (7813.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:37.236107: step 211770, loss = 0.23 (8028.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:37.555288: step 211780, loss = 0.25 (8103.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:37.876067: step 211790, loss = 0.17 (8004.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:38.196746: step 211800, loss = 0.20 (7937.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:38.660306: step 211810, loss = 0.20 (8156.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:38.982418: step 211820, loss = 0.18 (7846.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:39.302483: step 211830, loss = 0.22 (8097.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:39.623027: step 211840, loss = 0.24 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:39.942308: step 211850, loss = 0.23 (8042.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:40.262039: step 211860, loss = 0.34 (8011.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:40.581023: step 211870, loss = 0.20 (8197.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:40.901390: step 211880, loss = 0.22 (8039.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:41.220598: step 211890, loss = 0.18 (8013.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:41.540724: step 211900, loss = 0.25 (8037.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:42.004970: step 211910, loss = 0.28 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:42.326464: step 211920, loss = 0.20 (8007.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:42.648372: step 211930, loss = 0.21 (8026.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:42.968798: step 211940, loss = 0.22 (7962.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:43.291106: step 211950, loss = 0.14 (7826.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:43.613453: step 211960, loss = 0.15 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:43.934719: step 211970, loss = 0.21 (8120.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:44.254650: step 211980, loss = 0.24 (8061.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:44.578746: step 211990, loss = 0.24 (8030.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:44.899697: step 212000, loss = 0.20 (8106.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:45.464027: step 212010, loss = 0.19 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:45.783847: step 212020, loss = 0.21 (7918.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:46.104893: step 212030, loss = 0.22 (7976.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:46.425628: step 212040, loss = 0.32 (8100.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:46.746042: step 212050, loss = 0.22 (7881.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:47.063706: step 212060, loss = 0.16 (7996.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:47.381581: step 212070, loss = 0.26 (8080.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:47.701298: step 212080, loss = 0.16 (7951.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:48.023425: step 212090, loss = 0.20 (8071.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:48.341673: step 212100, loss = 0.17 (8046.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:48.806130: step 212110, loss = 0.19 (8074.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:49.125837: step 212120, loss = 0.21 (8047.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:49.444325: step 212130, loss = 0.17 (8086.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:49.763509: step 212140, loss = 0.21 (8107.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:50.085552: step 212150, loss = 0.19 (8026.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:50.407545: step 212160, loss = 0.21 (7963.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:50.730523: step 212170, loss = 0.22 (7824.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:51.051243: step 212180, loss = 0.17 (7973.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:51.378613: step 212190, loss = 0.19 (8007.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:51.700728: step 212200, loss = 0.18 (8071.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:52.169426: step 212210, loss = 0.27 (7001.4 examples/sec; 0.018 sec/batch)
2017-09-16 17:38:52.489107: step 212220, loss = 0.25 (7988.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:52.809955: step 212230, loss = 0.20 (7999.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:53.132394: step 212240, loss = 0.19 (8056.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:53.454142: step 212250, loss = 0.24 (7985.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:53.775037: step 212260, loss = 0.15 (7984.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:54.095543: step 212270, loss = 0.22 (8099.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:54.417824: step 212280, loss = 0.21 (8092.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:54.738685: step 212290, loss = 0.23 (7594.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:38:55.062291: step 212300, loss = 0.29 (7957.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:55.523836: step 212310, loss = 0.23 (8052.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:55.845803: step 212320, loss = 0.18 (8004.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:56.169099: step 212330, loss = 0.19 (7879.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:56.490843: step 212340, loss = 0.26 (8038.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:56.814019: step 212350, loss = 0.22 (8108.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:57.138891: step 212360, loss = 0.22 (7179.3 examples/sec; 0.018 sec/batch)
2017-09-16 17:38:57.459254: step 212370, loss = 0.18 (8050.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:57.780461: step 212380, loss = 0.19 (8013.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:58.102659: step 212390, loss = 0.18 (7820.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:58.423714: step 212400, loss = 0.16 (8068.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:58.877852: step 212410, loss = 0.18 (7895.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:59.201888: step 212420, loss = 0.18 (8110.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:59.523735: step 212430, loss = 0.14 (8008.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:38:59.844702: step 212440, loss = 0.26 (8000.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:00.164673: step 212450, loss = 0.21 (8055.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:00.486805: step 212460, loss = 0.28 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:00.806036: step 212470, loss = 0.28 (8068.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:01.126432: step 212480, loss = 0.23 (8009.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:01.446067: step 212490, loss = 0.25 (8116.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:01.769011: step 212500, loss = 0.14 (8023.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:02.232711: step 212510, loss = 0.25 (8104.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:02.553663: step 212520, loss = 0.14 (7685.6 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:02.873500: step 212530, loss = 0.19 (7857.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:03.192901: step 212540, loss = 0.20 (8063.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:03.514547: step 212550, loss = 0.22 (8059.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:03.834487: step 212560, loss = 0.25 (7975.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:04.153766: step 212570, loss = 0.28 (7874.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:04.473564: step 212580, loss = 0.22 (7975.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:04.793402: step 212590, loss = 0.24 (7949.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:05.115338: step 212600, loss = 0.18 (7986.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:05.588449: step 212610, loss = 0.19 (8080.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:05.910661: step 212620, loss = 0.19 (8033.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:06.233309: step 212630, loss = 0.23 (8002.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:06.553343: step 212640, loss = 0.22 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:06.875046: step 212650, loss = 0.20 (7884.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:07.195590: step 212660, loss = 0.20 (7923.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:07.516732: step 212670, loss = 0.21 (8131.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:07.836941: step 212680, loss = 0.20 (8019.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:08.156764: step 212690, loss = 0.23 (7846.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:08.476780: step 212700, loss = 0.17 (8032.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:08.932884: step 212710, loss = 0.26 (8113.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:09.253286: step 212720, loss = 0.24 (8030.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:09.574625: step 212730, loss = 0.22 (8058.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:09.899384: step 212740, loss = 0.20 (7966.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:10.218295: step 212750, loss = 0.16 (7940.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:10.540665: step 212760, loss = 0.20 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:10.859274: step 212770, loss = 0.20 (8048.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:11.179620: step 212780, loss = 0.18 (8065.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:11.500298: step 212790, loss = 0.22 (7803.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:11.820733: step 212800, loss = 0.20 (8041.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:12.287282: step 212810, loss = 0.20 (8017.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:12.610527: step 212820, loss = 0.24 (7874.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:12.932168: step 212830, loss = 0.19 (8105.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:13.254658: step 212840, loss = 0.25 (7893.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:13.574667: step 212850, loss = 0.17 (8125.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:13.893898: step 212860, loss = 0.20 (7984.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:14.214281: step 212870, loss = 0.21 (7894.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:14.534605: step 212880, loss = 0.19 (8044.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:14.855720: step 212890, loss = 0.20 (8131.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:15.177597: step 212900, loss = 0.19 (7968.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:15.627432: step 212910, loss = 0.22 (8074.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:15.950285: step 212920, loss = 0.17 (7827.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:16.270616: step 212930, loss = 0.16 (7932.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:16.590594: step 212940, loss = 0.24 (7962.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:16.910139: step 212950, loss = 0.26 (8031.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:17.231146: step 212960, loss = 0.23 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:17.552110: step 212970, loss = 0.19 (8135.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:17.873730: step 212980, loss = 0.18 (7818.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:18.196941: step 212990, loss = 0.25 (7889.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:18.517830: step 213000, loss = 0.15 (8054.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:19.074411: step 213010, loss = 0.24 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:19.398022: step 213020, loss = 0.19 (7681.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:19.791291: step 213030, loss = 0.25 (8054.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:20.109986: step 213040, loss = 0.18 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:20.429740: step 213050, loss = 0.16 (8090.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:20.749889: step 213060, loss = 0.21 (8061.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:21.076495: step 213070, loss = 0.25 (8038.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:21.398054: step 213080, loss = 0.19 (8064.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:21.717735: step 213090, loss = 0.19 (8076.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:22.039425: step 213100, loss = 0.14 (7882.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:22.492903: step 213110, loss = 0.19 (8053.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:22.812410: step 213120, loss = 0.22 (8067.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:23.133640: step 213130, loss = 0.22 (7886.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:23.453795: step 213140, loss = 0.26 (8026.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:23.773848: step 213150, loss = 0.22 (7916.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:24.095554: step 213160, loss = 0.14 (8071.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:24.416796: step 213170, loss = 0.27 (8068.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:24.737584: step 213180, loss = 0.23 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:25.056100: step 213190, loss = 0.21 (8112.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:25.375179: step 213200, loss = 0.23 (7991.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:25.837092: step 213210, loss = 0.25 (8033.4 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:26.159213: step 213220, loss = 0.19 (8053.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:26.479402: step 213230, loss = 0.21 (7828.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:26.800975: step 213240, loss = 0.26 (8179.2 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:27.123544: step 213250, loss = 0.21 (8072.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:27.443997: step 213260, loss = 0.27 (8054.9 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:27.765979: step 213270, loss = 0.18 (7952.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:28.087658: step 213280, loss = 0.25 (8167.1 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:28.408311: step 213290, loss = 0.23 (7865.8 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:28.729101: step 213300, loss = 0.19 (7780.5 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:29.211513: step 213310, loss = 0.17 (7945.3 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:29.540252: step 213320, loss = 0.16 (7477.7 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:29.891708: step 213330, loss = 0.25 (7895.6 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:30.225946: step 213340, loss = 0.26 (7664.9 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:30.582875: step 213350, loss = 0.18 (7504.2 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:30.930465: step 213360, loss = 0.19 (7371.4 examples/sec; 0.017 sec/batch)
2017-09-16 17:39:31.287857: step 213370, loss = 0.18 (7106.4 examples/sec; 0.018 sec/batch)
2017-09-16 17:39:31.676751: step 213380, loss = 0.16 (7013.3 examples/sec; 0.018 sec/batch)
2017-09-16 17:39:32.088074: step 213390, loss = 0.14 (5388.8 examples/sec; 0.024 sec/batch)
2017-09-16 17:39:32.530045: step 213400, loss = 0.24 (6026.2 examples/sec; 0.021 sec/batch)
2017-09-16 17:39:33.121648: step 213410, loss = 0.21 (6367.9 examples/sec; 0.020 sec/batch)
2017-09-16 17:39:33.558102: step 213420, loss = 0.20 (5341.4 examples/sec; 0.024 sec/batch)
2017-09-16 17:39:34.002408: step 213430, loss = 0.26 (5082.9 examples/sec; 0.025 sec/batch)
2017-09-16 17:39:34.442954: step 213440, loss = 0.25 (5233.3 examples/sec; 0.024 sec/batch)
2017-09-16 17:39:34.874030: step 213450, loss = 0.21 (6296.1 examples/sec; 0.020 sec/batch)
2017-09-16 17:39:35.326753: step 213460, loss = 0.23 (5330.7 examples/sec; 0.024 sec/batch)
2017-09-16 17:39:35.761446: step 213470, loss = 0.27 (6229.5 examples/sec; 0.021 sec/batch)
2017-09-16 17:39:36.202369: step 213480, loss = 0.20 (5701.5 examples/sec; 0.022 sec/batch)
2017-09-16 17:39:36.642384: step 213490, loss = 0.22 (5837.7 examples/sec; 0.022 sec/batch)
2017-09-16 17:39:37.083650: step 213500, loss = 0.17 (5335.9 examples/sec; 0.024 sec/batch)
2017-09-16 17:39:37.677085: step 213510, loss = 0.19 (6205.5 examples/sec; 0.021 sec/batch)
2017-09-16 17:39:38.117421: step 213520, loss = 0.27 (5908.8 examples/sec; 0.022 sec/batch)
2017-09-16 17:39:38.558955: step 213530, loss = 0.18 (5599.6 examples/sec; 0.023 sec/batch)
2017-09-16 17:39:38.901445: step 213540, loss = 0.21 (8003.0 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:39.221909: step 213550, loss = 0.24 (8097.7 examples/sec; 0.016 sec/batch)
2017-09-16 17:39:39.542547: step 213560, loss = 0.14 (7868\n
2017-09-16 17:41:24.362254: precision @ 1 = 0.872
